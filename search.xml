<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TaoTao要吃鸡（01背包问题）]]></title>
    <url>%2F2018%2F01%2F31%2F2018-1-30-chiji%2F</url>
    <content type="text"><![CDATA[多校联赛的一个题目，题目其实是基础动态规划01-背包问题的改编版。 题目链接 题目描述Taotao的电脑带不动绝地求生，所以taotao只能去玩pc版的荒野行动了，和绝地求生一样，游戏人物本身可以携带一定重量m的物品，装备背包之后可以多携带h（h为0代表没有装备背包）重量的东西。玩了几天taotao发现了一个BUG，当装备背包之后，如果可携带重量没有满，就 可以拿一个任意重的东西。（解释看样例）有一天taotao空降到了一个奇怪的岛上，岛上有n件装备，每个装备都有重量Wi和威力值Vi,但taotao不认识这些装备，所以他来求助你，挑选威力最大的装备，帮助他吃鸡。 例如有三个装备：(2, 3), (3, 2), (2, 3)，现在m=3，h=3，当拿了前两个物品后，占用容量5，不满总容量6，还可以再拿第三个，得到总威力为：8。 思路如果h=0那么没有那个BUG的存在，就是一个简单的01背包;h!=0那么枚举最后拿的物品，再对除了这个物品的其余物品做01背包即可。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define maxn 100int v[maxn + 10], w[maxn + 10];int f[maxn * 2 + 10];int n, m, h;int dp(int x)&#123; memset(f, 0, sizeof(f)); for (int i = 0; i &lt; n; ++i) &#123; if(i == x) continue; for (int j = h + m; j &gt;= v[i]; j--) &#123; f[j] = max(f[j], f[j - v[i]] + w[i]); &#125; &#125; if(x == -1) &#123; return f[m + h]; &#125;else&#123; return f[m + h - 1] + w[x]; &#125;&#125;int main(void)&#123; while(scanf("%d", &amp;n), n) &#123; scanf("%d%d", &amp;m, &amp;h); for (int i = 0; i &lt; n; ++i) &#123; scanf("%d%d", &amp;v[i], &amp;w[i]); &#125; if(!h) &#123; printf("%d\n", dp(-1)); &#125; else &#123; int ans = 0; for (int i = 0; i &lt; n; ++i) &#123; ans = max(ans, dp(i)); &#125; printf("%d\n", ans); &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>动态规划</tag>
        <tag>背包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双拆分数]]></title>
    <url>%2F2018%2F01%2F09%2F2018-1-9-algorithmshuangchai%2F</url>
    <content type="text"><![CDATA[题目描述对于一个数字串 s，若能找到一种将其分成左右两个非空部分 s1,s2 的方案，使得： s1,s2 均无前导零 存在两个正整数 a,b，使得 b 整除 a，且 a/b=s1, a*b=s2 那么我们记这是一个合法的分法。特别地，如果一个串有两个或更多个不同的合法的分法，那么我们称这个数字串是双拆分数字串。 给定一个 n，要求构造一个长度恰为 n 的双拆分数字串。如果无解，输出 -1。 思路首先根据上面的条件可以得出，一种合法的分法也就等价于s2/s1=k*k，也就s2/s1的结果是完全平方数。 然后就是找到一个长度为n的数，它有两种合法的分法。 当n&lt;=3时，显然无解，因为只有两个分割点，而且第二个分割点的分割使得s2&lt;s1。 我们又会想到，如果一个数n是双拆分数，那么在n后面加两个0得到的数肯定也是双拆分数。那么问题就简单了，因为n&lt;=3的双拆分数不存在，我们只需找到一个长为4和一个长为5的双拆分数即可，而其他任意长度的双拆分数我们都可以使用这两个双拆分数在后面添0得到。 可以写一个暴力程序得到一个长度为4的双拆分数为1144，长度为5的双拆分数为16400。当然对数学敏感的人应该很快就能构造出这两个数。]]></content>
      <categories>
        <category>算法</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>双拆分数</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop2.7.5集群搭建]]></title>
    <url>%2F2018%2F01%2F09%2F2018-1-8-hadoopcluster%2F</url>
    <content type="text"><![CDATA[搭建hadoop2.7.5集群组内最近要搭建一个spark平台，先让我们探探路，于是我就去阿里云和腾讯云各租了一台服务器用来搭建一个hadoop集群(-_-)。 talk is check, action! 环境准备系统信息两台服务器的系统都是cent os 7.3，其他版本也是大同小异。 创建用户在每台结点上创建一个名为hadoop的用户并修改密码：12useradd hadooppasswd hadoop 我们的hadoop程序都是部署在每台结点的hadoop用户上的，所以本文以后的操作如无特别说明，都是在hadoop用户上进行。 配置jdk先安装jdk：1sudo yum install java-1.8.0-openjdk.x86_64 再配置环境变量：1export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk 配置hosts文件我这两个服务器的外网ip和内网ip为： 公网ip 内网ip 服务器1 node1_ip_1 node1_ip_2 服务器2 node2_ip_1 node2_ip_2 在node1的/etc/hosts文件后面添加下入面内容：12node1_ip_2 node1 node1node2_ip_1 node2 node2 在node2的/etc/hosts文件后面加入下面内容：12node1_ip_1 node1 node1node2_ip_2 node2 node2 可以发现在给A结点配置hosts文件时，除了A结点外，在hosts文件内配置的都是外网ip，而A结点本身的配置只能是内网ip，其他结点类似。注意这一点很重要，不然会导致无法启动namenode。（很重要，血的教训） 配置免密码登录配置所有结点间的ssh免密码登录。原理就是如果要配置结点A免密码登录B，那么只需要将A结点下.ssh目录下的公钥（一般为id_rsa.pub文件的你内容）复制到结点B的.ssh/authorized_keys文件中即可。 这个操作使用scp-copy-id命令完成即可。 比如在这里的node1需要进行如下配置 123ssh-keygen # 产生秘钥ssh-copy-id localhostssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@node2 结点node2是一样的。 打开阿里云服务器的端口如果是使用的阿里云或者腾讯云，这些服务器的端口默认是关闭的（包括22号端口），要打开的话要登录相应的官网，找到安全组规则，点击快速添加规则，填写下图的内容便打开了所有的端口。 hadoop配置下载hadoop在所有结点运行：12wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz -P /tmptar -xf /tmp/hadoop-2.8.1.tar.gz -C /usr/local/hadoop --strip-components 1 配置hadoop运行环境在所有结点的~/.bash_profile中加入下面内容：12345678export HADOOP_HOME=/usr/local/hadoopexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 执行source ~/.bash_profile即可。 修改配置文件对所有结点的$HADOOP_HOME/etc/hadoop/中的配置文件做如下配置： 在hadoop-env.sh中加入：1export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk 向core-site.xml添加如下内容： 123456&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node2:9000/&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hdpdata&lt;/value&gt;&lt;/property&gt; 在hdfs-site.xml中加入如下内容：12345678&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///home/hadoop/namenode&lt;/value&gt;&lt;/property&gt; 在mapred-site.xml中加入如下内容：1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 在yarn-site.xml中加入如下内容：1234567891011121314&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;node2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 在slaves文件内加入如下内容：12node1node2 slaves内的内容代表datanode结点。 注意上面的配置在一台结点配置好了之后一定要将它们发送到所有结点，使得所有结点的hadoop配置相同。 格式化namenode1hdfs dfs namenode -format 启动hadoop12start-dfs.shstart-yarn.sh 输入jps，查看显示结果是否各个组件都成功启动：1234569461 ResourceManager8999 NameNode9131 DataNode9293 SecondaryNameNode15038 Jps9582 NodeManager 如果某个组件没有成功启动，可以查看$HADOOP_HOME/logs/下相应的日志文件。 测试hadoop1234hdfs dfs -mkdir /inputhdfs dfs -copyFromLocal /home/hadoop/words.txt /input/hdfs dfs -ls /inputhdfs dfs -cat /input/words.txt 好了可以愉快的玩耍了。。。 补充网络知识：服务器公网ip可以用于域名解析ip，服务器远程登录ip，是最主要的服务器ip地址。 内网ip不能用于域名解析。 不可以直接用于服务器远程登录，其主要作用是：跟当前帐号下的其他同集群的机器通信。 一些小型企业或者学校，通常都是申请一个固定的IP地址，然后通过IP共享（IP Sharing），使用整个公司或学校的机器都能够访问互联网。而这些企业或学校的机器使用的IP地址就是内网IP，内网IP是在规划IPv4协议时，考虑到IP地址资源可能不足，就专门为内部网设计私有IP地址（或称之为保留地址），一般常用内网IP地址都是这种形式的：10.X.X.X、172.16.X.X-172.31.X.X、192.168.X.X等。需要注意的是，内网的计算机可向Internet上的其他计算机发送连接请求，但Internet上其他的计算机无法向内网的计算机发送连接请求。 公网IP就是除了保留IP地址以外的IP地址，可以与Internet上的其他计算机随意互相访问。我们通常所说的IP地址，其实就是指的公网 IP。互联网上的每台计算机都有一个独立的IP地址，该IP地址唯一确定互联网上的一台计算机。这里的IP地址就是指的公网IP地址。 其实，互联网上的计算机是通过“公网IP＋内网IP”来唯一确定的，就像很多大楼都是201房间一样，房间号可能一样，但是大楼肯定是唯一的。公网IP地址和内网IP地址也是同样，不同企业或学校的机器可能有相同的内网IP地址，但是他们的公网IP地址肯定不同。那么这些企业或学校的计算机是怎样IP地址共享的呢？这就需要使用NAT（Network Address Translation,网络地址转换）功能。当内部计算机要连接互联网时，首先需要通过NAT技术，将内部计算机数据包中有关IP地址的设置都设成NAT主机的公共IP地址，然后再传送到Internet，虽然内部计算机使用的是私有IP地址，但在连接Internet时，就可以通过NAT主机的NAT技术，将内网我IP地址修改为公网IP地址，如此一来，内网计算机就可以向Internet请求数据了。 ——百度百科]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Skeleton Key-Image Captioning by Skeleton-Attribute Decomposition]]></title>
    <url>%2F2018%2F01%2F05%2F2017-12-30-cvpr2017sekl-lstm%2F</url>
    <content type="text"><![CDATA[最近在读CVPR2017的关于图像描述的文章，读到本文感到文章结构简单但是设置巧妙，所以特地markdown一下 论文地址 摘要最近，图像描述任务得到广泛关注。大多数已经存在的基于语言模型的方法是依据原先描述的词的顺序一个词一个词的生成。然而对于人，更自然的方法是，先定位好物体以及它们之间的关系，然后仔细描述每个物体以及它们的属性。文章提出一种“从粗糙到细致”的图像描述方法，通过将原始图像分解为一个主干句子和一些属性，这两者的生成是分开的。通过这种分解，文章的方法可以使得描述更加准确，生成更加合理的描述。而且还能生成长度不一更自然的描述。 引言近几年，自动生成图像的描述问题在计算机视觉领域受到了广泛的关注。但是这个问题是很有挑战性的，原因是描述的生成过程不仅需要学习到高层的图像语义而不仅仅是简单的物体和场景的识别，再就是需要产生一个语义和语法都正确的句子来描述重要的物体、物体间的属性以及它们之间的关系。 图像描述模型的发展历程可以总结为下图: 从图中可以看出，图像描述模型分为三个流派： 基于检索 基于模板 基于语言模型 基于检索的方法是通过对输入的图像在一个庞大的图像-文本对数据集中进行检索，得到相应的描述。这种方法的结果好坏依赖数据集的好坏和完整度，数据集很差或不完整都会影响描述结果。 基于模板的方法是根据预先设定好的模板进行生成描述，如预先设定好的模板可能是主谓宾，这样生成的描述依赖模板的好坏，而且模板是有限的，而句子是无限的，以有限描述无限会使生成的描述显得死板，不自然。 基于语言模型的方式是通过神经网络计算给定图像下得到描述的似然概率生成描述，训练的过程就是最大化这个似然概率。但是基于语言的方法会有下面两方面的问题： 泛化能力不够，生成的描述有可能只是对训练数据的”死记硬背” 一个词一个词的生成描述，有可能使得生成的物体在其属性的后面生成。 为了解决这些问题，文章提出了一种从粗糙到细致的描述生成方法，方法分两步： 第一步先生成描述的基本主干，基本主干中包含了图像中主要物体。 第二步对基本主干使用注意力生成相应的属性。 这样将描述物体和描述属性两个过程分离开，使得描述生成的更准确。 如下图： 本文的工作的灵感来自于认知神经学研究。文章介绍在视觉感知的过程如物体识别的过程中，有一种从上到下进行识别的机制起了重大作用，这种机制的原理还没有被人们索理解，但是有科学家猜测其先是通过低频率的空间特征得到对一个物体的初始猜测。然后低频率空间特征被映射为低层次视觉特征，用于得到高层次的语义。这个思想借鉴到图像描述任务就是从粗糙到精细的生成描述思想，通过将问题分解为两步，先描述句子概况，然后通过句子概况描述相应细节，前一步有助于解决下一步。 模型模型总体结构如下图，在训练阶段，将描述分解为主干句子和属性输入到模型中训练，测试阶段先基于图像生成主干，然后在主干句子上施加注意力产生相应的属性，然后将主干句子和属性合并成描述。使用两个lstm，一个是skel-lstm,用于对输入的图像产生骨架，另一个是attr-lstm用来得到主干句子中物体相应的属性。 描述的骨架－属性分解文章使用斯坦福的NLP工具constituency parser对句子进行分析，得到一个依赖树，如上图中。树中的结点对应短语的不同类型，如NP(名词短语)，VP(动词短语)，PP(介词短语)和AP(形容词短语)等。 那么怎么抽取句子中的物体呢？找到最低层的NP，将NP中的最后一个词视为物体，最后一个之前的词视为这个物体的属性。而其他属性的结点则保持不变的留在主干句子中。 有时上面的方式提取物体看起来不太适合，如对于coffee cup是将cup看做物体，将coffee看做描述cup的属性还是将coffee cup看做一个物体好?本文不区分这些，都使用上面的方式。实验效果显示模型会学到到它们内在的关系。 coarse-of-fine LSTM模型编码阶段使用CNN对图像进行编码，解码阶段使用两个LSTM，一个skel-lstm生成句子主干，一个attr-lstm生成为骨架中物体生成相应的属性。 skel-lstmskel-lstm使用的soft attention lstm，其结构如下图所示： 可以看出，上图其实就是一个基本的LSTM结构，只是它的每个门的输入多了注意力向量\(z_t\)和前一个时刻LSTM的输出$Ey_{t-1}$。 其中的$z_t$就是注意力映射向量，这个向量的计算方式如下： 我们使用$v_{ij}\in R^D$表示在位置$(i,j)\in L\times L$处的图像特征。第t步的注意力映射表示为归一化的权值$\alpha_{ij,t}$，它是通过下面的式子得到:1\alpha&#123;ij,t&#125;=Softmax(MLP(v_&#123;i,j&#125;, h_&#123;t-1&#125;)) 然后就能得到$z_t$：1z_t=\sum_&#123;i,j&#125;\alpha_&#123;ij,t&#125;v_&#123;ij&#125; 我们计算得到的注意力映射向量$z_t$不仅用于此处生成句子主干，而且还重用于生成后面的物体属性。 Attr-Lstm在句子主干生成后，我们需要为句子主干中的物体生成相应的属性，文章使用Attr-lstm直接生成一系列的属性，而不是分别为一个物体生成多个属性。 Attr-lstm的结构与传统用于图像描述的lstm结构类似，如下图： 上图中的LSTM的输入序列是：123x_&#123;-1&#125;=CNN(I) x_t=W_ey_t,t=0,1,...,N-1 其中I代表图像，CNN(I)代表图像的特征，$W_e$是词嵌入矩阵，$y_t$是词的one-hot向量，所以$W_ey_t$是单词的嵌入向量。 作者认为属性的生成与原图像、主干单词、主干句子的上下文这三种类型的信息相关。而主干句子的上下文信息就存储在skel-lstm中，那么在attr-lstm的第t不使用哪个skel-lstm哪一步的状态信息比较好呢？作者通过实验使用skel-lstm的前一个状态、当前状态，下一个状态的信息，结果发现当前步状态信息效果最好。而图像特征也不再使用原始图像特征，而是经过skel-lstm计算后的注意力映射特征$z_t$。 综上，在Attr-lstm中lstm的输入是:123x_&#123;-1&#125;=MLP(W_1z_T, W_tS_T^&#123;skel&#125;+W_hh_T^&#123;sekl&#125;)x_t=W_ey_t,t=0,1,...,N-1 其中，$z_t$就是上面提到的经过注意力映射的图特征，$S_T^{skel}$是第T个主干句子中的词向量，$h_T^{sekl}$是第T步skel-lstm的隐状态。 精调$z_t$考虑到注意力在序列到序列模型中的重要性，作者提出了一种对上面的注意力映射向量$\alpha_{ij}$的改进方式： 上面提到的$\alpha$其实是通过前一步的状态得到，所以它是前一单词的注意力，而作者认为，当前词的确定不只依赖前一个词，也依赖后一个词，所以定义了$\alpha_{post}$代表后一个词的注意力。 设$P_{attend}=p_1,P-2,...,p_Q$为LSTM的输出，其中Q代表sekl-lstm的输入词典大小。那么将图像I每个位置特征$v_{i,j}$输入到LSTM中就能得到一个概率向量$P_{ij}$。于是作者就使用这个概率对$\alpha_{ij}$进行改进。1\alpha_&#123;post(ij)&#125;=\frac&#123;1&#125;&#123;Z&#125;P^T_&#123;attend&#125;\times P_&#123;ij&#125; 其中Z是正则化项。 精调其实就是使用$P_{attend}和$P_{ij}$的相似度作为下一个词的注意力映射向量。 如下图： 长度因子太短的描述很可能会丢失物体的属性，所以作者在生成描述的时候加入一个长度因子来鼓励长描述的生成。不同长度的描述蕴含了不同丰富度的信息，这允许我们根据用户的偏好来调节产生匹配用户的偏好。 1log(\hat&#123;P&#125;)=log(P)+\gamma\times l 其中P是生成的描述的概率。 实验数据集实验使用的数据集有两个： MS-COCO Stock3M 数据预处理对于MSCOCO数据集，移除其中的标点，将描述全部转换为小写。去除在主干句子中出现次数低于5次的单词和在属性句子中出现3次的单词。 实验结果在SPICE上的对比图： 在其他评价标准上的对比图：]]></content>
      <categories>
        <category>papper</category>
      </categories>
      <tags>
        <tag>papper</tag>
        <tag>CVPR</tag>
        <tag>ImageCaption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-11-10-linuxinstall]]></title>
    <url>%2F2017%2F11%2F10%2F2017-11-10-linuxinstall%2F</url>
    <content type="text"><![CDATA[pip安装指定版本软件 pip install -v tensorflow=1.0]]></content>
  </entry>
  <entry>
    <title><![CDATA[2017-11-10-linuxgrep]]></title>
    <url>%2F2017%2F11%2F10%2F2017-11-10-linuxgrep%2F</url>
    <content type="text"><![CDATA[文本处理三剑客 grep文本过滤 egrep fgrep grep sed stream editor awk Linux上实现gawk,文本报告生成器 grep 文本搜索工具，根据用户指定的模式对目标文本逐行进行匹配检查，打印匹配到的行， 模式：由正则表达式字符及文本字符所编写的过滤条件n]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux经典案例]]></title>
    <url>%2F2017%2F11%2F07%2F2017-11-07-linxuexample%2F</url>
    <content type="text"><![CDATA[整理一下遇到的linux命令的题目，并对答案做个总结 统计单词数目并排序&ensp;&ensp;&ensp;统计一个文件中每个单词各自出现的次数并排序 sed -r ‘s# +#\n#g’ words.txt | sed -r ‘/^$/d’ | sort | uniq -c | sort -r | awk ‘{print $2,$1}’ cat words.txt | tr -s ‘ ‘ ‘\n’ | sort | uniq -c | sort -r | awk ‘{ print $2, $1 }’ tr -s ‘ ‘ ‘\n’ &lt; words.txt|sort|uniq -c|sort -nr|awk ‘{print $2, $1}’ cat words.txt | awk ‘{for(i=1;i&lt;=NF;++i){count[$i]++}} END{for(i in count) {print i,count[i]}}’ | sort -k2nr awk ‘{for(i=1;i&lt;=NF;i++) a[$i]++} END {for(k in a) print k,a[k]}’ words.txt | sort -k2 -nr 打印文件第十行&ensp;&ensp;&ensp;打印一个文件的第十行内容 &ensp;&ensp;&ensp;做法有很多： head -n 10 file.txt | tail -n +10 sed -n ‘10p’ file.txt awk ‘NR == 10’ file.txt awk ‘{if(NR==10) print $0}’ file.txt 找出不合法电话号码&ensp;&ensp;&ensp;给你一个文件，文件中每行是一个电话号码，有合法的有不合法的，找出合法的电话，合法的电话号码形式是：(xxx) xxx-xxxx或xxx-xxx-xxxx，每一行不含前导或后导空格。 &ensp;&ensp;&ensp;如文件内容如下: 987-123-4567123 456 7890(123) 456-7890 &ensp;&ensp;&ensp;你需要输出： 987-123-4567(123) 456-7890 grep -P ‘^(\d{3}-|(\d{3}) )\d{3}-\d{4}$’ file.txt sed -n -r ‘/^([0-9]{3}-|([0-9]{3}) )[0-9]{3}-[0-9]{4}$/p’ file.txt awk ‘/^([0-9]{3}-|([0-9]{3}) )[0-9]{3}-[0-9]{4}$/‘ file.txt 转置文件内容&ensp;&ensp;&ensp;如下所示，文件内容：name agealice 21ryan 30Output the following:&ensp;&ensp;&ensp;需要得到：name alice ryanage 21 30 awk ‘NF!=0 {for(c=1;c&lt;=NF;c++) mtx[NR,c]=$c; rows++; cols=NF;} END{for(c=1;c&lt;=cols;c++) { line=mtx[1,c]; for(r=2;r&lt;=rows;r++) { line=line” “mtx[r,c]}; print line; }}’ file.txt 分析日志找访问量最大文件分析图片服务日志，把日志（每个图片访问次数×图片大小的总和）排行，取top10，也就是计算每个url的总访问量大小。 awk ‘{print $7”\t” $10}’ filename | sort |uniq -c|awk ‘{print $1*$3,$1,$2}’ | sort -rn | head awk ‘{S[$7]++;T[$7]+=$10}END{for(k in S) print k,S[k],T[k]r’ filename]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>example</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux三剑客之awk]]></title>
    <url>%2F2017%2F11%2F07%2F2017-11-07-awk%2F</url>
    <content type="text"><![CDATA[awk作为linux系统中文本处理三剑客中的老大，它几乎实现文本处理的一切问题。其实awk本身已经不再是一个命令那么简单，而是一门语言。本文将带你走进awk的世界。 基本知识awk的主要功能是过滤内容（取列）。 命令一般格式：1awk [-F SEP] 'BEGIN&#123;&#125;&#123;&#125;END&#123;&#125;' -F选项指定文件中一行内容中列的分隔符，如文件中一行内容为a-b-c，然后指定awk选项-F为-，则awk处理时就会以a为第一列,b为第二列……，回想一下linux中的命令cut，它也是用来取文件中某一列内容的，他的-d参数指定列分隔符，然后用-f指定取那一列，如:12345678$ cat testa-b-cd-e-fg-h-k$ cut -d - -f 2beh 大括号内书写awk的模式或命令语句。在大括号内可以写类似C语言的语句，如:1awk '&#123;print $n&#125;' test 意思就是打印文件test中每一行的第n列。而列的定义是默认空格分割，可以使用-F选项指定，$后面可以跟1,2,3,...,NF-1,NF，其中NF表示最后一列的号码。 除了NF，awk中还有哪些特殊符号呢？ NR 行号 NF 最后一列列号 ARGC 命令行参数个数 ARGV 命令行参数数组 ENVIRON 支持队列中系统环境变量的使用 FILENAME awk浏览的文件名 FNR 浏览文件的记录数 FS 设置输入列分隔符，等价于命令行 -F选项 OFS 输出列分隔符 RS 输入行分隔符 ORS 输出行分隔符 awk数组awk的数组和C语言的数组其实没什么不同，只是在awk中的数组不需要声明。 如现在有一个文件，里面的内容如下：123456$ cat testa-db-ea-fc-gf-z 每一行的两个字母构成了一个字母对，现在需要根据第一个字母进行去重，对于第一个字母相同的字母对只保留最后出现的那一个字母对。应该怎么做？ 使用awk数组即可。12345$ awk -F '-' '&#123;a[$1]=$2&#125;END&#123;for(k in a) print k,a[k]&#125;' testa fb ec gf z 这里有一个问题，有些同学会将命令写为awk -F &#39;-&#39; &#39;BEGIN{a[$1]=$2}END{for(k in a) print k,a[k]}&#39; test，但是这样的话会发现将什么也不输出。这是为什么呢？ 这就跟awk提供的两个特殊的表达式BEGIN和END相关了。 awk提供BEGIN和END的作用是在程序开始之能够初始化一些状态，在程序结束之后能够做一些扫尾工作。 在BEGIN之后的{}内的操作将在awk开始扫描输入之前执行，而END之后的{}内的操作将在扫描完全部的输入之后执行。因此，通常使用BEGIN来显示变量和预置（初始化）变量，使用END来输出最终结果。 所以很明显，这位同学写的语句中为数组赋值的语句只在awk扫描文本之前执行一遍，而在扫描文本的过程中并没有向数组中填写内容，所以没有任何输出结果。 那么awk支不支持二维数组呢？其实也支持，awk的数组可以理解为一个字典，它的索引只接受字符串，但是我们可以使用数字索引，awk内部会把数字内部索引转换为字符串。 使用a[1,2]时代表使用了二维数组，但是awk内部会将其转换为a[&quot;1\0342&quot;]。其中,可以用\034代替，也可以使用SUBSEP代替，它的意思是Subscript Sepqrator数组下标分隔符。 所以下面的几条命令是等价的:1234awk 'BEGIN &#123;Arr[2,79]=78 print Arr["2\03479"]&#125;'awk 'BEGIN &#123;Arr[2,79]=78 print Arr[2,79]&#125;'awk 'BEGIN &#123;Arr[2,79]=78 print Arr["2\03479"]&#125;'awk 'BEGIN &#123;Arr[2,79]=78 idx=2 SUPBSEP 79 print Arr[idx]&#125;' 条件和循环awk中的{}可以书写流程控制语句，如条件语句:12345678910111213141516171819if (expression) &#123; statement; statement; ... ...&#125;if (expression) &#123; statement;&#125; else &#123; statement2;&#125;if (expression) &#123; statement1;&#125; else if (expression1) &#123; statement2;&#125; else &#123; statement3;&#125; 条件语句中常使用的逻辑运算符也跟C语言一样，如&amp;&amp;表示且等。 循环:123456while(expression)&#123; statement&#125;for(k in arrary)&#123; statement&#125; awk中的循环和C语言基本相同，都可以使用break、continue等有语句。 实验下面来介绍一个小例子，说明awk的强大应用，取出下列文本中电话号码并进行去重。 1234567$ cat testa-111111-hgeheb-222222-popopc-111111-cdsfsd-123456-rtryra-245124-bdfefc-123456-aaads 可以使用cut命令:1234567$ cut -d - -f 2 test | sort | uniq -c 1 1 a hehe a 1 b hehe b 1 c hehe c 2 c hehe d 2 f hehe f 也可以使用awk：1awk '&#123;print $2&#125;' test | sort | uniq -c 参考链接 awk的BEGIN和END]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学校生活中的一些资料]]></title>
    <url>%2F2017%2F11%2F05%2F2017-11-05-sources%2F</url>
    <content type="text"><![CDATA[&ensp;&ensp;&ensp;收集到的一些小资料，放在这主要是图个好管理，也就不公开了。 学术道德与学术规范PPT 分析学术道德失范问题产生的原因答案 高校、导师、研究生在研究生学术道德建设方面扮演的角色和完成的工作答案 自己将从哪些方面确保研究生学习阶段不发生学术道德失范现象答案 GANPPT july GAN入门 code 用keras实现DCGAN]]></content>
      <categories>
        <category>资料</category>
      </categories>
      <tags>
        <tag>资料</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux三剑客之sed]]></title>
    <url>%2F2017%2F11%2F03%2F2017-11-4-linuxsed%2F</url>
    <content type="text"><![CDATA[## sed&ensp;&ensp;&ensp;sed(stream editor)流编辑器 sed -n 取消自动打印（sed会自动将全部未过滤内容输出） sed -i 将更改作用到源文件，不加-i则只会把sed的输出修改而不会改变源文件 &ensp;&ensp;&ensp;sed替换用法： sed &#39;s#string1#string2#g&#39; filename&ensp;&ensp;&ensp;其中s代表sub，意思是替换，#是分隔符，string1要替换的文本，string1要替换成的文件，g是global全局的意思，可以不带g，不带g的意思是值替换第一个（想想vim中的替换）。&ensp;&ensp;&ensp;#可以换成别的字符，但是要换的话，3个#都得换，换成的字符不能是s、g（当s或g出现在命令体中时）或string1和string2中的任何字符，如果非得使用上述字符，则加反斜线\\。sed 流编辑器 增删改查 过滤 取行 sed [options] [sed-command] [input] sed命令前面可以加地址范围n1[,n2] 10[sed-command] 10,20[sed-command] 10,+20[sed-command] 1～2[sed-command] 10，$[sed-command] 地址还支持正则匹配 /libai/[sed-command] /libai/,/luna/[sed-command] /libai/,$[sed-command]如果匹配有多个，回想sed的流程来分析 地址还支持数字和正则匹配混合 10,/libai/[sed-command] /libai/,10[sed-command] /libai/,+10[sed-command] 增删改查 a 追加 sed &#39;2a hehehe&#39; filename i 插入 sed &#39;2i hehehe&#39; filename a和i的区别可以想想vim的a和i a和i可以单行增加也可以多行增加 sed 2a hehehe\nhohoho filename 多行时可以在文本中显示加入\n也可以在写命令要加入文本时在终端显式敲回车 d 删除 sed &#39;d&#39; filename全部删除 sed &#39;2d&#39; filename sed &#39;2,5d&#39; filename c 行间替换,用法跟前面的a和i、d相似 s 替换指定字符串，g命令替换标志—全局替换标志 sed &#39;s#str1#str2#g&#39; filename将str1替换为str2 -i参数 不加此参数，那么sed所做的任何操作都是在模式空间中做的，也就是在文件在内存中的映像做的，原文件内容是不变的，加了这个参数，就将在模式空间做的更改更新到文件上。 变量替换 sed s#$x#$y#g zimux.txt 将zimux.txt中变量x中的值替换为变量为y的值。这种情况下，sed-command不能使用单引号，可以使用双引号和什么都不加。非要用单引号，可以sed &#39;s#&#39;$x&#39;#&#39;$y&#39;#&#39; filename也可以使用eval sed &#39;s#$x#y#g&#39; filename。 分组替换 跟正则表达式的分组替换和vim中的分组匹配替换是一样的。echo &#39;abc123&#39; | sed &#39;s#[a-z]+([0-9]+)#\1#g&#39; -r选项启动扩展正则表达式 特殊符号&amp; 代表被匹配的内容,相当于\0第0组企业案例：批量重命名文件ls *.jpg | sed -r &#39;s#(.*)_finished.*#mv &amp; \1.jpg#g&#39; rename命令 p打印输出的内容，常与-n选项配合使用，-n选项取消默认输出。其实默认情况下会对匹配到的内容输出了，p又输出了一次。所以加上-n取消默认输出。 set 修改文件及另存文件及替换命令 修改文件 -i 备份 sed -i[SUFFIX] &#39;set-commant&#39; filename 另存文件 sed &#39;w outputfile&#39; filename 将模式空间的内容另存到outputfile文件中 sed &#39;[地址范围][模式范围] s#[替换的字符串]#[替换后的字符串]#[替换标志]&#39; [输入文件] 替换标志 全局标志g 数字标志1，,2，,3，… 打印p 写w 忽略大小写i 执行命令标志e 地址范围是指定哪一行放入模式空间操作。 全局标志g 不带g代表已经被前面选中的东西中匹配的第1列 带g表示匹配的所有列 Ms# # #Ng Ms 对第M行进行操作 不带g表示对匹配的第一列进行操作 带g 表示匹配的所有列 Ng 从第N处/列开始匹配 Ms Ng 从第M行的第N出匹配进行处理 数字表示X 只对第X处/列替换 打印p sed &#39;s#ab#dd#;w outputfile&#39; filename 写命令 w sed &#39;s#ab#dd#w outputfile&#39; filename 写标志 w 两种是不同的 忽略大小写标志 i 执行标志e 将模式空间的任何内容当做shell bash 命令执行 案例 系统开机启动优化 sed -r &#39;s#(.*),(.*),(.*)#\L\3,\E\1,\U\2#g&#39; filename 特殊符号= 获取行号 一条sed 命令执行多行sed命令 删除第三行到末尾的数字 并将10替换为01 sed -e &#39;3,$d&#39; -e &#39;s#10#01#g&#39; filename sed &#39;3,$d;s#10#01#g&#39; filename sed -f person.sed filename person.sed是一个sed脚本 案例 一个文件100行，把第5，35，,70行单独拿出来 sed -n &#39;3p;35p;70p&#39; filename 特殊符号{}的用法 sed -n &#39;2,4{p;=}&#39; filename l命令 打印中包含不可见字符 转换字符 tf &#39;abc&#39; &#39;ABC&#39; &lt; filename sed ‘y#abc#ABC#’ filename 退出sed命令q 执行到某个地方不再执行 从文件中读取数据 命令r sed ‘r num.txt’ filename 保持空间和模式空间 模式空间： n 清空当前模式空间，并读入下一行 N 不清空当前模式空间，并读入下一行，并用\n连接模式空间的两行 案例 用户名密码文件变为 stu=aaa sed &#39;N;s#\n#=#&#39; filename sed操作多个文件 模拟其他命令 cat sed -n ‘p’ sed ‘N’ sed ‘n’ sed ‘s# # #’ grep sed -n ‘//p’ sed -n ‘// !p’ head sed -n ‘1,3p’ sed ‘2q’ wc sed -n ‘$=’]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WGAN前作：GAN问题分析]]></title>
    <url>%2F2017%2F11%2F03%2F2017-11-03-GAN-question%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[DCGAN]]></title>
    <url>%2F2017%2F11%2F02%2F2017-11-03-DCGAN%2F</url>
    <content type="text"><![CDATA[前面咱们介绍了GAN，但是GAN的训练是不稳定的，DCGAN通过将GAN和CNN结合，并在上面加入一些限制，使得DCGAN训练较为稳定，并且能够使用DCGAN训练得到好的图像特征表示，然后使用网络的生成器和判别器作为特征抽取用于有监督任务。 模型&ensp;&ensp;&ensp;文章的方法是基于最近在CNN架构上的三种变化提出的。 Springenberg et al., 2014提出的全卷积网络，使用大步长的卷积操作代替采样操作，这样使得网络学习它自己的空间采样。文章将这种方法用在了判别器和生成器中。 消除卷积网络上端的全连接层。Mordvintsev et al.在图像分类任务中使用全局平均采样代替全连接层，作者发现使用全局平均采用能够增加模型的稳定性而降低模型的收敛速度。因此文本直接将生成器和判别器的输入和输出的全连接层移除，而都采用卷积操作（卷积在一定程度上就是采样）。 在除了生成器的输出层和判别器的输入层之外，每一层都使用Batch Normalization(BN,批处理化)。BN归一化每个神经元的输入为均值为0方差为1，这样解决了由低初始化引起的一些问题而且使得梯度在深度网络中传输的更好。BN已经被证明在GAN中很有用，使得GAN不会集中所有采样于一个点。 对于生成器，除了输出层使用tanh激励函数以为其余层均使用ReLU。对于判别器，所有层都使用LeakyReLU。&ensp;&ensp;&ensp;模型的总体结构如下： &ensp;&ensp;&ensp;上面是生成器，下面是判别器。 &ensp;&ensp;&ensp;生成器的模型其实就是卷积的逆操作，如下图： 训练训练参数 训练图像使用tanh将数值约束到[-1,1] batch size为128 初始化使用均值为0，方差为0.02的高斯分布 LeakyReLU参数为0.2 使用Adam Optimizer，学习率设置为0.0002，动量项\(\beta_1\)为0.5 实验&ensp;&ensp;&ensp;关于实验这里就不多说了，详细看原论文。 &ensp;&ensp;&ensp;下图是模型在LSUN上面的效果。 向量算数&ensp;&ensp;&ensp;在词嵌入中，词向量国王-词向量男人+词向量女人的结果向量是一个跟词向量女王很相似的向量。 &ensp;&ensp;&ensp;作者发现对于生成器向量z也有这个特点，如下图: 总结&ensp;&ensp;&ensp;虽然DCGAN在原始GAN中加入了很多限制使得其训练变得稳定，但是实际上DCGAN的训练还是不太稳定。]]></content>
      <categories>
        <category>DL</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>DCGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解LSTM（译）]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-30-LSTM%2F</url>
    <content type="text"><![CDATA[LSTM(long short term memory, 长短记忆网络)广泛应用于文本，很多基于其的模型最近几年被提出来解决时序模型，因此对其的理解有助于将其应用于新问题。本文的工作主要是对博文进行翻译，以理解LSTM 循环神经网络&ensp;&ensp;&ensp;人们的思考是持续性的，不是每秒都从头开始重新思考。当你读到这篇文章时，你都是在理解前面已经读过的单词的基础上理解每一个单词，你不会将前面的东西扔了重新思考。因此你的思考具有持续性。 &ensp;&ensp;&ensp;但是传统的神经网络不能做到这样，想一下，如果你想预测电影中每一帧要发生什么，传统神经网络并不能使用电影前面的信息来推断下一帧要发生的事情。 &ensp;&ensp;&ensp;循环神经网络可以做到这一点，通过循环的输入网络，使得信息能够保持住。 循环神经网络 &ensp;&ensp;&ensp;在上图中的网络A，通过将输入\(x_t\)处理输出\(h_t\)。循环的存在使得信息能够从网络的当前步传递到下一步。 &ensp;&ensp;&ensp;上图中循环的存在使得循环神经网络看起来不是那么容易理解。换了角度看，通过将循环神经网络每一步展开，如下图，循环神经网络可以被看做是多个相同网络的复制，每一个网络将信息传递给下一个网络。 展开的循环神经网络 &ensp;&ensp;&ensp;这种像链式的结果很自然的使得RNN能够很好的应用与序列数据。 &ensp;&ensp;&ensp;最近几年，循环神经网络被应用到了很多领域：机器翻译、语言模型、语言识别、图像描述等，对这些应用起着重大贡献是LSTMs，一种特殊的RNN，其效果比标准版本的RNN好很多，很多基于循环神经网络的应用使用的都是LSTM，本文讨论的就是它。 长短时依赖的问题&ensp;&ensp;&ensp;RNN的作用是能够将前面的信息跟现在的工作连接起来，例如利用电影中前面的情节预测当前帧要发生什么。如果RNN能做到这一点的话，那么它是很有用的，但是它真的能完全做到这一点吗？不一定。 &ensp;&ensp;&ensp;有时我们只需要最近的信息来预测当前的状态。例如，考虑一个语言模型，通过前面的词预测下一个词是什么，如果我们要预测&quot;the cloud are in the sky&quot;中的最后一个词sky，不需要太前面的信息，只要前面的the cloud are in的信息就可以预测sky。在这种情况下，预测所需要的与预测有关的前面的相关信息跟当前位置间隔很小，RNN的表现是很好的。 相关信息间隔很小 &ensp;&ensp;&ensp;但是有的时候我们在预测时确需要更多的上下文，考虑我们要预测文本&quot;I grew up in France...I speak fluent Franch&quot;中的最后一个词。最近的信息只能告诉我们最后一个词是一种语言，而我们要更准确的知道是哪个词，我们需要得到前面的上下文France的信息，而France距离最后面是很远的。这种情况下，预测所需要的相关信息的间隔很大，很不幸的是，当间隔很大时，RNN不能学习到很远的信息。也就是说在这种情况下，RNN的效果不好。 相关信息间隔很大 &ensp;&ensp;&ensp;在理论上，RNNs完全能够解决这种长时间依赖问题。人们可以很小心的调参使得一些小的RNN-base模型能够有好的效果。但是，在实践中RNN并不能学习到长时间依赖信息。原因有很多，其中很重要的是梯度爆炸和梯度消失问题，详细可以看(1991) [German]和Bengio, et al. (1994)。 LSTM网络&ensp;&ensp;&ensp;LSTM(Long Short Term Memory Networks,长短时记忆网络)，是一种特殊的RNN网络，它能够学习到长时间依赖。它最早被介绍在Hochreiter &amp; Schmidhuber (1997)，最近LSTM被人们广泛应用和改进。 &ensp;&ensp;&ensp;所有的RNN网络的网络结构都是以重复单元构成的链式架构。在标准RNN中，这个重复单元是一种非常简单的结构，例如只含有一个单一的tanh层。 标准RNN的重复单元，只含有一层tanh &ensp;&ensp;&ensp;LSTM的结构和RNN一样也是链式的。但是它的重复单元不同。相对于RNN的重复单元只含有一层，LSTM的重复单元是通过特殊方式连接的四层。 LSTM的重复单元：含有相互连接的四层 &ensp;&ensp;&ensp;接下来我们可以一步步的学习LSTM的详细结构。&ensp;&ensp;&ensp;先来规定一下操作的定义： 神经网络单元&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;元素级操作&ensp;&ensp;&ensp;&ensp;矩阵转换&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;连接&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;复制&ensp;&ensp;&ensp; &ensp;&ensp;&ensp;前面的LSTM重复单元的途中，每条线都携带一个向量，从输出结点出来输入到其他结点去。粉红色的圆代表元素级的运算，如矩阵相加等。黄色的方块代表可训练的神经网络层，线合并的地方代表连接，线分开的地方，代表将一个输入复制成两个输出，输出到不同的地方去。 LSTM的核心概念&ensp;&ensp;&ensp;LSTM的关键概念是细胞状态，在下图中最上面的水平线中携带的信息就是细胞状态。 &ensp;&ensp;&ensp;细胞状态就像一个传输背包，它在经过整条链的过程中只进行很小的线性变换。所以细胞状态中的信息在通过整个链的过程中变化是很少的。 &ensp;&ensp;&ensp;通过呼叫门，LSTM可以向细胞状态中添加或者移除信息。 &ensp;&ensp;&ensp;门的作用是控制信息是否穿过，门由一个sigmoid层 和一个元素级乘法操作构成。 &ensp;&ensp;&ensp;sigmoid层输出的值是0~1，决定组件的多少信息能够通过，如果是0代表任何信息都不能通过，如果是1代表任何信息都能通过。 &ensp;&ensp;&ensp;LSTM有3个门，来控制和保护细胞状态。 一步一步走进LSTM&ensp;&ensp;&ensp;LSTM的第一步是决定细胞状态的信息哪些需要扔掉，哪些需要保留。这一决策是通过忘记门层来完成。忘记门通过结合\(h_{t-1}和x_t\)的信息，为每个\(C_{t-1}\)矩阵的元素生成一个在0~1之间的数，这个数的作用就是上面说的门上的sigmoid输出的数的作用。 &ensp;&ensp;&ensp;回到前面讨论过的语言模型，在前面所有的词的基础上预测下一个词。在这个问题中，细胞状态的信息肯定包含了前一个主语的信息，而我们如果进入下一个主语预测，那么我们肯定希望忘记前一个主语的信息，也就是将前一个主语的信息从细胞状态中移除。 &ensp;&ensp;&ensp;下一步是决定哪些新信息需要被添加到细胞状态中。分为两步，首先使用一个称为输入门的sigmoid层决定哪些信息能够被加入。然后使用一个tanh层结合\(h_{t-1}和x_t\)为一个新的候选值\(\tilde{C_t}\)，最后将这两步的输出值结合起来得到添加到细胞状态的信息。&ensp;&ensp;&ensp;回到语言模型模型，就是上一步已经将前一个主语丢掉了，这一步就是在细胞状态中加入新主语信息。 &ensp;&ensp;&ensp;现在更新细胞状态，将其从\(C_{t-1}\)更新到\(C_t\)，前面的两步已经把决策和数据都准备好了，现在只需将他们结合起来即可。 &ensp;&ensp;&ensp;将旧状态乘以\(f_t\)，将状态中的信息需要忘记的去除掉。然后将得到的结果加上\(i_t * \tilde{C_t}\)，也就是将已经决定好要添加的信息添加到细胞状态上。 &ensp;&ensp;&ensp;回到语言模型，这一步的意思就是丢掉旧信息添加新信息。 &ensp;&ensp;&ensp;最后，我们决定要输出什么。输出是基于细胞状态的，跟以前一样，先用\(h_{t-1}和x_t\)经过一个sigmoid来确定细胞状态的什么信息需要输出。将细胞状态经过一个tanh(将值限制在-1到1之间)后跟前面sigmoid输出相乘得到最终输出。因此我们输出了应该输出的东西。 &ensp;&ensp;&ensp;回到语言模型的例子，因为刚看到了一个主语，那么看完之后要输出对下一个词的预测有影响的信息，比如跟动词相关的信息。例如，可能输出主语的单复数相关的信息，这样下一个词拿到这些信息就能决定动词的形式。 LSTM变体&ensp;&ensp;&ensp;上面我们描述的是最基础的LSTM，但是不是所有的LSTM都与上面的形式一样的。事实上，每篇使用LSTM的论文中LSTM架构都会有或多或少的不同。变化不大，但是它们中的一些还是值得一提。 &ensp;&ensp;&ensp;一种LSTM变体是来自Gers &amp; Schmidhuber (2000)，其在基本LSTM的基础上加入了peephole connections(猫眼连接)，也就是在每个门的输入上都加上细胞状态。 &ensp;&ensp;&ensp;上面在三个门上都加入了细胞状态，其实在一些论文中可能会只在某几个门上加。 &ensp;&ensp;&ensp;另一种变体是将忘记门和输入门耦合在一起。不再像原始LSTM那样分别决策要扔掉什么信息和添加什么信息到细胞状态，现在同时进行这两个决策。只有当要添加东西的时候才取扔掉旧东西，只有当要扔掉旧东西时才添加新东西。 &ensp;&ensp;&ensp;一个更经典的变体是GRU(Gated Recurrent Unit)，来自Cho, et al. (2014)，他将忘记门和输入门合并成一个更新门，将细胞状态和隐藏状态合并成一个状态，还做了一些其他的改的。GRU的结构比LSTM简单，很快得到广泛应用。 &ensp;&ensp;&ensp;这就是一些比较著名的LSTM变体。当然还有很多其他的变体，像Deep Gated RNNs。当然也有很多跟LSTM完全不同的结构来学习长时间依赖的，如Clockwork RNNs&ensp;&ensp;&ensp;哪些变体最好？这些变体的变化起作用吗？Greff, et al. (2015)对流行的变体做了一个比较，发现他们都是相同的。Jozefowicz, et al. (2015)对上万种RNN架构做了测试，发现一些模型在特定工作下的效果比LSTM好。 总结&ensp;&ensp;&ensp;前面我提到的人民使用RNNs取得的显著效果中，大部分是使用LSTM实现的。这说明，它在大多数任务上的效果都会很好。 &ensp;&ensp;&ensp;LSTM的原理写成一大堆公式很很吓人的，但是如果跟着本文一步一步的走下来，会很清楚的明白这些式子和LSTM的原理。]]></content>
      <categories>
        <category>DL</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用对比学习进行图像描述]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-30-contrastivelearning%2F</url>
    <content type="text"><![CDATA[人工智能的发展愈演愈烈，计算机视觉近几年的研究方向也在图像文本的融合方面发展，近几年，图像描述、图像问答、图文匹配等任务在各大会议上活动频繁，笔者最近读完了CVPR2017中关于图像描述的文章（CVPR文章总结），感觉意犹未尽，于是又将目光瞅准了ICLR2017，很快找到了下面这篇文章，文章突出了一种学习方法–对比学习，文章表示可以将对比学习应用于现有的任何图像描述模型中从而提高模型的效果。 论文地址 摘要&ensp;&ensp;&ensp;最近，图像描述取得了很大的进展，但是，现有模型往往忽略了描述的特殊性，好的描述一般会从特殊的角度去描述图像的内容。文章提出了一种对比学习方法，通过在现有模型上面加两个限制，以此提高模型生成描述的特殊性，而且还能保持描述总体的质量。文章证明了对比学习方法具有很好的泛化性，可以用于拥有各种各样架构的模型。 引言&ensp;&ensp;&ensp;现有的模型生成的描述有时会显得很僵硬，而且由于其有时是直接从训练集的描述中直接拿到一个片段放在当前描述中，使得描述很别扭。 &ensp;&ensp;&ensp;在现实中，当人们对一张图片进行描述时，他们一般会侧重一些特殊的图像中的物体去描述图像的内容。特殊的描述往往能够让人民很简单的从 相似的图片中找出目标图片。文章的后续章节证明了缺少特殊性会影响图像描述的质量。 &ensp;&ensp;&ensp;从技术角度看，缺少特殊性与模型生成描述的方式有关。现有模型都是通过最大似然训练得到，通过最大化给定图片下生成描述的概率来优化模型。但是这种方法并没有显示的考虑特殊性，不同图像的描述之间的不同没有被显示的考虑。文章发现，现有模型产生的描述跟训练集的描述有很大的相似，而没有特殊性。]]></content>
      <categories>
        <category>papper</category>
      </categories>
      <tags>
        <tag>papper</tag>
        <tag>Image Caption</tag>
        <tag>对比学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带复制机制的循环神经网络用于Image Caption]]></title>
    <url>%2F2017%2F10%2F24%2F2017-10-24-LSTMC%2F</url>
    <content type="text"><![CDATA[这是一篇CVPR2017年的论文，使用带有复制机制的LSTM网络进行图像描述，它能够对新物体进行描述。 正文论文地址 摘要&ensp;&ensp;&ensp;图像描述需要很大的训练图像-文本对，然而，在实践中获得足够的训练对的花费是很大的，由此使得最近提出的图像描述模型的建模能力被限制在训练集的语料库之内，也就是很难去描述语料库之外的物体。 &ensp;&ensp;&ensp;在本篇论文中，作者提出了一种带有复制机制的LSTM网络（简称为LSTM-C），它的网络结构是包含复制机制的卷积神经网络加上能够进行图像描述的循环神经网络，它能够描述在训练语料库外的新的物体。 &ensp;&ensp;&ensp;模型先使用物体探测数据集训练对新物体的分类器。然后通过一个带有复制机制的解码器RNN进行描述的生成，它会从新物体中选择合适的词放在描述的合适位置。 引言&ensp;&ensp;&ensp;图像描述是一项跟计算机视觉和自然语言处理相关的研究方向。现有的方法都是基于CNN-RNN机制的。如下图所示： &ensp;&ensp;&ensp;但是这种方法没有对于训练集语料库之外的物体进行探测。但是获得更丰富的训练数据集花费又是很大的。 &ensp;&ensp;&ensp;文章通过使用额外的视觉探测数据集训练物体探测器来消除上面提到的限制。文章提出一种带有复制机制的LSTM网络。复制机制来自于人类语言交流中，通过将输入的一个片段不做任何修改直接放入到输出中，在人类的语言过程中这一机制的体现是死记硬背，人类在组织语言时是通过从记忆中的一些话中拿到一些片段直接放入到要说的话中。作者将复制机制扩展为从另外的语料库中选择单词并将其放在图像的描述中的合适的位置。 &ensp;&ensp;&ensp;文章的主要贡献就是将使用复制机制从外部物体探测数据选取物体和生成模型生成句子有效结合。 相关工作Image Caption&ensp;&ensp;&ensp;图像描述模型可以被分成三种类型： 基于模板 基于搜索 基于语言模型 Novel Object Caption&ensp;&ensp;&ensp;新事物描述是一个最近被广泛关注的新问题，通过使用已有的图像-文本对和没有配对的图像文本数据在RNN-base模型的基础上对训练集语料库以外的事物进行描述。 带复制机制的图像描述&ensp;&ensp;&ensp;文章提出的模型通过将复制机制加入到CNN-RNN图像描述模型的网络中的解码阶段，使得在产生下一个单词的时候不仅需要RNN的解码还需要复制机制的作用。 &ensp;&ensp;&ensp;模型的总体架构如下： 符号定义&ensp;&ensp;&ensp;假设图像\(I\in R^{D_v}\)的描述为\(S={w_1,w_2,…,w_{N_s}},w_t\in R^{D_w}\)，描述包含\(N_s\)个单词，这样句子就能够被描述成\(D_w\times N_s\)的矩阵\(W\equiv [w_1,w_2,…,w_{N_s}]\)。 &ensp;&ensp;&ensp;图像文本对数据的文本字典用\(W_g\)表示。文章使用另外的图像识别数据集训练物体探测器，非成对的物体识别数据集的字典用\(W_c\)，图像I中含有词\(w_i\in W_c\)表示的物体的概率用\(\delta(w_i)\)表示。下图是一般情况下两种字典的关系： &ensp;&ensp;&ensp;至于物体探测器用什么方式进行探测，文章对于单标签的图像数据集（如ImageNet数据集）使用CNN训练分类器，对于多标签的图像数据集（如MSCOCO）使用多实例学习。 图像描述中的序列模型&ensp;&ensp;&ensp;受机器翻译中的encoder-decoder框架的启发，最近的图像描述框架都是基于encoder-decoder的。这种模型首先将图像编码成为固定变量的向量，然后将这个向量解码成目标句子。模型的训练目标是最小化能力函数\(E(I,W)=-log\ Pr(W|I)\)，也就是最大化在给定图像I的条件下生成描述W的概率。 &ensp;&ensp;&ensp;在生成每一个词的时候，通过RNNs会得到前面已经生成的词的模型，所以概率\(Pr(W|I)\)可以被表示成为： $$log\ Pr(W|I)=\sum_{t=1}^{N_s}log\ Pr(w_t|I,w_0,w_1,…,w_{t-1})$$ &ensp;&ensp;&ensp;模型的框架图跟上面的图是一样的。 复制机制&ensp;&ensp;&ensp;复制机制已经被证明在使用外部字典的序列学习（如文本摘要）中很有效(论文)。复制机制来自于人类语言交流中，通过将输入的一个片段不做任何修改直接放入到输出中，在人类的语言过程中这一机制的体现是死记硬背，人类在组织语言时是通过从记忆中的一些话中拿到一些片段直接放入到要说的话中。作者将复制机制扩展为从另外的语料库中选择单词并将其放在图像的描述中的合适的位置。 &ensp;&ensp;&ensp;在第t步的解码时，生成的单词\(w{t+1}\)直接从图像探测数据集中复制过来的概率为： $$Pr_t^c(w_{t+1})=\varphi(w_{t+1}^T M_c)h^t\delta(w_{t+1})$$其中\(M_c\in R^{D_w\times D_h}\)代表文本的转换映射矩阵，\(\varphi\)代表元素级的非线性激活函数，\(h_t\)代表LSTM的上一步解码的输出。\(\delta(w_{t+1})\)代表词\(w_{t+1}\)在当前图像中的概率。 带有复制机制的LSTM&ensp;&ensp;&ensp;将复制机制加入到LSTM中取描述新的物体，在LSTM解码过程中预测下一个单词时同时利用解码生成和复制机制得到下一词的概率。复制机制得到的词可能没在LSTM用来训练的成对数据集中出现，这样使用复制机制就能描述成对数据集之外的新物体。 &ensp;&ensp;&ensp;这样就可以定义最后的概率了，对于第t步的解码过程，得到单词\(w_{t+1}\)的概率为： $$Pr_t(w_{t+1})=\begin{cases}\frac{1}{K}e^{Pr_t^g(w_{t+1})}\quad\quad\quad\quad\quad\quad\quad,w_{t+1}\in W_g\bigcap \overline{W_c}\\\frac{\lambda}{K}e^{Pr_t^g(w_{t+1})}+\frac{1-\lambda}{K}e^{Pr_t^c(w_{t+1})},w_{t+1}\in W_g\bigcap W_c\\\frac{1}{K}e^{Pr_t^c(w_{t+1})}\quad\quad\quad\quad\quad\quad\quad,w_{t+1}\in \overline{W_g}\bigcap W_c\\0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad ,otherwise\end{cases}$$&ensp;&ensp;&ensp;其中\(\lambda\)代表复制机制和LSTM生成对下一次生成起作用的调节比重，K代表softmax的归一化项，\(Pr_t^g(w_{t+1})\)代表LSTM生成词\(w_{t+1}\)的概率。 &ensp;&ensp;&ensp;这个概率的计算就是copy layer的作用。 &ensp;&ensp;&ensp;则损失函数表示为： $$E(I|S)=-\sum_{t=0}^{N_s-1}log\ Pr_t(w_{t+1})$$ &ensp;&ensp;&ensp;N表示训练集中的图像文本对数目，则要解决的问题可以总结为以下最优化问题： $$\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}E(I^{(i)}|S^{(i)})+|\theta|^2$$ 实验数据集 Held-out MSCOCO&ensp;&ensp;&ensp;去除不含bottle，bus,，couch,，microwave,，pizza,，racket， suitcase,和zebra八种物体的数据剩下的数据集。每张图片有五句描述， ImageNet&ensp;&ensp;&ensp;挑选出物体不在MSCOCO数据集中出现的图像进行训练，ImageNet数据集用来训练物体探测器。 实验设置特征和参数设置&ensp;&ensp;&ensp;图像特征使用16层VGG网络的第七层全连接网络特征。单词表示为one-hot和glove。多实例学习采用从VGG网络扩展来的全连接网络(FCN)。 评估标准METEOR&ensp;&ensp;&ensp;参加网上博文：机器翻译中的评价标准 &ensp;&ensp;&ensp;有的时候METEOR分数很高，但是新物体描述能力很弱，于是为了有效的测试模型对新物体的描述能力使用F1-分数。 F1分数&ensp;&ensp;&ensp;下面是从百度百科参考的一张图： &ensp;&ensp;&ensp;为了更好的评估模型对新物体的描述能力，使用了下面两种评估标准： Novel&ensp;&ensp;&ensp;在生成的句子中的新物体占总的新物体数量的比例。 Accuracy&ensp;&ensp;&ensp;描述新物体正确的比例。即对含有新物体图片描述正确的数量与含有此新物体的总的图片数量的比值。 实验分析&ensp;&ensp;&ensp;分析可以得出，文章提出的模型超过其他模型，除了couch和microwave，因为这些东西在物体探测器中不容易判别，所以效果不好。&ensp;&ensp;&ensp;上图是在ImageNet数据集的实验结果。&ensp;&ensp;&ensp;生成的描述对比图： \(\lambda\)的分析&ensp;&ensp;&ensp;在0~0.6很平稳，在0.2时达到最大，大于0.6时下降很快，说明文章提出的复制机制起到了作用。 资料 组会PPT 组会pdf]]></content>
      <categories>
        <category>papper</category>
      </categories>
      <tags>
        <tag>papper</tag>
        <tag>CVPR</tag>
        <tag>ImageCaption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[位置推荐论文阅读：POI2Vec]]></title>
    <url>%2F2017%2F10%2F23%2F2017-10-22-POI2Vec%2F</url>
    <content type="text"><![CDATA[实验室的一些同学最近在搞位置推荐，为了跟他们有共同话题，就跟着他们阅读了一些论文，发现很有意思，一些思路和解决问题的思路历程很值得深思。下面就说一篇刚阅读完的一篇论文：POI2Vec。 正文论文地址 预备知识&ensp;&ensp;&ensp;由于本文的方法是从word2vec创新而来，所以在继续向下读之前务必要熟悉word2vec的原理。word2vec原理 摘要&ensp;&ensp;&ensp;随着社交生活中位置信息数据的增加，关于POI（Point-of-Interest）的研究如用户移动行为建模和POI地点推荐等被广泛研究。最近，推荐用户下一个可能去的POI地点成为一个研究的热点，很多对此的研究论文已经发表。&ensp;&ensp;&ensp;现在大多数已经存在的推荐系统研究注重于给用户推荐POI热点，很少有研究是给地点推荐用户。本文研究的课题是，给定一个POI热点，预测哪些用户将访问这个热点。&ensp;&ensp;&ensp;文章提出一种隐含表示模型：POI2Vec，能够对地理因素进行建模。地理因素已经被证明为影响用户行为的一种重要因素。已有的模型对地理因素的建模不好。&ensp;&ensp;&ensp;一个用户的行为是受他近期的签到行为和他的偏好影响的。但是用户的签到数据很稀疏，而且很难去对POI的连续转换和用户偏好进行建模。在NLP中，word2vector技术用来进行词嵌入，捕捉单词之间的序列语义关系。最近，word2vectpr被用在POI推荐中，但是存在两个问题，一个是不能捕捉POI的地理影响。地理因素在POI推荐中起着重要的作用，第二，它分别对序列影响和用户偏好分别建模，因为访问行为一般同时与序列影响和用户偏好有关，所以同时对两者进行建模会更合理。&ensp;&ensp;&ensp;为了对地理因素进行建模，文章提出一个隐含表示模型，称为POI2Vec，在模型中，每个POI被表示为一个隐低维空间内的一个向量，向量的內积表示POI之间的联系。我们使用层次softmax进行学习隐含向量空间。对于层次softmax来说，很重要的一点是建立一个合适的二叉树。这里，不使用word2vector中使用的哈弗曼树，文章提出一种二叉树建立方法，能够对地理因素进行建模。我们层次化的对POI进行分割得到POI的一些不同区域。因为一个POI可能会对周围的POI进行影响，所以我们将一个POI放入多个周围的区域中。在二叉树中，一个POI可能会出现不只一次，这样能够捕捉到POI跟其他POI的丰富的关系。 相关工作&ensp;&ensp;&ensp;已有方法： Collaborative Filtering(协同过滤) Factorization models(分解模型) Markov Chain(马尔科夫) Hidden Markov Chain(隐马尔科夫) Metric Embedding(矩阵嵌入)&ensp;&ensp;&ensp;文章的工作与现有工作至少有两点不同： 从一个不同的视角考虑一个新的任务：找到一个POI在接下来的几个小时内的潜在用户。 提出的模型同时学习序列影响和用户偏好。问题定义&ensp;&ensp;&ensp;用户集合表示为U，POI集合表示为L，每个POI地点l的地理表示为\(\langle l^{Lat},l^{Lon}\rangle\)。用H表示用户的签到数据集，每次签到(u,l,t)表示用户u在时间t时访问地点l。&ensp;&ensp;&ensp;下面给出问题的定义：&ensp;&ensp;&ensp;给定用户集合U和POI集合L，并指定时间t和时间阈值\(\tau\)，预测在时间范围\((t,t+\tau)\)内有哪些用户会访问地点l。POI2Vec 表示模型POI2Vec对连续序列影响建模&ensp;&ensp;&ensp;先用POI2Vec对POI地点的序列影响进行建模。&ensp;&ensp;&ensp;POI2Vec思路很简单，就是将word2vec用于POI，动机是：作者观察到签到数据中的POI分布和文本中word的分布很类似。&ensp;&ensp;&ensp;给定用户u和他当前的地点\(l_c^u\)，用\(C(l_c^u)\)代表用户u在访问\(l_c^u\)之前的一定时间范围内访问的所有地点，即\(C(l_c^u)=\lbrace l_i^u;0&lt;\Delta(l_i^u,l_c^u)&lt;\tau\rbrace\)。\(\Delta(l_i^u,l_c^u)\)代表\(l_i^u\)和\(l_c^u\)之间的时间长度。目标是得到给定其POI地点的访问上下文之后，预测会访问当前POI的概率。&ensp;&ensp;&ensp;将每个POI表示成一个向量\(w(l)\in R^D\)，采用CBOW(Continuous Bag-of-Words,连续词袋模型)对其进行建模。&ensp;&ensp;&ensp;则概率Pr(l|C(l))为：$$Pr(l|C(l))=\frac{e^{(w(l)\cdot\phi(C(l)))}}{\sum_{l_i\in L}e^{w(l_i)\cdot\phi(C(l))}}$$&ensp;&ensp;&ensp;其中：$$\phi(C(l))=\sum_{l_c\in C(l)}w(l_c)$$&ensp;&ensp;&ensp;看到上面的式子是不是感觉很熟悉，其实它就是softmax的POI版本（仔细思考这句话）。&ensp;&ensp;&ensp;同word2vec一样当POI数量很大时计算上面的式子的时间复杂度太高。所以需要优化，优化一般使用层次softmax。层次softmax的关键步骤是建立二叉树，在word2vec中是通过单词的频率建立哈弗曼树，而在此文章中，为了考虑POI的地理因素，作者提出了一种创建二叉树的方法，具体看下一节。加入地理因素&ensp;&ensp;&ensp;地理因素在POI推荐中起着重要作用，那么怎么在上面提到的层次softmax建二叉树过程中加入地理因素呢？一个矩形每次将上次得到的区域二等分，反复分割就得到了\(2^n\)个矩形，这样的分割方式可以形成一个二叉树。 先区域分割，分割成一个一个的小区域，注意在分区域时，每次等分，最后分割成一个一个同样大小的矩形，矩形的边长记为\(\theta\)，这样每一个POI地点都属于一个区域。在同一个区域内的POI点被视为很接近。但是有的时候不同区域的POI在距离上也是相近的，那么怎么办呢？使得一个POI可以属于多个区域，以一个POI为中心，做一个\(\theta\times\theta\)的矩形，如果此矩形与其他区域重叠，则将此POI赋于到这些重叠的区域中。具体如下图所示&ensp;&ensp;&ensp;如图将区域分成\(R_{00},R_{01},R_{10},R_{11}\)四个区域，那么\(l_1\)属于\(R_{01}\)，\(l_2\)属于\(R_{11}\)，\(l_4\)属于\(R_{00}\)，\(l_3\)属于\(R_{10}\)，但是可以从图中明显可以看出\(l_1和l_2\)也是很近的，这个时候就需要将一个POI点赋于多个区域了，按照上面的方法，分别以\(l_1和l_2\)为中心画出长度为\(\theta\)的矩形（图中红色边框的矩形）。可以看到以\(l_1和l_2\)为中心的矩形跟区域\(R_{01}和R_{11}\)都有重叠，所以将\(l_1\)加入到\(R_{11}\)区域中，将\(l_2\)加入到\(R_{01}\)区域中，其他的点类似。&ensp;&ensp;&ensp;很明显可以发现，每个POI可能属于的区域数目为1、2或4个。&ensp;&ensp;&ensp;为了后面创建哈弗曼树，文章定义了一个POI属于一个区域的概率：以POI为中心的矩形与区域重叠部分的面积占区域总面积的比例。为了后续使用，将这个概率记为：\(Pr(R_i)^l\)，\(R_i\)代表编号为i的区域。&ensp;&ensp;&ensp;然后就是创建层次softmax的二叉树了，先使用区域创建顶层二叉树，如下图蓝线上面的树，可以发现这是一颗完全二叉树，树的叶子结点就是最小的区域。其实这棵树就代表区域的分割过程。然后以创建好的二叉树的叶子结点为根分别创建哈弗曼树，哈弗曼树创建的依据在此POI上用户们的签到频率。&ensp;&ensp;&ensp;总结一下，本文提出的POI2Vec模型有两个优点，一个是它考虑了地理因素的影响，在同一个区域的POI在地理上是相近的，二是在创建的二叉树中，每个POI可能有多条路径，这样会使得模型效果更好，因为一个POI跟其他POI产生很多关系会被充分捕捉。概率估计&ensp;&ensp;&ensp;现在来定义一下各个结果概率：&ensp;&ensp;&ensp;跟在word2vec中一样，在二叉树的非叶子结点上是可训练的参数，最终从跟到树的叶子计算的概率就是在当前上下文下访问l的概率：$$path=(b_0^l,b_1^l,…,b_n^l)\\Pr(l|C(l))^{path}=\prod_{b_i^l\in path}Pr(b_i^l|\phi(C(l)))\\Pr(b_i|\phi(C(l))=sigmoid(\psi(b_i^l)\phi(C(l)))\\$$&ensp;&ensp;&ensp;其中\(\psi(b_i^l)\in R^D\)代表每个结点上的参数向量。&ensp;&ensp;&ensp;但是一个POI在二叉树中可能有多条路径，因此需要将它们结合起来计算概率：$$Pr(l|C(l))=\prod_{path_k\in P(l)}Pr(path_k)\cdot Pr(l|C(l))^{path_k}$$&ensp;&ensp;&ensp;其中\(P(l)\)代表POI点l在二叉树中所有路径的集合。&ensp;&ensp;&ensp;如果\(path_k\)为l的路径，而l属于区域\(R_x\)，那么\(Pr(path_k)=Pr(R_x)^l\)，而\(Pr(R_x)^l\)就是POI属于某一个区域的概率，上面定义过它的计算。参数学习&ensp;&ensp;&ensp;学习目标为最大化得到所有POI点的后验概率。$$\Theta=arg\min_{\Theta}\prod_{(l,C(l))\in H}Pr(l|C(l))$$&ensp;&ensp;&ensp;其中\(\Theta\)是一些可学习的参数。将POI2Vec用于建模用户偏好&ensp;&ensp;&ensp;前面已经使用POI2Vec对连续POI序列预测下一个POI问题进行建模，现在再加入用户偏好。&ensp;&ensp;&ensp;跟前面的POI概率定义相似，用户u访问POI点l的概率为：$$Pr(l|u)=\frac{e^{w(l)\cdot x(u)}}{Z(u)}$$&ensp;&ensp;&ensp;其中\(Z(u)=\sum_{l_i\in L}e^{(w(l_i)\cdot x(u))}\)，同样的Pr(l|u)也可以使用上面创建好的层次softmax树计算，而且可以使用同一棵树训练计算（这样就同时对连续POI序列和用户偏好进行建模了）。&ensp;&ensp;&ensp;如果在计算时既有连续POI上下文也有用户偏好数据，并假设它们相互独立，那么：$$Pr(l|u,C(l))=Pr(l|u)xPr(l|C(l))$$&ensp;&ensp;&ensp;综上所述，用户在某一时间访问某一地点的概率为：$$Pr(u,l,t)=\begin{cases}Pr(l|u,C(l))\quad if\ C(l)\ exists\\Pr(l|u)\quad\quad\quad\quad otherwise\end{cases}$$&ensp;&ensp;&ensp;目标函数为：$$\Theta=arg\max_{\Theta}\prod_{(u,l,t)\in H}Pr(u,l,t)$$预测潜在访问用户&ensp;&ensp;&ensp;用户u在不久后访问l的似然概率为\(F(x(u)\cdot w(l),w(l_c)\cdot w(l))\)，\(x(u)\cdot w(l)\)反映用户偏好，\(w(l_c)\cdot w(l)\)反映序列影响。\(F\)是聚合函数，一般聚合函数可以使用Max函数或者Sum函数。&ensp;&ensp;&ensp;对于每个用户计算他的分数：$$s(u,l)=\begin{cases}F((x(u)\cdot w(l),w(l_c)\cdot w(l)))\quad with\ recent\ positions\\x(u)\cdot w(l)\quad\quad\quad\quad\quad\quad\quad\quad\quad otherwise\end{cases}$$&ensp;&ensp;&ensp;根据分数找到Top-k个用户即可。实验&ensp;&ensp;&ensp;实验结果： FMC: the factorized Markov chain model ME: Metric Embedding model NS: the negative sampling technique for word2vec HS: conventional hierarchical softmax with Huffman tree FPMC: factorizing personalized Markov chains PRME: personalized ranking metric embedding CWRAP: explore the context of locations to model user preference U: only utilize user preference to predict potential visitors. URP: only consider users with recentpositions. MAX: Max aggregation function is used SUM: utilize the Sum aggregation function]]></content>
      <categories>
        <category>papper</category>
      </categories>
      <tags>
        <tag>papper</tag>
        <tag>word2vec</tag>
        <tag>AAAI2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词嵌入以及word2vec]]></title>
    <url>%2F2017%2F10%2F22%2F2017-10-20-wordembedding%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp; ##]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>word2vec</tag>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR 2017 Image Caption论文阅读笔记]]></title>
    <url>%2F2017%2F10%2F16%2F2017-10-16-2017cvprimagecaption%2F</url>
    <content type="text"><![CDATA[正文 &ensp;&ensp;&ensp;最近在看cvpr2017的papper，看到了一些有关图像描述的好文章，读了一下，做个笔记。 Attend to You: Personalized Image Captioningwith Context Sequence Memory Networks&ensp;&ensp;&ensp;亮点在于亮点： 个性化图像描述 使用上下文序列记忆网络进行学习，而非RNNs，LSTMs &ensp;&ensp;&ensp;文章指出了两个应用，一个是在社交网络中，人们拍了照片要发图的时候，能够自动生成描述，二是自动生成标签。&ensp;&ensp;&ensp;文章使用一个记忆仓库存储多种类型的上下文信息，每次将捕捉到的单词加入到记忆中，保留长时间信息，随后使用记忆生成下一个词。这样避免了因梯度消失而引起的信息损失。随后使用CNN记忆架构学习周围信息进行上下文理解。 Context Sequence Memory Network(CSMN)&ensp;&ensp;&ensp;CSMN网络架构如下： &ensp;&ensp;&ensp;模型建立了一个记忆仓库，记忆仓库中含有三种类型的记忆： 图像记忆 用户偏好记忆 序列历史记忆 &ensp;&ensp;&ensp;如上图中的a图，将图像先用CNN处理，取CNN中的某一层数据作为图像特征，然后将图像特征乘以一个向量W即可得到最终图像记忆，向量W是可训练的参数。而用户偏好的计算是使用TF-IDF算法得到用户的前TOP-K个单词。然后用one-hot向量表示这些单词，再乘以一个向量W即得到的向量就是用户偏好记忆。同样向量W是可训练的参数。 &ensp;&ensp;&ensp;而序列历史记忆则是将前面预测到的单词连接在一起即可。如图中\(y_1,…,y_{t-1}\)就是序列历史记忆。每次生成新单词后会将其加入到序列历史记忆中。 &ensp;&ensp;&ensp;可以发现每种记忆一共训练了两份，分别以a和c为下标。每份记忆训练的W也是不同的，所以最终得到的两种记忆有可能是不同的（因为训练得到的W是不同的）。 &ensp;&ensp;&ensp;那么训练的两份记忆是用来干啥的呢？下面一边介绍模型的预测流程一边说明： &ensp;&ensp;&ensp;先将上一次预测到的单词经过一个线性变换得到一个查询\(q_t\)，然后使用使用查询与第一份记忆做点乘并做softmax操作，这样就得到了记忆注意力向量\(p_t\)，\(p_t\)的作用是选择哪些记忆会对下一个单词的产生起效果。将\(p_t\)与第二份记忆相乘即可得到能对下一个单词的产生起作用的记忆。随后对得到的记忆进行卷积操作，通过使用不同的卷积核，我们能够通过卷积得到不同的记忆单元的组合表示。随后通过线性变换和softmax即可得到下一个单词的概率。每次产生新的单词会更新序列历史记忆。 &ensp;&ensp;&ensp;对于hashtag的预测，我们只需要去掉word output memory即可。 为什么记忆CNN有用&ensp;&ensp;&ensp;尽管卷积记忆网络不能对结构化的时序数据建模，除非加入时序信息。但是文章提出的网络有所不同，序列记忆能够使得CNN捕捉时序信息。用户偏好记忆可以使得CNN能够正确的捕捉上下文单词的重要性。例如用户当前相关的单词有fashion、street和landscape，如果fashion在用户偏好记忆的最上面，而street在与fashion相关的词旁边，那么用户的偏好可以被认为是：street fashion，而如果landscape在最上面，street在landscape相关词周围，那么用户偏好被认为是landscape。如果不使用记忆CNN，那么street的这两种用法是很难区分的。 实验 ###]]></content>
      <categories>
        <category>papper</category>
      </categories>
      <tags>
        <tag>papper</tag>
        <tag>Image Caption</tag>
        <tag>CVPR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown中使用公式]]></title>
    <url>%2F2017%2F10%2F13%2F2017-10-13-markdownlatex%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;markdown作为一种简洁方便的写作方式，广受博主们的喜爱，但是有的时候需要向markdown中加入公式，该怎么办呢？&ensp;&ensp;&ensp;我一般使用两种方式，一种是使用在线公式编辑器编辑公式保存为图片，然后将图片上传到github等平台，然后在markdown中关联即可。 &ensp;&ensp;&ensp;另一种是使用MathJax引擎。 &ensp;&ensp;&ensp;使用方式很简单： 将下面代码加入到markdown文件中，作用是在当前文件引入MathJax引擎。 1&lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"&gt;&lt;/script&gt; &ensp;&ensp;&ensp;然后再使用latex方式编辑公式即可，用$$公式$$表示行间公式，本来latex中使用\(公式\)表示行内公式，但因为Markdown中\是转义字符，所以在Markdown中输入行内公式使用\\(公式\\) 问题使用大括号12345$$\begin&#123;cases&#125;\mu=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i&#125;x\_i\\\\\sigma^2=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i&#125;(x\_i-\mu)^2\end&#123;cases&#125;$$ $$\begin{cases}\mu=\frac{1}{n}\sum_{i}x_i\\\sigma^2=\frac{1}{n}\sum_{i}(x_i-\mu)^2\end{cases}$$]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto-encoder by forest]]></title>
    <url>%2F2017%2F10%2F11%2F2017-10-10-eforest%2F</url>
    <content type="text"><![CDATA[国内大神周志华即前段时间发表的媲美（号称)深度神经网络的深度森林之后又发表了用森林实现自动编码器。作为国内机器学习界首屈一指的大牛，不管其提出的模型效果怎样，阅读其文章肯定能使我们学习一些有用的东西。怀着这样的心情，我阅读了这篇文章。 正文论文原文 摘要&ensp;&ensp;&ensp;自动编码器是一种很重要的算法，一般使用深度神经网络实现。文章提出一种全新实现自动编码其的方法：使用森林实现。文章提出一种方法使用决策树的一些信息完成编码和解码操作。实验显示，eforest有低的重建误差，而且效率高。 引言&ensp;&ensp;&ensp;自动编码器是一类将输入数据映射到隐藏空间，然后再映射到原始空间的模型，它使用重建误差作为目标函数。自动编码器分为两个过程：编码和解码。编码过程将原始数据映射到隐藏空间，解码数据将数据从隐藏空间映射到原始数据空间。传统实现这两个过程的方式是使用神经网络。&ensp;&ensp;&ensp;集成学习是一种强大的学习方法。它通过使用训练多个学习期，然后将这些学习器结合起来完成任务。&ensp;&ensp;&ensp;决策树集成方法是集成学习中一种常用方法，bagging和boosting都通常采用决策树作为子学习器。随机森林是bagging的变种，GBDT是boosting的变种。GBDT有很多有效的实现，如XGBOOST。&ensp;&ensp;&ensp;文章提出了一种编码森林（EncoderForest），通过一个集成树模型进行前向编码和反向解码，而且可以使用监督或者无监督训练。实验显示EncoderFroest有如下优点: 准确： 它的实验重建误差比使用MLP和CNN的自动编码器低。 有效： efroest在一个单一KNL(多核CPU)上运行比CNN-Base自动编码器在一个Titan-X GPU上运行还快。 容错率：训练好的模型能够正常运行即使模型部分损坏。 重利用： 在同一个领域下，使用一个数据集训练的模型可以直接应用到另一个数据集下。模型&ensp;&ensp;&ensp;自动编码器有连个基本的功能，编码和解码。对于一个森林来说，编码并不困难，因为至少其叶节点信息可以被看做一种编码，甚至可以说，结点集合的一个子集或者路径的分支都能够为编码提供更多的信息。编码过程&ensp;&ensp;&ensp;首先我们提出模型的编码过程。对于给定的一个训练过的有T棵树的集成树模型（也可以是空的森林，编码过程即使森林形成过程），前向编码过程将输入数据送到每棵树的根节点，并计算每棵树，得到其所属的叶节点，最后返回一个T维向量，这个T维向量的每一项是每棵树中求到的上述叶节点在树中的编号。注意，算法跟决策树的分割规则无关。只需要是T棵树即可。&ensp;&ensp;&ensp;算法伪代码如下：解码过程&ensp;&ensp;&ensp;一般来说，决策树都是用来前向预测，将数据计算从树的根节点到叶子结点，但是其反向重建是未定义的。&ensp;&ensp;&ensp;下面通过一个小例子来探索解码过程。&ensp;&ensp;&ensp;假设我们正在解决一个二分类问题，数据有三个属性，第一个和第二个属性是数值型属性，第三个属性是布尔型属性（取值为YES, NO），第四个属性是一个三值属性，取值可以是RED，BLUE，GREEN。给定一个实例x，xi代表x的第i个属性值。&ensp;&ensp;&ensp;现在我们假设我们在编码过程形成一个森林，如下图：&ensp;&ensp;&ensp;现在我们只知道，实例x落在每棵树的哪个结点，上图中的红色结点，我们的目标是重构实例x。&ensp;&ensp;&ensp;文章提出了一种简洁有效的反向重建方法。&ensp;&ensp;&ensp;首先，在树中的每个叶子结点对应于一条唯一的从根到叶子的路径。在上面的图中已经用红色标出这样的路径。&ensp;&ensp;&ensp;然后，每条路径都会对应一条符号规则，所以我们就得到了n条（树的数目）符号规则：&ensp;&ensp;&ensp;然后，我们可以根据上面的规则推出MCR(最大完备规则)，最大完备规则的意思是，在规则中的每一个约束的范围不能再扩大。如果扩大，则会产生冲突。&ensp;&ensp;&ensp;例如，由上面的规则集我们可以得到MCR:&ensp;&ensp;&ensp;可以发现MCR中每一项都不能再扩大范围了，如果扩大会与上面的规则集中的规则的某一项产生冲突。&ensp;&ensp;&ensp;那么显然，原始的数据肯定落在有MCR定义的范围内。&ensp;&ensp;&ensp;计算完MCR之后，就可以根据MCR重构原始样本了，目录型属性如上面的第三和第四属性只需要根据MCR中的指定取即可，而数值型属性则可以根据MCR中的范围取一个特殊值即可（中位数、均值、或者最大最小值）。&ensp;&ensp;&ensp;计算MCR的伪代码如下：&ensp;&ensp;&ensp;根据上面的描述，现在可以总结一下解码的过程。给定T棵树和编码完的T维向量:&ensp;&ensp;&ensp;首先根据编码完的T维向量从树中得到T个决策规则，再根据这些规则得到MCR，再根据MCR重构得到x，算法如下：实验]]></content>
      <categories>
        <category>DL</category>
      </categories>
      <tags>
        <tag>ensemble learning</tag>
        <tag>Auto-encoder</tag>
        <tag>forest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式题目]]></title>
    <url>%2F2017%2F09%2F26%2F2017-09-26-regexptest%2F</url>
    <content type="text"><![CDATA[最近刚学完正则表达式，于是就找了一些练习题做了一下，下面收集一些正则表达式的好题目进行练习。 abba&ensp;&ensp;&ensp;题目： &ensp;&ensp;&ensp;这个题目很有意思，仔细观察发现，题目的意思就是匹配不含长度为4回文的串。如abba、inflammableness分别含有长度为4的回文abba和amma，所以不能匹配。那么怎么写呢？想想咱们小时候做数学题的时候，遇到否定的问题一般把它变为肯定再做，这里也是一样，先写出能够匹配带4-回文的串的模式，.*(.)(.)\2\1.*,然后呢就要加否定了，那么怎么正则表达式中怎么加否定呢？很简单，环视(?!)，想想当(?!exp)放在前面是什么作用来？排除数据！这不就是否定的功能吗？那么问题就简单了，得到模式(?!.*(.)(.)\2\1.*).*。而对与原题目来说，对于左边匹配只需能够匹配串的一部分即可，对于右边只需能够得到匹配失败即可。所以模式还能缩短，具体过程我就不说了，只给出最终模式^(?!(.)+\1)|fu。 Prime &ensp;&ensp;&ensp;这个题目很有意思啊，仔细观察发现，其实意思就是匹配长度为素数的串，不匹配长度为合数的串，怎么做呢？还是沿用上一题的方法，先写匹配合数的，再加否定，为什么呢？因为匹配合数的好写啊，小时候的数学知识告诉我们，合数简单的来说就是有大于1小于自身的因子，也就是说，存在m大于1且小于n，使得n能整除m那么n就是合数。合数是啥搞明白了，那么怎么写正则模式呢？分组！先写m的匹配(..+)，这个模式代表匹配大于等于2长度的串。再写n能够整除m，n能整除m的意思也就是说很多个m合在一起可以组成n，也就是(..+)\1+。这里一个技巧就是\1分组后面也是可以加+和*的。这样最终模式也就能够写出了^(?!(..+)\1+$).*$。同前面一样，对于题目的特殊性，题目的最终答案可以短一些^(?!(..+)\1+$)]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式（全）]]></title>
    <url>%2F2017%2F09%2F23%2F2017-08-23-regexp2%2F</url>
    <content type="text"><![CDATA[正则表达式是一个非常强大的工具，掌握了它会极大的提高你的工作效率。下面就总结一下正则表达式的一些基本用法，和它的进阶用法，如环视、平衡组等 正文常用匹配 . 用来匹配不包含换行的任意字符 \s 匹配空白符（空格、tab、换行）&ensp;&ensp;&ensp;\S为\s的反义 * 匹配0个或多个字符 + 匹配一个或多个 \\ 转义字符 [] 单字符取一个，比如[a-z]会匹配所有的小写字母中的一个 []中的^代表非的意思，如[^a-z]匹配一个非小写字母的字符 如上所示-表示范围，如[0-9]匹配从0到9的所有数字中的一个 如果要包含-字符，可以给它加上转义[\-] []里面的特殊符有五个：[]-\^，其他字符都是普通字符，包括*.?等。 ^ 字符串开始&ensp;&ensp;&ensp;用^可以匹配所有的行（包括空行） $ 字符串的结束 {start, end} 前一个匹配的循环次数。其中start和end都是十进制整数&ensp;&ensp;&ensp;如\(ab\)\{3}匹配ababab，其中()和需要转义，前后括号都需要转，而{}只需要转前括号即可，正则表达式引擎能够自动识别。 ? 匹配0个或1个字符 表示使用非贪婪模式&ensp;&ensp;&ensp;那么什么是贪婪模式和非贪婪模式呢？贪婪模式就是在符合当前匹配的前提下，匹配尽量多的字符，而非贪婪模式则是匹配尽量少的字符。默认情况下是贪婪模式。&ensp;&ensp;&ensp;例如对于字符串&lt;a&gt;hello&lt;/a&gt;&lt;a&gt;world&lt;/a&gt;使用模式&lt;a&gt;.*&lt;/a&gt;进行匹配的话，匹配结果就是&lt;a&gt;hello&lt;/a&gt;&lt;a&gt;world&lt;/a&gt;，这就是贪婪模式，虽然到第一个&lt;/a&gt;的时候就已经能够完成匹配了，但是因为.*模式能够匹配任意长度的字符串，而且后面还能找到&lt;/a&gt;，所有贪婪模式下会继续匹配指导找不到&lt;/a&gt;。而使用非贪婪模式&lt;a&gt;.*?&lt;/a&gt;匹配的结果就是&lt;a&gt;hello&lt;/a&gt;和&lt;a&gt;world&lt;/a&gt;。 | 或 如模式ab|bc|de匹配ab或者bc或者de （) 分组&ensp;&ensp;&ensp;有时候啊，需要取出匹配中的一部分数据，这个时候就需要用到分组了。匹配中的()用来标示分组，分组从左到右用左括号进行对分组计数（非捕获和环视的左括号不能数），从1开始计数，如模式(ab)dd(ef)中ab就是分组1，而ef就是分组2。&ensp;&ensp;&ensp;使用分组时，有两种方式：\group和$group \group 模式本身使用子模式使用，如(ab)\1\1 $group 替换时调用模式中分组内容时使用&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;如将adbc中的bc匹配为de时，先使用(ad)bc模式进行匹配，然后用模式$1de进行替换。 (?:) 非捕获组&ensp;&ensp;&ensp;有的时候()只是用来对字符进行分界使用，而不是用来作为子模式，这个时候就会用到非捕获组，如模式(ab|cd|de)def(zz)，使用这个模式的时候想要得到后面的一个分组zz，而前面的()只是用来对字符进行分界。但是这时zz的这个分组数为2，如果对前面的()进行非捕获(?:ab|cd|de)def(zz)，那么zz的分组数变为1。 高级概念 贪婪模式和非贪婪模式&ensp;&ensp;&ensp;上面已经说明 环视&ensp;&ensp;&ensp;环视也成为零宽断言或断言。其作用是加入一些条件，使得在找到满足的匹配后需要满足这些条件才能满足最后的匹配。主要有下面用法： (?&lt;=exp) 匹配前面是exp的 (?&lt;!exp) 匹配前面不是exp的 (?=exp) 匹配后面是exp的 (?!exp) 匹配后面不是exp的如： (?&lt;=\d)abc 匹配前面是数字的，1abc匹配，dabc不匹配 (?&lt;!\d)abc 匹配前面不是是数字的，dabc匹配，1abc不匹配 abc(?=\d) 匹配后面是数字的，abc1匹配，abcd不匹配 abc(?!\d) 匹配后面不是数字的，abcd匹配，abc1不匹配&ensp;&ensp;&ensp;有一点要注意，当(?!exp)模式放在后面时代表匹配后面不是exp的，而它也可以放在匹配模式的前面，这是它代表：对后面数据的一种过滤，如模式(?!\d).*意思是在所有匹配.*的串里，排除\d匹配的串。这个作用很实用，常用来过滤数据。&ensp;&ensp;&ensp;如要匹配非纯数字和非纯字母的数据，匹配所有数据的模式是.*，匹配纯数字的模式是^[\d]+$，匹配纯字母的模式是^[a-zA-Z]+$，使用?!exp模式排除得到最终模式(?!^[\d]+$)(?!^[a-zA-Z]+$).*。这里注意排除数据时，不只能够使用(?!exp)，其他的环视表达式放在非正常位置时都可以用来排除数据，如[A-Z](?&lt;!B)表示[A-Z]范围排除B。&ensp;&ensp;&ensp;还有就是环视又成为零宽断言，这里的零宽代表什么意思呢，意思就是(..)内部匹配到的数据不会放到匹配结果中，如前面的模式(?=\d)abc匹配后面是1的abc，如与abc1是匹配的，但是匹配结果是abc，不包括1。零宽还有另一个意思，对于原字符串，用环视匹配过的部分可以再次利用进行匹配（如果不是环视的话，匹配过的部分是不能再次匹配的），如对于(?!exp)排除数据方法的使用，前面例子中，整个匹配过程中，正则表达式一共走了3次字符串匹配，第一次匹配不全部是字母，第二次匹配不全部是数字，第三次匹配全部是字母数字组合，这三次匹配使用的都是一个源字符串。 平衡组&ensp;&ensp;&ensp;当要匹配的内容有嵌套时，就需要使用平衡组了，它的语法我的理解就是模拟了一个栈完成括号匹配： (?&#39;group&#39;) 将当前组捕获的内容命名为group，并压入栈中。 (?&#39;-group&#39;) 从栈中弹出最后压入的名为group的内容，如果栈为空，则匹配失败 (?(group)yes|no) 如果栈中存在名为group的内容，则执行yes部分，否则执行no部分 (?!) 零宽负向先行断言，总是匹配失败&ensp;&ensp;&ensp;下面是一个平衡组使用的例子：1234567891011121314&lt; #最外层的左括号 [^&lt;&gt;]* #最外层的左括号后面的不是括号的内容 ( ( (?'Open'&lt;) #碰到了左括号，在黑板上写一个"Open" [^&lt;&gt;]* #匹配左括号后面的不是括号的内容 )+ ( (?'-Open'&gt;) #碰到了右括号，擦掉一个"Open" [^&lt;&gt;]* #匹配右括号后面不是括号的内容 )+ )* (?(Open)(?!)) #在遇到最外层的右括号时，判断黑板上还有没有没擦掉的"Open"；如果还有，则匹配失败&gt; #最外层的右括号 模式修饰符 i 忽略大小写 ((?i)[A-Z]+)c模式匹配abcABC得到的结果是abc和ABC 总结&ensp;&ensp;&ensp;正则表达式的在书写时可以分为三步: 分析要解析的数据 用通配符匹配去掉一些不需要的数据 使用分组等得到想要的数据 学习链接 正则表达习题 正则表达式测试 平衡组 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim模式]]></title>
    <url>%2F2017%2F09%2F23%2F2017-08-10-vimmatch%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;对于vim模式匹配是一个不可或缺的部分，熟练的掌握它，能够使你的效率，事半功倍。 模式匹配和原义匹配&ensp;&ensp;&ensp;如过默认情况下，(){}[]在vim正则表达式中使用的时候就只能代表其原意义，但是有的时候需要使用他们的特殊意义，如()的分组功能。这个时候就要使用\\转义了，\(\)、\{}、\[\]等。但是这样书写麻烦而且不易读，怎么办呢？vim设置了magic，其作用就是指导哪些字符需要转义，哪些不需要。&ensp;&ensp;&ensp;magic(\m)：除了$ . * ^之外其他元字符都要转义。nomagic(\M)：除了$ ^之外其他元字符都要转义。也可以在正则表达式中通过\m \M开关临时切换。\m后面的正则表达式会按照magic处理，\M后面的正则表达式按照nomagic处理， 而忽略实际的magic设置。 :set ignorecase 设置忽略查找时大小写 \c 在每次查找时加入它会使本次查找忽略大小写 \C 在每次查找时加入它会使本次查找不忽略大小写&ensp;&ensp;&ensp;\c和\C在一条匹配中不管放在匹配的哪个位置都会对整条匹配起作用 \V very nomagic 任何字符都需要转义 \v very magic 任何字符都不需要转义 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>vim</tag>
        <tag>宏录制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+github添加域名解析]]></title>
    <url>%2F2017%2F09%2F21%2F2017-08-30-hexowww%2F</url>
    <content type="text"><![CDATA[hexo+github添加域名解析 本地配置&ensp;&ensp;&ensp;在博客目录下的source下创建CNAME文件，并在里面加入你的域名。 域名配置&ensp;&ensp;&ensp;我这里是使用的godaddy的域名，配置如下:]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>域名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下制作动图]]></title>
    <url>%2F2017%2F09%2F19%2F2017-09-19-ffmpeggif%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;动图是现今互联网上一种表示信息的重要形式，它可以用来表示工作流程、产品功能等。我在写markdown时常遇到一些描述操作流程的问题，这时使用单个图片是不足够的，而使用视频又面临加载慢的问题，很自然的就想到了用动图来解决这个问题。 &ensp;&ensp;&ensp;废话少说，下面就讨论如何在linux制作动图。 录制视频&ensp;&ensp;&ensp;动图是一个缩小版的的视频，所以要先录制视频，然后再转换为动图。 &ensp;&ensp;&ensp;我找到的比较简单使用的linux录制视频软件是SimpleScreenRecorder，具体安装如下： Arch Linux 1sudo pacman -S SimpleScreenRecorder Ubuntu 123sudo add-apt-repository ppa:maarten-baert/simplescreenrecordersudo apt-get updatesudo apt-get install simplescreenrecorder &ensp;&ensp;&ensp;使用: 转换为gif&ensp;&ensp;&ensp;录制完视频后就要将视频转换为gif格式了，有很多转换工具，这里使用的是ffmpeg。 &ensp;&ensp;&ensp;安装： Arch Linux 1sudo pacman -S ffmpeg Ubuntu 1sudo apt-get install ffmpeg &ensp;&ensp;&ensp;使用下面命令将录制好的视频转换为gif动图: ffmpeg -ss 00:00:20 -i input.flv -to 10 -r 10 -vf scale=200:-1 output.gif &ensp;&ensp;&ensp;参数： -ss 指定从视频的哪个地方开始转换 -i 后面跟要操作的那个视频文件 -to 指定到视频的哪个位置为止不再转换 -r 帧速率，可以增大这个值输出更画质更优的GIF文件 -vf 图形筛选器，GIF的缩放大小 合并多张动图&ensp;&ensp;&ensp;有的时候需要将多张动图合并为一张，该怎么做呢？很简单，使用convert命令。 &ensp;&ensp;&ensp;参数： -delay value 每张图片直接的播放时间间隔 -loop iterations 循环次数，0代表无限循环 &ensp;&ensp;&ensp;将要合并的多张动图放到一个文件夹中，然后执行： convert -delay 100 -loop 0 *.gif output.gif 从图像序列生成动图&ensp;&ensp;&ensp;有的时候我们有的是多张图片的序列，怎么将这些序列转化为动图呢？跟上面的命令相似，将图片序列放入到一个文件夹中，然后执行： convert -delay 100 -loop 0 *,gpg output.gif 查看图像的大小&ensp;&ensp;&ensp;convert还有一个应用就是查看图像的大小： convert a.jpg -print &quot;Size: %wx%h\n&quot; /dev/null 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>动图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo配置打赏功能]]></title>
    <url>%2F2017%2F09%2F19%2F2017-09-19-hexoexceptional%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;现在是互联网的世界，越来越多的互联网文章出现，人们的忙碌使得他们往往借助互联网寻求解决问题的方法。碎片化文字和博客的出现使得人们解决问题的方式更加简单。由此，往往对于一些优秀的博客，我们常常希望给他一定的鼓励以希望他继续创作，基于此，打赏功能应运而生。 &ensp;&ensp;&ensp;本人使用的是hexo+next构建博客，因此下面就讲解一下在hexo next下进行打赏功能的配置。 生成二维码&ensp;&ensp;&ensp;使用微信和支付宝生成相应的收款码分别保存为wechat.png和alipay.png &ensp;&ensp;&ensp;将生成的二维码照片放入到themes下的hexo-next主题目录下的source/images/文件夹下 配置&ensp;&ensp;&ensp;进入next主题目录，编辑主题的_config.yml，加入如下代码（如果已经存在，则修改成跟如下代码一致）: 123reward_comment: 您的支持将鼓励我继续创作！wechatpay: /images/wechat.png$alipay: /images/alipay.png$ 取消闪烁效果&ensp;&ensp;&ensp;如下图，当将鼠标放到二维码图片上时，图片下面的字会闪烁，在我看来，这不是一个好的效果，于是决定取消它（如果你喜欢的话可以不取消）。 &ensp;&ensp;&ensp;如何取消呢？只需将主题目录下的source/css/_common/components/post/post-reward.styl文件中的下面代码注释掉即可。 12345678910#wechat:hover p&#123;$ animation: roll 0.1s infinite linear;$ -webkit-animation: roll 0.1s infinite linear;$ -moz-animation: roll 0.1s infinite linear;$&#125;$#alipay:hover p&#123;$ animation: roll 0.1s infinite linear;$ -webkit-animation: roll 0.1s infinite linear;$ -moz-animation: roll 0.1s infinite linear;$&#125;$ 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>打赏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法]]></title>
    <url>%2F2017%2F09%2F10%2F2017-05-06-EM%2F</url>
    <content type="text"><![CDATA[今天来分享EM算法，几个月前，通过阅读各种文献终于对EM算法有了较为清晰的认识，一直就想写篇博客总结一些，可惜拖延症拖延至今(QAQ)。期望最大算法（EM算法）是一种从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。先大体知道这么回事，理解了整篇文章再回来品读这句话。 正文必备知识jenson不等式&ensp;&ensp;&ensp;jenson不等式以丹麦数学家(Johan Jensen)命名。它给出积分的凸函数值和凸函数的积分值间的关系。jenson不等式的具体内容是啥呢？&ensp;&ensp;&ensp;若f(x)在(a,b)之间是凸函数，那么：$$f(\sum_{i=1}^k\theta_i x_i)\leq\sum_{i=1}^k\theta_i f(x_i)$$&ensp;&ensp;&ensp;其中\(\theta_1,\theta_2,…,\theta_k\geq0,\sum_{i=1}^k\theta_i=1\)&ensp;&ensp;&ensp;对于连续函数：&ensp;&ensp;&ensp;若 \(p(x)&gt;0 \quad on \quad S\subseteq dom f, \int_Sp(x)dx=1\)&ensp;&ensp;&ensp;则：$$f(\int_Sp(x)dx)\leq\int_Sf(x)p(x)dx$$&ensp;&ensp;&ensp;Jensen不等式是关于凸性(convexity)的不等式。凸性是一个很好的性质，在最优化问题里面，线性和非线性不是本质的区别，只有凸性才是。如果最优化的函数是凸的，那么局部最优就意味着全局最优，否则无法推得全局最优。有很多不等式都可以用Jensen不等式证得，从而可以把他们的本质归结为凸性。例如，均值不等式：$$\frac{\sum_{i=1}^nx_i}{n}\geq(\prod_{i=1}^nx_i)^{\frac{1}{n}}$$&ensp;&ensp;&ensp;本质上可以归结为对数函数log(x)的凸性。 聚类&ensp;&ensp;&ensp;首先让我们回忆一下k-means算法。&ensp;&ensp;&ensp;k-means算法，也叫k-平均或者k-均值算法，是一种广泛使用的聚类算法，其经常作为很多聚类算法的基础。&ensp;&ensp;&ensp;假设输入样本为 \(S=x_1,x_2,…,x_m\)&ensp;&ensp;&ensp;那么算法步骤为： 选择初始的k个簇中心: \(\mu_1,\mu_2,…\mu_k\) 将每个样本x标记为距离簇中心最近的簇：$$label_i=argmin_{1\leq j\leq i}||x_i-u_j||$$ 更新簇中心：$$\mu_j=\frac{1}{|c_j|}\sum_{i\in c_j}x_i$$ 重复最后两个步骤，指导满足终止条件。&ensp;&ensp;&ensp;终止条件有很多，可以使用迭代次数，簇中心变化率，最小平方误差MSE。&ensp;&ensp;&ensp;现在来思考一下，经典的k-means算法可以很方便的将未标记的样本分成多个集合，但并不能给出一个样本属于某一个集合的后验概率，当然咱们可以使用一些类似SVM中的方式得到类似于概率的数值。最大似然估计&ensp;&ensp;&ensp;最大似然估计属于统计学的内容，其常常用来估计样本的分布，找出与样本分布最接近的概率分布模型，它是一种非参数估计。其简单有效的估计方式使得其在应用中占据重要地位。&ensp;&ensp;&ensp;假如小红有一枚硬币，她抛了十次，结果分别是，正反反正反反反反正反&ensp;&ensp;&ensp;假设p为每次抛硬币结果为正的概率，则出现上面结果的概率为：$$\begin{align}P&amp;=p(1-p)(1-p)p(1-p)(1-p)(1-p)(1-p)p(1-p)\\&amp;=p^3(1-p)^7\end{align}$$&ensp;&ensp;&ensp;重点来了，最大似然概率的思想就是最大化似然概率，什么是似然概率，似然概率就是出现当前样本的概率，也就是上面的概率P，那么怎么最大化上面的概率P呢？小时候学的微积分派上用场了：一阶导为0求极点…。可以求得上式的最优解为:p=0.3&ensp;&ensp;&ensp;上面的整个思路历程就是最大似然估计的全过程。&ensp;&ensp;&ensp;将上面的例子更泛化一点，投N次硬币，得到结果，其中n次朝上，N-n次朝下。&ensp;&ensp;&ensp;假定朝上概率为p，则似然函数为：$$P=p^n(1-p)^{N-n}$$&ensp;&ensp;&ensp;因为对数函数是严格单增，为了方便计算，通常将似然函数变为对数似然函数计算：$$\begin{align}P&amp;=log(p^n(1-p)^{N-n})\\&amp;=nlog(p)+(N-n)log(1-p)\end{align}$$求最大：$$\frac{n}{p}-\frac{N-n}{1-p}=0 \Rightarrow p=\frac{n}{N}$$&ensp;&ensp;&ensp;更进一步的，给定一组样本\(x_1,x_2,…,x_n\)，已知他们来自于高斯分布\(N(\mu,\sigma)\)，试估计\(\mu,\sigma\)&ensp;&ensp;&ensp;写出似然函数：&ensp;&ensp;&ensp;已知高斯分布的概率密度函数为：$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}$$&ensp;&ensp;&ensp;则似然函数为：$$L(x)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$&ensp;&ensp;&ensp;对数似然：$$\begin{align}l(x)&amp;=log(L(x))=\sum_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\&amp;=\left( \sum_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma} \right) + \left( \sum_{i=1}^ne^{-\frac{(x-\mu)^2}{2\sigma^2}} \right)\\&amp;=-\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x\_i-\mu)^2\end{align}$$ &ensp;&ensp;&ensp;最大化似然函数，分别对\(\mu和\sigma\)求偏导求极值得： $$\begin{cases}\mu=\frac{1}{n}\sum_{i}x_i\\\sigma^2=\frac{1}{n}\sum_{i}(x_i-\mu)^2\end{cases}$$&ensp;&ensp;&ensp;可以发现咱们使用最大似然估计得到\(\mu和\sigma\)的值很符合直观想象：样本的均值就是高斯分布的均值，样本的方差就是高斯分布的方差。 提出问题&ensp;&ensp;&ensp;然而我们遇到的问题的随机变量有的时候是无法直接观察到的。比如：有N个人，其中有男性有女性，他们的身高分别服从\(N(\mu_1,\sigma_1)\)和\(N(\mu_2,\sigma_2)\)分布，要估计\(\mu_1，\sigma_1，\mu_2，\sigma_2\)。&ensp;&ensp;&ensp;我们如果知道哪些是男性哪些是女性就好了，只需要分别对他们使用上面的最大似然估计。但是遗憾的是我们并不知道，因此我们需要使用EM算法。想一想这个问题，我们也可以先对数据进行聚类，得到男性和女性两类，再分别建模。待我们学完EM算法，我们会发现其实两种方法的思想是相似的。最后我们会分析一下k-means和EM算法的关系。&ensp;&ensp;&ensp;上面的问题包含两个高斯模型，因此这类问题又称为高斯混合模型（GMM），高斯混合模型是EM算法解决问题的一个基本问题。 GMM的直观解法&ensp;&ensp;&ensp;随机变量X是由K个高斯分布混合而成，取各个高斯分布的概率为\(\pi_1,\pi_2,…,\pi_K\)，第i个高斯分布的均值为\(\mu_i\)，方差为\(\sum_i\)。随机变量X的一组样本为\(x_1,x_2,…,x_n\)，估计参数\(\pi，\mu，\sum\)。&ensp;&ensp;&ensp;按上面的步骤计算，先写出似然函数： $$L(x)=\prod_{i=1}^n\sum_{k=1}^K\pi_kN(x_i|\mu_i,\sum_k)$$ &ensp;&ensp;&ensp;取对数：$$l(x)=\sum_{i=1}^n log \left( \sum_{k=1}^K\pi_k N(x_i|\mu_i,\sum_k) \right)$$&ensp;&ensp;&ensp;显然上面的式子无法直接用求导的方式找最大值，为了解决这个问题，我们将问题分为两步： 估计数据来自哪一部分&ensp;&ensp;&ensp;对于每个样本\(x_i\)，它由第k个高斯分布生成的概率为：$$\gamma(i,k)=\frac{\pi_kN(x_i|\mu_k,\sum_k)}{\sum_{j=1}^K \pi_jN(x_i|\mu_j,\Sigma_j)}$$&ensp;&ensp;&ensp;注意上面的式子中的\(\mu和\sigma\)也是待估计的值，因此采用采样迭代法：首先先验的给出\(\mu和\sigma\)，然后计算\(\gamma\)，再通过计算出的\(\gamma\)计算\(\mu和\sigma\)，重复达到收敛即可。&ensp;&ensp;&ensp;这里的\(\gamma(i,k)\)也可以看做第k个高斯分布在生成数据\(x_i\)时所做的贡献。&ensp;&ensp;&ensp;上面已经粗略的说了计算的步骤，那么\(\mu和\sigma\)怎么由\(\gamma\)计算呢? 估计每个高斯分布的参数&ensp;&ensp;&ensp;对于每个高斯分布\(N_k\)而言，可以看做由它生成了\({\gamma(i,k)x_i|i=1,2,…,n}\)这些点。由于是高斯分布，所以直接使用前面讨论过的结果即可：$$\begin{cases}\mu=\frac{1}{n}\sum_{i}x_i\\\sigma^2=\frac{1}{n}\sum_{i}(x_i-\mu)^2\end{cases}$$&ensp;&ensp;&ensp;得到：$$\begin{cases}N_k=\sum_{i=1}^N\gamma(i,k)\\\mu_k=\frac{1}{N_k}\sum_{i=1}^{N_k}\gamma(i,k)x_i\\\sum_k=\frac{1}{N_k}\sum_{i=1}^{M_k}\gamma(i,k)(x_i-\mu_k)(x_i-\mu_k)^T\\\pi_k=\frac{N_k}{N}=\frac{1}{N}\sum_{i=1}^{N}\gamma(i,k)\end{cases}$$&ensp;&ensp;&ensp;重复上面的两步，达到收敛条件（各参数无明显变化等）即可。这就是EM算法的直观过程。EM算法&ensp;&ensp;&ensp;假设有数据集\({x_1,x_2,…,x_n}\)包含m个独立样本，希望从中找到该组数据的模型\(p(x,z)\)的参数。&ensp;&ensp;&ensp;写出对数似然：$$\begin{align}l(\theta)&amp;=\sum_{i=1}^nlog(p(x_i, \theta))\\&amp;=\sum_{i=1}^nlog(\sum_z p(x,z;\theta))（全概率公式）\end{align}$$&ensp;&ensp;&ensp;z是隐随机变量，不方便直接找到参数估计。因此使用一个求最大值时常用的一个策略：计算\(l(\theta)\)的下界，求这个下界的最大值；重复这个过程，直到收敛到局部最大值。先求到的下界有可能比较宽松，不断重复，使得精度提升，使得求到的下界更紧，从而最终接近真实结果。&ensp;&ensp;&ensp;令\(Q_i\)是z的某一个分布，\(Q_i\geq0\)，使用jenson不等式：$$\begin{align}l(\theta)&amp;=\sum_{i=1}^n log\sum_z p(x,z;\theta)=\sum_{i=1}^nlog\sum_{z_i}p(x_i,z_i;\theta)\\&amp;=\sum_{i=1}^nlog\sum_{z_i}Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\\&amp;\geq\sum_{i=1}^n\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\end{align}$$&ensp;&ensp;&ensp;为了取等号，使得：$$\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}=c$$&ensp;&ensp;&ensp;由此：$$Q_i(z_i)\propto p(x_i,z_i;\theta) 又\sum_zQ_i(z_i)=1$$$$\begin{align}\Rightarrow Q_i(z_i)&amp;=\frac{p(x_i,z_i;\theta)}{\sum_zp(x_i,z_i;\theta)}\\&amp;=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}\\&amp;=p(z_i;x_i,\theta)\end{align}$$&ensp;&ensp;&ensp;如此我们就得到EM算法的整体框架： E-step 对于每个i，令\(Q_i(z_i)=p(z_i;x_i,\theta)\) M-step 求\(\theta=argmax_{\theta}\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\)&ensp;&ensp;&ensp;重复进行上述两个步骤直至收敛。&ensp;&ensp;&ensp;至于EM算法的收敛性证明，我就不说了，直接上从网上找的一段证明：使用EM算法解决GMM问题&ensp;&ensp;&ensp;还记得咱们刚开始推到的GMM问题的解决方法吗？咱们当时是通过计算每一个高斯分布在每一个数据中占的比例估计每一个高斯分布的样本数据，然后再估计高斯分布的参数。现在咱们使用上面推导的EM算法解决一些这个问题。&ensp;&ensp;&ensp;随机变量X是由K个高斯分布混合而成，取各个高斯分布的概率为\(\varphi_1,\varphi_2,…,\varphi_K\)，第i个高斯分布的均值为\(\,u_i\)，方差为\(\sum_i\)。若观测到随机变量X的一系列样本\(x_1,x_2,…,x_n\)，试估计参数\(\phi,\mu,\sum\)。 E-step \(Q_i(z_i=j)=P(z_i=j|x_i;\phi,\mu,\sum)\) M-step 将多项式分布和高斯分布的参数带入：$$\begin{align}\sum_{i=1}^n\sum_{z_i}&amp;Q_i(z_i)log\frac{p(x_i,z_i;\phi,\mu,\sum)}{Q_i(z_i)}\\&amp;=\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)log\frac{p(x_i|z_i;\phi,\mu,\sum)p(z_i=j;\phi)}{Q_i(z_i=j)}\\&amp;=\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x_i-\mu_j)^T\Sigma_j^{-1}(x_i-\mu_j))\phi_j}{Q_i(z_i=j)}\end{align}$$&ensp;&ensp;&ensp;对均值求导：$$\begin{align}\bigtriangledown_{\mu_l}&amp;\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x_i-\mu_j)^T\Sigma_j^{-1}(x_i-\mu_j))\phi_j}{Q_i(z_i=j)}\\&amp;=-\bigtriangledown_{\mu_l}\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)\frac{1}{2}(x_i-\mu_j)\Sigma_j^{-1}(x_i-\mu_j)\\&amp;=-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)\bigtriangledown_{\mu_l}(x_i^T\Sigma_l^{-1}x_i-x_i^T\Sigma_j^{-1}\mu_j-\mu^T\Sigma_j^{-1}x_i+\mu^T\Sigma_j^{-1}\mu_j)\\&amp;=-\frac{1}{2}\sum_{i=1}^nQ_i(z_i=l)(-x_i^T\Sigma_l^{-1}-x_i^T\Sigma_l^{-1}+2\mu_l^T\Sigma_l^{-1})\\&amp;=\sum_{i=1}^nQ_i(z_i=l)(x_i^T\Sigma_l^{-1}-\mu_l^T\Sigma_l^{-1})\end{align}$$&ensp;&ensp;&ensp;令等式为0，得到均值为：$$\mu_l=\frac{\sum_{i=1}^nQ_i(z_i=l)x_i}{\sum_{i=1}^nQ_i(z_i=l)}$$&ensp;&ensp;&ensp;同理可以求得方差： $$\Sigma_j=\frac{\sum_{i=1}^nQ_i(z_i=j)(x_i-\mu_j)(x_i-\mu_j)^T}{\sum_{i=1}^nQ_i(z_i=j)}$$ &ensp;&ensp;&ensp;现在来估计多项式分布\(\phi\)的分布： $$\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac{1}{2}(x_i-\mu_i)^T\Sigma_j^{-1}(x_i-\mu_j))\phi_j}{Q_i(z_i=j)}$$ &ensp;&ensp;&ensp;因为咱们是要估计\(\phi\)，因此先删除无关的常数项，化简为： $$\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)log\phi_j$$ &ensp;&ensp;&ensp;概率的归一性，建立拉格朗日方程： $$L(\phi)=\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)log\phi_j+\beta(\sum_{j=1}^K\phi_j-1)$$ &ensp;&ensp;&ensp;有log函数在，求解到的\(\varphi_i\)一定是正的，因此不用考虑\(\varphi_i\)这个条件。&ensp;&ensp;&ensp;求偏导，赋为0，得：$$\frac{\partial}{\partial\phi_j}L(\phi)=\sum_{i}^n\frac{Q_i(z_i=j)}{\phi_j}+\beta$$ $$\sum_{j=1}^K\phi_j=1$$ $$\begin{align}-\beta&amp;=\sum_{i=1}^n\frac{Q_i(z_i=j)}{\phi_j}\cdot1\\&amp;=\sum_{i=1}^n\frac{Q_i(z_i=j)}{\phi_j}\cdot(\sum_{j=1}^K\phi_j)\\&amp;=\sum_{i=1}^n\sum_{j=1}^KQ_i(z_i=j)\\&amp;=\sum_{i=1}^n1\\&amp;=n\end{align}$$ &ensp;&ensp;&ensp;由此： $$\phi_j=\frac{1}{n}\sum_{i=1}^nQ_i(z_i=j)$$ &ensp;&ensp;&ensp;这样就得到了结果： $$\begin{cases}\mu_l=\frac{\sum_{i=1}^nQ_i(z_i=l)x_i}{\sum_{i=1}^nQ_i(z_i=l)}\\\Sigma_j=\frac{\sum_{i=1}^nQ_i(z_i=j)(x_i-\mu_j)(x_i-\mu_j)^T}{\sum_{i=1}^nQ_i(z_i=j)}\\\phi_j=\frac{1}{n}\sum_{i=1}^nQ_i(z_i=j)\end{cases}$$ &ensp;&ensp;&ensp;直观的看我们用EM算法求到的结果和前面的直观解法得到的结果不太一样，但是他们其实是一样的。（仔细想想！！！） EM算法的另一种理解&ensp;&ensp;&ensp;其实EM算法可以看做坐标上升。所谓坐标上升就是每次通过更新函数中的一维，通过多次的迭代以达到优化函数的目的。 $$J(Q,\theta)=\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}$$ &ensp;&ensp;&ensp;上面的J就是前面咱们推到出的EM算法的目标函数。E-step就是在Q维上最大化J，M-step就是在\(\theta\)上最大化J。 k-means和EM算法的关系&ensp;&ensp;&ensp;可以发现EM算法的过程跟k-means算法的过程非常相似，都是先确定一个，更新另一个，再重新更新第一个，反复进行到收敛。实际上，k-means就是EM算法的一个实现。在k-means中先是假定隐含变量类别并将某一类别赋给每一个样例，这个过程就是E-step，M-step通过确定好的类别对其每个类的参数进行优化（这里就是更新类中心并重新分配类别）。 &ensp;&ensp;&ensp;总的来说，EM算法就是使用极大似然估计的方法，通过坐标上升的优化方法对隐变量和显变量的联合分布就行求最值的过程。 &ensp;&ensp;&ensp;好了，EM学完了，不得不说，EM算法中包含的东西真多啊！下一个准备学习的是—-&gt;LDA。 参考链接 k-means和em 坐标上升]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN初涉]]></title>
    <url>%2F2017%2F09%2F07%2F2017-09-06-GAN%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;论文原文Generative Adversarial Nets &ensp;&ensp;&ensp;生成对抗网络是近几年研究火热的方向，为了赶上潮流，本人最近也要开始研究(咳咳，有点把科研说的俗了QAQ)。 &ensp;&ensp;&ensp;生成对抗网络是一种生成式模型，使用一个对抗过程训练网络，而不再像已有方式那样使用复杂的马尔科夫等求解。 &ensp;&ensp;&ensp;那么问题来了，什么是生成式模型呢。生成模型是相对判别模型来说的。判别模型很简单，机器学习算法的作用一般是给定样本x，得到其所在的类别(分类)、预测结果（回归）。而学习这个过程的不同分为生成模型和判别模型。 判别模型&ensp;&ensp;&ensp;如果是通过学习样本x和结果y之间的条件概率P(y|x)或者y=f(x)的方式称为判别模型。事实上，y=f(x)的隐含也是学习了P(y|x)。因为你仔细想想，使用y=f(x)的意思就是根据x学习一个函数f将x映射到y，而f是根据数据得到的，在这个学习f的过程中不难想到，实际上是学习到了P(y|x)然后得到f函数最终将x映射到y。 生成模型&ensp;&ensp;&ensp;如果是通过学习x和y的联合分布律从而进行得到x到y的映射方式称为生成模型。使用公式:得到。 &ensp;&ensp;&ensp;现在生成模型主要有两种用法： Density estimation(密度评估) &ensp;&ensp;&ensp;如图所示，简单的来说，密度估计就是估计数据概率分布。 Sample generation(采样生成) &ensp;&ensp;&ensp;采样生成则是根据应有的数据，生成跟已有的数据同分布的新数据。 应用&ensp;&ensp;&ensp;生成模型的应用有很多，如： &ensp;&ensp;&ensp;根据视频前面的帧，预测生成下一个帧。 &ensp;&ensp;&ensp;提高图像的分辨率 &ensp;&ensp;&ensp;场景生成或根据物体轮廓图生成物体的整个图像。 生成模型原理&ensp;&ensp;&ensp;生成模型的原理总的来说，就是通过最大化已知数据的似然概率来确定模型的隐含参数，也就是最大似然估计的过程。 生成模型族谱 &ensp;&ensp;&ensp;下面是我从网上抄来的一些现有生成模型的总结： Fully Visible Belief Nets WaveNet Change of Variables Variational Autoencoder Boltzmann Machines 生成对抗网络&ensp;&ensp;&ensp;好了介绍了这么多，终于来到生成对抗网络了。 &ensp;&ensp;&ensp;生成对抗网络总体原理和生成网络是一致的，也是使用极大似然估计的方式，但是它的好处是使用对抗方式完成。 &ensp;&ensp;&ensp;生成对抗网络由一个生成器和一个判别器组成。生成器生成跟真实数据分布一致的数据，而判别器则判别数据真实数据还是由判别器生成的，生成器的目标是生成更接近真实分布的数据而不被判别器所判别，而判别器的目标是判别出真实数据和生成器生成数据的不同。这样两者就组成了一个零和博弈，生成器生成越来越接近真实数据的分布，从而使得判别器不能判别出数据是来自生成器还是真实数据，这时判别器就需要提高自己的判别能力，而判别器能力的提高，又会使得生成器提高自己的能力以生成更接近真实分布的数据而不被判别器所识别。最终两者达到一个平衡点，即生成器和判别器的能力再也不能提高，这个平衡点在博弈论被称为纳什均衡。 博弈&ensp;&ensp;&ensp;下面就介绍一下一些博弈论的知识。 &ensp;&ensp;&ensp;所谓零和博弈，零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。 —-转自百度百科&ensp;&ensp;&ensp;而所谓纳什均衡则是指一个博弈游戏的参与人所采用的这样一个策略组合，在该策略组合上任何参与人单独改变策略都不会得到好处。也就是说，如果在一个策略组合上，当所有其他人都不改变策略时，没有人会改变自己的策略（如果改变，则损失收益），则该策略组合就是一个纳什均衡。 &ensp;&ensp;&ensp;下面是我从网上搜集的一些纳什均衡的案例可以用来辅助理解纳什均衡。 智猪博弈 &ensp;&ensp;&ensp;猪圈里有两头猪，一头大猪，一头小猪。猪圈的一边有个踏板，每踩一下踏板，在远离踏板的猪圈的另一边的投食口就会落下少量的食物。如果有一只猪去踩踏板，另一只猪就有机会抢先吃到另一边落下的食物。当小猪踩动踏板时，大猪会在小猪跑到食槽之前刚好吃光所有的食物；若是大猪踩动了踏板，则还有机会在小猪吃完落下的食物之前跑到食槽，争吃到另一半残羹。 &ensp;&ensp;&ensp;那么，两只猪各会采取什么策略？答案是：小猪将选择“搭便车”策略，也就是舒舒服服地等在食槽边；而大猪则为一点残羹不知疲倦地奔忙于踏板和食槽之间。 &ensp;&ensp;&ensp;原因何在？因为，小猪踩踏板将一无所获，不踩踏板反而能吃上食物。对小猪而言，无论大猪是否踩动踏板，不踩踏板总是好的选择。反观大猪，已明知小猪是不会去踩动踏板的，自己亲自去踩踏板总比不踩强吧，所以只好亲力亲为了。 囚徒困境 &ensp;&ensp;&ensp;（1950年，数学家塔克任斯坦福大学客座教授，在给一些心理学家作讲演时，讲到两个囚犯的故事。） &ensp;&ensp;&ensp;假设有两个小偷A和B联合犯事、私入民宅被警察抓住。警方将两人分别置于不同的两个房间内进行审讯，对每一个犯罪嫌疑人，警方给出的政策是：如果一个犯罪嫌疑人坦白了罪行，交出了赃物，于是证据确凿，两人都被判有罪。 &ensp;&ensp;&ensp;如果另一个犯罪嫌疑人也作了坦白，则两人各被判刑8年；如果另一个犯罪嫌人没有坦白而是抵赖，则以妨碍公务罪（因已有证据表明其有罪）再加刑2年，而坦白者有功被减刑8年，立即释放。如果两人都抵赖，则警方因证据不足不能判两人的偷窃罪，但可以私入民宅的罪名将两人各判入狱1年。 囚徒困境 A/B 坦白 不坦白 坦白 -8,-8 0,-10 不坦白 -10,0 -1,-1 &ensp;&ensp;&ensp;关于案例，显然最好的策略是双方都抵赖，结果是大家都只被判1年。但是由于两人处于隔离的情况，首先应该是从心理学的角度来看，当事双方都会怀疑对方会出卖自己以求自保、其次才是亚当·斯密的理论，假设每个人都是“理性的经济人”，都会从利己的目的出发进行选择。 &ensp;&ensp;&ensp;这两个人都会有这样一个盘算过程：假如他坦白，如果我抵赖，得坐10年监狱，如果我坦白最多才8年；假如他要是抵赖，如果我也抵赖，我就会被判一年，如果我坦白就可以被释放，而他会坐10年牢。综合以上几种情况考虑，不管他坦白与否，对我而言都是坦白了划算。两个人都会动这样的脑筋，最终，两个人都选择了坦白，结果都被判8年刑期。 &ensp;&ensp;&ensp;基于经济学中Rational agent的前提假设，两个囚犯符合自己利益的选择是坦白招供，原本对双方都有利的策略不招供从而均被判处一年就不会出现。这样两人都选择坦白的策略以及因此被判8年的结局，纳什均衡首先对亚当·斯密的“看不见的手”的原理提出挑战：按照斯密的理论，在市场经济中，每一个人都从利己的目的出发，而最终全社会达到利他的效果。但是我们可以从“纳什均衡”中引出“看不见的手”原理的一个悖论：从利己目的出发，结果损人不利己，既不利己也不利他。 普通范式博弈 &ensp;&ensp;&ensp;GOO公司和SAM公司是某手机产品生态的两大重量级参与者，双方在产业链的不同位置上各司其职且关系暧昧，有时也往往因商业利益和产品影响力的争夺而各怀异心。二者的收益也随着博弈的变化而不断更替。 GOO/SAM 合作 背叛 合作 3, 3 -3, 5 背叛 5,-3 -1,-1 &ensp;&ensp;&ensp;上图表格模拟了两家公司的博弈现状，双方各有两个可选策略“合作”与“背叛”，格中的四组数据表示四个博弈结局的分数（收益），每组数据的第一个数字表示GOO公司的收益，后一个数字表示SAM公司的收益。 &ensp;&ensp;&ensp;博弈是同时进行的，一方参与者必须站在对方的角度上来思考我方的策略选择，以追求收益最大化。这在博弈论里称作Putting yourselves into other people’s shoes。 &ensp;&ensp;&ensp;现在我们以GOO公司为第一人称视角来思考应对SAM公司的博弈策略。假如SAM公司选择合作，那么我方也选择合作带来的收益是3，而我方选择背叛带来的收益是5，基于理性的收益最大化考虑，我方应该选择背叛，这叫严格优势策略；假如SAM公司选择背叛，那么我方选择合作带来的收益是-3，而选择背叛带来的收益为-1，为使损失降到最低，我方应该选择背叛。最后，GOO公司的分析结果是，无论SAM公司选择合作还是背叛策略，我方都必须选择背叛策略才能获得最大化的收益。 &ensp;&ensp;&ensp;同理，当SAM公司也以严格优势策略来应对GOO公司的策略选择时，我们重复上述分析过程，就能得出结论：无论GOO公司选择合作还是背叛策略，SAM公司都必须选择背叛策略才能获得最大化收益。 &ensp;&ensp;&ensp;最后我们发现，本次博弈的双方都采取了背叛策略，各自的收益都为-1，这是一个比较糟糕的结局，尽管对任何一方来说都不是最糟糕的那种。这种局面就是著名的“囚徒困境”。 &ensp;&ensp;&ensp;但是，博弈的次数往往不止一次，就像COO与SAM公司双方的商业往来也许会有很多机会。当二者经历了多次背叛策略的博弈之后，发现公式上还有一个（3，3）收益的双赢局面，这比（-1，-1）的收益结果显然要好很多，因此二者在之后的博弈过程中必然会尝试互建信任，从而驱使双方都选择合作策略。 &ensp;&ensp;&ensp;这里有一个理想化假设，那就是假设双方都知道博弈次数是无限的话，也就是说双方的商业往来是无止尽的，那么二者的策略都将持续选择合作，最终的博弈收益将定格在（3，3），这就是一个纳什均衡。既然博弈次数是无限的，那么任何一方都没有理由选择背叛策略去冒险追求5点短暂收益，而招致对方在下一轮博弈中的报复（这种报复在博弈论里称作“以牙还牙”策略）。 &ensp;&ensp;&ensp;还有另一种假设情况是，假使双方都知道博弈次数是有限的，也许下一次博弈就是最后一次，那么为了避免对方在最后一轮博弈中选择背叛策略而使我方遭受-3的收益损失，于是双方都重新采取了背叛的策略选择，最后的博弈结果又回到了（-1，-1），这就形成了第二个纳什均衡。 &ensp;&ensp;&ensp;由此可见，随着次数（博弈性质）的变化，纳什均衡点也并非唯一，这在下一个例子中有着更明显的表现。 饿狮博弈 &ensp;&ensp;&ensp;题设为A、B、C、D、E、F六只狮子（强弱从左到右依次排序）和一只绵羊。假设狮子A吃掉绵羊后就会打盹午睡，这时比A稍弱的狮子B就会趁机吃掉狮子A，接着B也会午睡，然后狮子C就会吃掉狮子B，以此类推。那么问题来了，狮子A敢不敢吃绵羊？ &ensp;&ensp;&ensp;为简化说明，我们先给出此题的解法。该题须采用逆向分析法，也就是从最弱的狮子F开始分析，依次前推。假设狮子E睡着了，狮子F敢不敢吃掉狮子E？答案是肯定的，因为在狮子F的后面已没有其它狮子，所以狮子F可以放心地吃掉午睡中的狮子E。 &ensp;&ensp;&ensp;继续前推，既然狮子E睡着会被狮子F吃掉，那么狮子E必然不敢吃在他前面睡着的狮子D。 &ensp;&ensp;&ensp;再往前推，既然狮子E不敢吃掉狮子D，那么D则可以放心去吃午睡中的狮子C。依次前推，得出C不吃，B吃，A不吃。所以答案是狮子A不敢吃掉绵羊。&ensp;&ensp;&ensp;细心的人也许会发现，假如增加或减少狮子的总数，博弈的结果会完全不同。我们用下图来验证： &ensp;&ensp;&ensp;我们在狮子F的后面增加了一只狮子G，总数变成7只。用逆向分析法按照上题步骤再推一次，很容易得出结论：狮子G吃，狮子F不吃，E吃，D不吃，C吃，B不吃，A吃。这次的答案变成了狮子A敢吃掉绵羊。 &ensp;&ensp;&ensp;对比两次博弈我们发现，狮子A敢不敢吃绵羊取决于狮子总数的奇偶性，总数为奇数时，A敢吃掉绵羊；总数为偶数时，A则不敢吃。因此，总数为奇数和总数为偶数的狮群博弈结果形成了两个稳定的纳什均衡点。 硬币正反 &ensp;&ensp;&ensp;你正在图书馆枯坐，一位陌生美女主动过来和你搭讪，并要求和你一起玩个数学游戏。美女提议：“让我们各自亮出硬币的一面，或正或反。如果我们都是正面，那么我给你3元，如果我们都是反面，我给你1元，剩下的情况你给我2元就可以了。”那么该不该和这位姑娘玩这个游戏呢？这基本是废话，当然该。问题是，这个游戏公平吗？ &ensp;&ensp;&ensp;每一种游戏依具其规则的不同会存在两种纳什均衡，一种是纯策略纳什均衡，也就是说玩家都能够采取固定的策略(比如一直出正面或者一直出反面)，使得每人都赚得最多或亏得最少；或者是混合策略纳什均衡，而在这个游戏中，便应该采用混合策略纳什均衡。 你/美女 美女出正面 美女出反面 你出正面 3,-3 -2, 2 你出反面 -2, 2 +1,-1 &ensp;&ensp;&ensp;假设我们出正面的概率是x，反面的概率是1-x，美女出正面的概率是y，反面的概率是1-y。为了使利益最大化，应该在对手出正面或反面的时候我们的收益都相等，由此列出方程就是 &ensp;&ensp;&ensp;3x + (-2)*(1-x)=(-2) * x + 1*( 1-x )解方程得x=3/8。 &ensp;&ensp;&ensp;同样，美女的收益，列方程 &ensp;&ensp;&ensp;-3y + 2( 1-y)= 2y+ (-1) * ( 1-y) &ensp;&ensp;&ensp;解得y也等于3/8，而美女每次的期望收益则是2(1-y)- 3y = 1/8元。这告诉我们，在双方都采取最优策略的情况下，平均每次美女赢1/8元。 &ensp;&ensp;&ensp;其实只要美女采取了(3/8,5/8)这个方案，不论你再采用什么方案，都是不能改变局面的。如果全部出正面，每次的期望收益是 (3+3+3-2-2-2-2-2)/8=-1/8元；如果全部出反面，每次的期望收益也是(-2-2-2+1+1+1+1+1)/8=-1/8元。而任何策略无非只是上面两种策略的线性组合，所以期望还是-1/8元。但是当你也采用最佳策略时，至少可以保证自己输得最少。否则，你肯定就会被美女采用的策略针对，从而赔掉更多。 模型&ensp;&ensp;&ensp;对于生成器来说，通过将有特定分布的噪音通过一个可微的函数映射为输出x，这里的这个函数G就是生成器，如下式： 函数必须可微，以便可以求梯度进行学习进行学习 没有可逆性的必要 可以对任何维度的z进行训练 z的维度有时需要大于x &ensp;&ensp;&ensp;同样对于判别器来说，其也是一个可微函数通过对输入的数据x进行判别是来自生成器 还是来自真实数据 &ensp;&ensp;&ensp;函数G和D都可以使用神经网络实现，如MLP(多层感知机)等。 训练过程&ensp;&ensp;&ensp;我们训练D使得它能够正确区分来自生成器的数据和真实数据，训练G使得生成器的数据更接近真实数据。 &ensp;&ensp;&ensp;假设已经给定G，优化D，那么根据最小化交叉熵损失写出目标函数为： &ensp;&ensp;&ensp;其实就是一个最大最小游戏。 &ensp;&ensp;&ensp;对于任何非零实数m,n，且实数值y在范围[0,1]之间，那么： &ensp;&ensp;&ensp;在m/m+n处取得最小值，于是在给定G的条件下，在 &ensp;&ensp;&ensp;处取得最小值，这里也能表示GAN估计的是两个概率分布的比值。 &ensp;&ensp;&ensp;当真实分布和生成器分布相同时，两者等于二分之一，此时D为1/2，D的损失函数的梯度为0，无法继续优化。达到均衡。 &ensp;&ensp;&ensp;上面已经讨论了当生成器分布和真实数据一致时，判别器达到最优，那么此时生成器是否也是最优呢？ &ensp;&ensp;&ensp;D的训练目标可以看做最大化判别似然概率P(Y=y|x)，其中Y代表x是来自真实数据(y=1)还是来自生成器数据(y=0)。因此： &ensp;&ensp;&ensp;将上式带入上面公式C(G)中，得： &ensp;&ensp;&ensp;再将-log4从C(G)中减去，并将原始C(G)使用KL距离表示，得： 观察上面式子的后两项，其结果可以用JS散度表示： &ensp;&ensp;&ensp;由于两个分布的JS散度值是非负的，而且当且仅当两者相等时取0，所以C(G)的全局最小值为-log4，此时两个分布相同，即： &ensp;&ensp;&ensp;一般来说，训练时对D更新k次，再对G更新一次。 &ensp;&ensp;&ensp;如上图所示，蓝色的线代表判别器，绿色的线代表生成器，黑色的线代表真实数据，由上面图从左到右的渐变过程可以发现，GAN是逐步将生成器分布拟合到真实数据分布上，最终判别器不再发生变化，达到均衡。 &ensp;&ensp;&ensp;训练算法伪代码如下： 目标函数改进&ensp;&ensp;&ensp;在证明GAN能够做得拟合真实分布时，Goodfellow做了一个很大胆的假设：用来评估样本真实度的D网络具有无限的建模能力，也就是说不管真实样本和生成的样本有多复杂，D-网络都能把他们区分开。这个假设呢，也叫做非参数假设。 当然，对于深度网络来说，咱只要不断的加高加深，这还不是小菜一碟吗？深度网络擅长的就是干这个的么。 &ensp;&ensp;&ensp;但是，一旦真实样本和生成样本之间重叠可以忽略不计（这非常可能发生，特别当这两个分布是低维流型的时候），而又由于D-网络具有非常强大的无限区分能力，可以完美地分割这两个无重叠的分布，这时候，经典GAN用来优化其生成网络（下文称G-网络）的目标函数–JS散度就会变成一个常数！ &ensp;&ensp;&ensp;我们知道，深度学习算法，基本都是用梯度下降法来优化网络的。一旦优化目标为常数，其梯度就会消失，也就会使得无法对G-网络进行持续的更新，从而这个训练过程就停止了。这个难题一直一来都困扰这GAN的训练，称为梯度消失问题。 &ensp;&ensp;&ensp;由此对上面的目标函数进行分析，如果判别器达到最优后，由于生成器的目标函数使用的是判别器目标函数的相反数，所以，此时生成器也无法继续优化，那么问题就来了，实际情况中肯定存在判别器达到最好之后，而生成器还不是最优的情况，所以将目标函数改进如下： &ensp;&ensp;&ensp;这样： 不再使用一个单一的loss 生成器最大化判别器没有考虑到的似然概率 当判别器达到最优时，生成器仍然可以继续优化 &ensp;&ensp;&ensp;KL散度: $$D_{KL}(P\Vert Q)=\int_{-\infty}^{\infty}p(x)log\frac{p(x)}{q(x)}dx$$ &ensp;&ensp;&ensp;KL散度中当p和q的位置变化后影响图： &ensp;&ensp;&ensp;当p在左边时： $$\begin{align}D_{KL}(P\Vert Q)&amp;=\int_{-\infty}^{\infty}p(x)log\frac{p(x)}{q(x)}dx\\&amp;=\int\ p(x)log\ p(x)-\int\ p(x)log\ q(x)\\&amp;=-常熟-常熟log\ q(x)\\&amp;\propto –log\ q(x)\end{align}$$ &ensp;&ensp;&ensp;最后的这个形式就是最大似然估计的形式，所以看上图左边，虽然真实分布是双峰，但是使用最大似然估计得到的分布是单峰的。同样的原理可以分析右边的图：真实分布是双峰，生成分布只拟合其中一个峰。 &ensp;&ensp;&ensp;然后就可以用KL散度写损失函数了： $$J^{(D)}=-\frac{1}{2}E_{x~p_{data}}log\ D(x)-\frac{1}{2}E_zlog(1-D(G(z)))\\J^{(G)}=-\frac{1}{2}E_zexp(\sigma^{-1}(D(G(z))))$$ &ensp;&ensp;&ensp;上面损失函数的推导可以看下面的参考链接。 &ensp;&ensp;&ensp;当判别器达到最优时，生成器的梯度达到最大似然概率。 &ensp;&ensp;&ensp;Goodfellow经过实验得到的三种损失函数的比较图如下： 总结&ensp;&ensp;&ensp;GAN是一种通过对输入的随机噪声z （比如高斯分布或者均匀分布），运用一个深度网络函数G(z)，从而希望得到一个新样本，该样本的分布，我们希望能够尽可能和真实数据的分布一致。 参考链接 “On Distinguishability Criteria for Estimating Generative Models”, Goodfellow 2014]]></content>
      <categories>
        <category>DL</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim中的正则表达式]]></title>
    <url>%2F2017%2F08%2F25%2F2017-08-25-vireg%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;前面已经介绍过正则表达式，由于我是一个vim的重度使用者，所以vim中的正则表达式也是我经常使用的。当然，其实概念原理都是一样的，所以本文只是展示一下通过从网上搜集的vim正则的符号使用而已。 量词 vim 意义 * 0个或多个 \+ 1个或多个 \? 0个或1个，\?不能在 ? 命令（逆向查找）中使用 \{n,m} n个到m个 \{n,} 最少n个 \{,m} 最多m个 \{n} 恰好n个 \{-n,m} n个到m个(非贪婪) \{-} 0个或多个(非贪婪) \{-1,} 1个或多个(非贪婪) \{-,1} 0个或1个(非贪婪) \_. 匹配包含换行在内的所有字符 \= 匹配一个可有可无的项 \_s 匹配空格或断行 环视和固化分组 vim 正则表达式 意义 \@= (?= 顺序环视 \@! (?! 顺序否定环视 \@&lt;= (?&lt;= 逆序环视 \@&lt;! (?&lt;! 逆序否定环视 \@&gt; (?&gt; 固化分组 \%(atom) (?: 非捕获组 &ensp;&ensp;&ensp;这里要注意vim中的环视表达式的书写位置和普通正则不同，如(?&lt;=abc)dec在vim中应该写为\(abc\)\@&lt;=dec.h。 元字符 元字符 说明 . 匹配任意一个字符 [abc] 匹配方括号中的任意一个字符。可以使用-表示字符范围，如[a-z0-9]匹配小写字母和阿拉伯数字。 [^abc] 在方括号内开头使用^符号，表示匹配除方括号中字符之外的任意字符。 \d 匹配阿拉伯数字，等同于[0-9]。 \D 匹配阿拉伯数字之外的任意字符，等同于[^0-9]。 \x 匹配十六进制数字，等同于[0-9A-Fa-f]。 \X 匹配十六进制数字，等同于[^0-9A-Fa-f]。 \w 匹配单词字母，等同于[0-9A-Za-z_]。 \W 匹配单词字母之外的任意字符，等同于[^0-9A-Za-z_]。 \t 匹配字符。 \s 匹配空白字符，等同于[ \t]。 \S 匹配非空白字符，等同于[^ \t]。 \a 所有的字母字符. 等同于[a-zA-Z] \l 小写字母 [a-z] \L 非小写字母 [^a-z] \u 大写字母 [A-Z] \U 非大写字母 [^A-Z] $ 匹配行尾 ^ 匹配行首 \&lt; 匹配单词词首 \&gt; 匹配单词词尾 \&amp; 并列 &ensp;&ensp;&ensp;vim还有一个不常见的有用的元字符，\zs和\ze，它们的意思是限定匹配的范围。先想一下，当我们要进行查找时，需要先写一个正则表达式，这个正则表达式称为模式。而通过这个模式在原文本匹配到的内容成为匹配，有的时候需要对匹配的部分内容进行截取，这个时候就需要用到\zs（匹配开始）和\ze（匹配结束）了。如我们要在文本the word &quot;world&quot;中匹配在双引号中的内容，我们很自然的写出模式&quot;[^&quot;]+&quot;，这样就会得到匹配内容&quot;world&quot;，但是这不符合我们要求啊，我们不想要双引号本身，怎么办？使用\zs和\ze，写出模式&quot;\zs[^&quot;]+\ze&quot;，这样就得到了匹配结果world。\zs和\ze是零宽度的，它们只是指定匹配的开始和结束，并不匹配任何字符。我们也可以使用环视来达到\zs和\ze的效果，如上面的模式可以改为&quot;\@&lt;=[^&quot;]+&quot;\@=]&quot;达到同样的效果。 &ensp;&ensp;&ensp;&lt;和&gt;能够匹配单词的边界，其可以使用元字符\zs和\ze得到，\W\ze\w相当于\&lt;，\w\ze\W相当于\&gt; &ensp;&ensp;&ensp;另外\|表示”或”操作符转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令（持续添加）]]></title>
    <url>%2F2017%2F08%2F03%2F2017-08-03-linuxsusualorder%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;在使用linux过程中会学习到很多有用的新命令。本文就总结一些我目前遇到的，以后会持续添加。 xsel&ensp;&ensp;&ensp;xsel是一个在linux下管理复制粘贴功能的命令，具体可以使用man xsel查看。 &ensp;&ensp;&ensp;我使用这个命令的初衷是我在linux下工作时。经常需要复制文件名或者文件的内容，当然可以使用鼠标，但是作为vim重度使用者，使用鼠标是不是太麻烦了？QAQ，于是就鼓捣了一些，找到了这个命令。 用法&ensp;&ensp;&ensp;在这里介绍一些常用的用法。 &ensp;&ensp;&ensp;xsel是用来管理复制粘贴的选择器的，所谓选择器就是复制的数据放在哪（我的理解)，这里我用到的选择器是系统复制粘贴版clipboard。 常用属性： 输入选项 -a, --append将标准输入中的内容添加到选择器中 -f, --follow当标准输入增加时，将增加的部分添加到选择器（我也不懂待查，一般也用不着） -i, --input将标准输入放入选择器 输出选项 -o, --output将选择器中的内容放入到标准输出中 活动选项 -c, --clear清空选择器中的内容 选择器选项 -b, --clipboard指定选择器为clipboard &ensp;&ensp;&ensp;还有其他一些选项，但是不常用，这里不介绍，如果需要可以查看man xsel 常见使用： cat filename | xsel -b -i tr&ensp;&ensp;&ensp;tr命令用来对标准输入中的字符进行转换和删除，然后将转换后的内容输出到标准输出。 命令格式&ensp;&ensp;&ensp;tr [选项] 字符集1 [字符集2] 将标准输入中的字符串1的字符转换为字符串2中的字符。 选项 -c, -C, --complement 使用字符集1时使用的是它的补集，使用这个命令后，这个命令的字符集1已经实际上变成我们制定的字符集的补集。 -d, --delete 删除字符集1中的字符，这里注意-d和-c选项搭配时的效果（删除咱们指导的字符集的补集中的字符）。 -s, --squeeze-repeats 对于最后一个指定的字符集r如果指定了两个字符集那么就是字符集2，如果指定了一个字符集，那么就是字符集1）中的字符，如果它在标准输入中连续出现多次，那么将多次变为一次显示。如标准输入是adfffffddccd，指定的最后一个字符集是fd那么标准输入会将连续的f和连续的d变为一次，即标准输入变为adfdccd -t, --truncate-set1 tr命令最终的结果是将字符串1中的字符转换为字符串2中的字符，注意如果字符串1的长度和字符串2的长度一样，那么就可以从左到右一一对应转换，但是如果长度不一样该怎么办呢，默认情况下，如果字符集2的长度大于字符集1的长度，超出的部分被忽略，如果字符集1的长度大于字符集2的长度，通过将字符集2的最后一个字符重复多次将字符集2的长度扩展至跟字符集1一样。但是我们不想用这种扩展方式怎么办，这时-t就是将长度长的字符集1截断跟长度短的字符集2一样的长度。 --help 显示帮助文档。 --version 显示版本 字符集 \NNN 八进制按键编码 \a Ctrl-G 铃声\007 \b Ctrl-H 退格符\010 \f Ctrl-L 走行换页\014 \n Ctrl-J 新行\012 \r Ctrl-M 回车\015 \t Ctrl-I tab键\011 \v Ctrl-X \030 [:alnum:] 所有字母和数字 [:alpha:] 所有字母 [:blank:] all horizontal whitespace [:cntrl:] 所有控制字符 [:digit:] 所有数字 [:graph:] 图形符号 [:lower:] 所有小写字母 [:print:] 所有可打印字符 [:punct:] 所有标点符号 [:space:] 所有空白符号 [:upper:] 所有大写字母 [:xdigit:] 十六进制字符 [=CHAR=] 所有等于CHAR的字符 CHAR1-CHAR2 字符1到字符2之间的所有字符。 [CHAR*] 根据需要将CHAR连续复制多次，达到一定长度 [CHAR*REPEAT] 连续REPEAT次字符 &ensp;&ensp;&ensp;上面即展示了一些字符在tr命令中的书写形式，也表示了tr命令中的字符集1和字符集2可以使用的所有字符类别。 注意&ensp;&ensp;&ensp;如果没有-d选项而且字符集1和字符集2都给定了，那么进行转换操作（将字符集1中的字符转换为字符集2中的字符，参见-t选项)，而且-t选项只会在转换操作时使用。字符集2会在必要的时候通过重复它的最后一个字符扩展到跟字符集1的长度一样（除非指定-t命令，这样就会截断字符集1与字符集2的长度一样）。如果字符集2的长度大于字符集1的长度，超过的部分忽略。-s选项使用的是指定的最后一个字符集，-s选项的执行顺序在转换或删除操作之后。只有[:lower:]和[:upper:]被保证在扩展时是递增的。 使用实例将I love programming中的所有字符o转换为字符t，字符e转换为字符z12$ echo "I love programming" | tr "oe" "tz"I ltvz prtgramming 将I love programming中的所有字符o和字符e删除12$ echo "I love programming" | tr -d "oe" I lv prgramming 将I hessssssdbbbaadffff中的连续出现字符s和字符f变为一个12$ echo "I hessssssdbbbaadffff" | tr -s "sf"I hesdbbbaadf 将I hessssssdbbbaadffff中除了字符I和字符e和字符f之外的字符删除12echo "I hessssssdbbbaadffff" | tr -c -d "Ief"Ieffff seq&ensp;&ensp;&ensp;打印一个序列，sequence&ensp;&ensp;&ensp;常用方法： seq -s 指定分隔符,-s后面加分隔符 seq -w 加入前导0使得输出等宽 seq .. FIRST seq .. FIRST LAST seq .. FIRST INCREMENT LAST grep&ensp;&ensp;&ensp;查找过滤。 -v 排除 -B 显示匹配的行，并显示该行之前的num行 -A 显示匹配的行，并显示该行之后的num行 -C 显示匹配的行，并显示该行前后的num行 tail&ensp;&ensp;&ensp;取文件的最后几行 tail -n 取文件最后n行 tail -f 监控文件的变化 查看图像大小 convert a.jpg -print &quot;Size: %wx%h\n&quot; /dev/null 常见面试取文件20行到30行内容 sed -n &#39;20,30p&#39; filename awk &#39;{if(NR&gt;31 &amp;&amp; NR&lt;19) printf $1&quot;\n&quot;}&#39; filename head -30 filename | tail -11 将一个目录下的所有以.sh为后缀的文件中的字符串aaa替换为bbb find dirname -type f -name &quot;*.sh&quot; | xargs sed -i &#39;s#aaa#bbb#g&#39; sed -i &#39;s#aaa#bbb#g&#39; find dirname -type f -name &quot;*.sh&quot; find dirname -type f -name &quot;*.sh&quot; -exec sed -i &#39;s#bbb#aaa#g&#39; {} \; dns配置文件resolv.conf man resolv host在企业的作用 man host 内部DNS 主机名和ip地址进行对应 编辑/etc/hosts 开发、产品、测试等人员用于通过域名测试产品 服务器之间的调用可以用域名（内部的DNS)，方便迁移 主机名 host 命令 /etc/hostname当前生效 /etc/sysconfig/network永久生效 fstab df -h查看分区情况 设置文件系统挂载信息文件，使得系统启动自动挂载文件系统 /etc/fstab dd if=/dev/zero of=/dev/sdc bs=4096 count=100 mount -t ext4 -o loop,noatime,noexec /dev/sdc /mnt mkfs. fsck 检查不好的处于卸载状态的磁盘 设备么 uuid 标签是等同的，都可以用来挂载 fstab很重要，一旦错误将启动不了 (1) (2) 救援模式 rescue mount -o rw,remount / ### cat /etc/arch-release uname -r 版本 uname -m 32位还是64位 hostname uname -n查看主机名和hostname一样 uname -a所有 useradd /etc/passwd /etc/shadow /etc/group passwd $PS1 whoami su - 用户名 -的作用是将当前的环境变量也切换到相应位置 selinux /etc/selinux/config /etc/systemconfig/selinux sed -i “s#SELINUX=DISABLED#SELINUX=ENFO#” /etc/selinux/config setenforce 0 临时生效 getenforce 运行级别 /etc/inittab runlevel init切换级别 开机启动 shhd rsyslog network crond sysstat （1）用chkconfig(2)放入/etc/rc.local服务器档案文件，放入rc.local中注释备案。 当挂载NFS网络文件系统时，网卡还没启动，fstab已经启动，这时fstab无法挂载网络文件系统，这时只能使用比网卡启动晚的rc.local才能完成任务。 打印行号的方法 cat -n vi/vim :set number grep -n ‘.’ filename grep -n ‘$’ filename grep -n ‘^’ filename nl filename awk ‘{print NR,$0}’ filename sed ‘=’ filename less -N filename setfacl getfacl定时任务crond&ensp;&ensp;&ensp;分两种，一种是系统自动完成的，如日志，一种是用户安装的 crontab -l crontab -e crond 适合周期性任务。 at 适合执行一次，突发性任务。 anacron 适合非7*24小时开机的任务。 crond守护进程，一直运行，crontab是管理它的命令，crontab -l 列表，crontab -e 编辑。 参考资料 各种命令的man page]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中的sort命令]]></title>
    <url>%2F2017%2F08%2F01%2F2017-8-1-linuxsort%2F</url>
    <content type="text"><![CDATA[sort是linux中用于文本行排序的命令，它常和uniq命令搭配使用，是在linux文本处理中常用到的命令。 命令讲解命令格式:1sort [选项]...[文件]... 将所有文件的行进行排序输出。如一个文件也没有指定或者指定的文是-，那么将从标准输入读入内容。 常用选项： -c,--check 对检查文件中的行是否已经排好序，如果文件乱序，则输出第一个乱序的行的相关信息，最后返回1。这个选项本身不会对数据排序。 -r,--reverse sort默认升序排序，-r指定为降序排序。 -u,--unique 对结果进行去重，如果一行内容出现多次，只保留第一次。 -n,--numeric-sort 只对出现的数字排序,其他非数字的内容忽略，如一行的内容是ab2def3sd4，那么在排序时只会以234作为一行的内容进比较排序。 -t,--field-separator=SEP 指定分隔符SEP,使用SEP作为一行中字段的分隔符，而不是使用空白。 -k,--key=KEYDEF sort默认是使用整行进行排序，如果指定了-k选项，那将不使用整行而是使用行的一部分内容作为键进行排。 KEYDEF=pos1[,pos2] 表示根据第pos1字段到pos2字段之间的内容进行排序，如-k 2就是根据第二个字段进行排序。 每个pos还能指定从第几个字符开始进行排序。如-k3.1,4.3表示根据从第三个字段的第1个字符到第四个字段的第3个字符之间的内容进行排序。 一条sort命令可以指定多个-k选项，如-k 3 -k 4表示先根据第三字段进行排序，如果第三字段相同则根据第四字段排序。 在-k中还可以指定其他选项，如-k 3nr表示根据第三字段中的数字进行反向排序。 现在就能看懂KEYDEF的真正定义了： [ FStart [ .CStart ] ] [ Modifier ] [ , [ FEnd [ .CEnd ] ][ Modifier ] ] 实验123456# 查看文件内容$ cat testc2bsf2-c2-hehee3gsf5-c1-hehed1esf1-c5-heheb0dsf3-c4-hehe 123456# 排序$ sort testb0dsf3-c4-hehec2bsf2-c2-hehed1esf1-c5-hehee3gsf5-c1-hehe 123# 检查文件是否排好序$ sort -c testsort: te:3: disorder: d1esf1-c5-hehe 123456# 降序排列$ sort -r teste3gsf5-c1-hehed1esf1-c5-hehec2bsf2-c2-heheb0dsf3-c4-hehe 123456# 根据数字进行排序$ sort -n testb0dsf3-c4-hehec2bsf2-c2-hehed1esf1-c5-hehee3gsf5-c1-hehe 123456789101112# 去重$ cat filefedfde$ sort -u filedef 123456# 指定分隔符并根据第一个字段进行排序$ sort -t '-' -k 2 teste3gsf5-c1-hehec2bsf2-c2-heheb0dsf3-c4-hehed1esf1-c5-hehe 123456# 指定分隔符并根据从第一个字段的第二个字符到第二个字段的第一个字符之间的数字内容进行降序排序$ sort -t '-' -k1.2,2.1nr teste3gsf5-c1-hehec2bsf2-c2-hehed1esf1-c5-heheb0dsf3-c4-hehe 参考内容 Linux man page]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>命令</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j的安装与简单使用]]></title>
    <url>%2F2017%2F07%2F25%2F2017-07-15-neo4jstart%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;neo4j是一种图数据库，跟关系数据库不同的是它不仅能存储数据，还能存储数据之间的关系，数据存储在结点上，关系存储在边上。 安装 下载 地址 解压文件 tar -xf &lt;filecode&gt; 将文件主目录加入到$NEO4J_HOME中 启动服务 $NEO4J_HOME/bin/neo4j console 通过浏览器访问服务http://localhost:7474/ 第一次进入会提示修改密码。 使用参考链接]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lex模块的简单使用]]></title>
    <url>%2F2017%2F07%2F25%2F2017-07-25-lex%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;PLY是由python实现的编译器构建工具，其中包含lex和yacc两个模块。lex.py模块用来将输入字符通过一系列的正则表达式分解成标记序列，yacc.py通过一些上下文无关的文法来识别编程语言语法。yacc.py使用LR解析法，并使用LALR(1)算法（默认）或者SLR算法生成分析表。 &ensp;&ensp;&ensp;本文主要介绍lex.py，因为我最近用到它做一些字符串方面的数据处理问题，跟编译没关系，也就没看yacc。 安装&ensp;&ensp;&ensp;安装很简单： pip 安装：pip install ply 源码：PLY main下载完后使用tar -zxvf解压后可以复制里面的lex.py出来引用就可以了。 简介&ensp;&ensp;&ensp;lex.py将输入的串解析为一系列自定义的字符串。如输入串是下面： for i range(1, 10) &ensp;&ensp;&ensp;lex将串分解为标记： for, i, range, (, 1, ,, 10, ) &ensp;&ensp;&ensp;为了更好的区分和使用标记，每类标记取一个类型名字(type): 如： &ensp;&ensp;&ensp;ID, NUMBER, LPAREN, RPAREN, 等 那么原始串就是： (&#39;FOR&#39;, &#39;for&#39;), (&#39;ID&#39;, &#39;x&#39;), (&#39;IN&#39;, &#39;in&#39;), (&#39;RANGE&#39;, &#39;range&#39;), (&#39;LPAREN&#39;, &#39;(&#39;), (&#39;NUMBER&#39;, ‘1’), … 用法123456789101112131415161718192021222324252627282930313233343536373839404142import ply.lex as lex# List of token names. This is always requiredtokens = ( 'NUMBER', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN',)# Regular expression rules for simple tokenst_PLUS = r'\+'t_MINUS = r'-'t_TIMES = r'\*'t_DIVIDE = r'/'t_LPAREN = r'\('t_RPAREN = r'\)'# A regular expression rule with some action codedef t_NUMBER(t): r'\d+' t.value = int(t.value) return t# Define a rule so we can track line numbersdef t_newline(t): r'\n+' t.lexer.lineno += len(t.value)# A string containing ignored characters (spaces and tabs)t_ignore = ' \t'# Error handling ruledef t_error(t): print "Illegal character '%s'" % t.value[0] t.lexer.skip(1)# Build the lexerlexer = lex.lex() &ensp;&ensp;&ensp;上面的方式定义好了lex，下面的代码使用它: 12345678910111213# Test it outdata = '... '# Give the lexer some inputlexer.input(data)# Tokenizewhile True: tok = lexer.token() if not tok: break # No more input print tokfor tok in lexer: print tok &ensp;&ensp;&ensp;上面给出了两种迭代方式。 &ensp;&ensp;&ensp;由lexer.token()方法返回的标记是LexToken类型的实例，拥有tok.type,tok.value,tok.lineno和tok.lexpos属性，其中tok.type和tok.value分别代表标记的类型和值一个t.type的属性（字符串表示）来表示标记的类型名称，t.value是标记值（匹配的实际的字符串），t.lineno表示当前在源输入串中的作业行，t.lexpos表示标记相对于输入串起始位置的偏移 标记列表&ensp;&ensp;&ensp;上述代码中声明了一个标记列表，标记列表可以人为给每种类型标记起个名字，然后在后面方便使用。 123456789tokens = ( 'NUMBER', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN',) 标记的规则&ensp;&ensp;&ensp;每种标记用一个正则表达式规则来表示，每个规则需要以t_开头声明，表示该声明是对标记的规则定义。对于简单的标记，可以定义成这样（在Python中使用raw string能比较方便的书写正则表达式）：t_PLUS = r&#39;\+&#39; &ensp;&ensp;&ensp;这里，紧跟在t_后面的单词，必须跟标记列表中的某个标记名称对应。如果需要执行动作的话，规则可以写成一个方法。例如，下面的规则匹配数字字串，并且将匹配的字符串转化成Python的整型： 1234def t_NUMBER(t): r'\d+' t.value = int(t.value) return t &ensp;&ensp;&ensp;如果使用方法的话，正则表达式写成方法的文档字符串。方法总是需要接受一个LexToken实例的参数，该实例有一些属性（上面已经介绍）。方法可以在方法体里面修改这些属性。记住如果再方法中改变了属性，那么要返回结果token（如上面代码中的return t)，否则，标记将被丢弃。 &ensp;&ensp;&ensp;在lex内部，lex.py用re模块处理模式匹配，在构造最终的完整的正则式的时候，用户提供的规则按照下面的顺序加入： 所有由方法定义的标记规则，按照他们的出现顺序依次加入 由字符串变量定义的标记规则按照其正则式长度倒序后，依次加入（长的先入） &ensp;&ensp;&ensp;顺序的约定对于精确匹配是必要的。比如，如果你想区分=和==，你需要确保==优先检查。如果用字符串来定义这样的表达式的话，通过将较长的正则式先加入，可以帮助解决这个问题。用方法定义标记，可以显示地控制哪个规则优先检查。 标记的值&ensp;&ensp;&ensp;标记被lex返回后，它们的值被保存在value属性中。正常情况下，value是匹配的实际文本。事实上，value可以被赋为任何Python支持的类型。例如，当扫描到标识符的时候，你可能不仅需要返回标识符的名字，还需要返回其在符号表中的位置，可以像下面这样写： 1234567def t_ID(t): ... # Look up symbol table information and return a tuple t.value = (t.value, symbol_lookup(t.value)) ... return t 丢弃标记&ensp;&ensp;&ensp;想丢弃像注释之类的标记，只要不返回value就行了，像这样： 12345def t_COMMENT(t): r'\#.*' pass # No return value. Token discarded &ensp;&ensp;&ensp;为标记声明添加ignore_前缀同样可以达到目的： t_ignore_COMMENT = r&#39;\#.*&#39; &ensp;&ensp;&ensp;如果有多种文本需要丢弃，建议使用方法来定义规则，因为方法能够提供更精确的匹配优先级控制（方法根据出现的顺序，而字符串的正则表达式依据正则表达式的长度） 行号和位置信息&ensp;&ensp;&ensp;默认情况下，lex.py是不提供行号的。因为lex.py根本不知道何为”行”的概念（换行符本身也作为文本的一部分）。不过，可以通过写一个特殊的规则来记录行号： 123def t_newline(t): r'\n+' t.lexer.lineno += len(t.value) &ensp;&ensp;&ensp;在这个规则中，当前lexer对象t.lexer的lineno属性被修改了，而且空行被简单的丢弃了，因为没有任何的返回。 &ensp;&ensp;&ensp;lex.py也不自动做列跟踪。但是，位置信息被记录在了每个标记对象的lexpos属性中，这样，就有可能来计算列信息了。例如：每当遇到新行的时候就重置列值： 123456789# Compute column.# input is the input text string# token is a token instancedef find_column(input,token): last_cr = input.rfind('\n',0,token.lexpos) if last_cr &lt; 0: last_cr = 0 column = (token.lexpos - last_cr) + 1 return column &ensp;&ensp;&ensp;通常，计算列的信息是为了指示上下文的错误位置，所以只在必要时有用。 忽略字符&ensp;&ensp;&ensp;t_ignore规则比较特殊，是lex.py所保留用来忽略字符的，通常用来跳过空白或者不需要的字符。虽然可以通过定义像t_newline()这样的规则来完成相同的事情，不过使用t_ignore能够提供较好的词法分析性能，因为相比普通的正则式，它被特殊化处理了。 字面字符&ensp;&ensp;&ensp;字面字符可以通过在词法模块中定义一个literals变量，例如： literals = [ &#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;/&#39; ] 或者： literals = &quot;+-*/&quot; &ensp;&ensp;&ensp;字面字符是指单个字符，表示把字符本身作为标记，标记的type和value都是字符本身。不过，字面字符是在其他正则式之后被检查的，因此如果有规则是以这些字符开头的，那么这些规则的优先级较高。 错误处理&ensp;&ensp;&ensp;最后，在词法分析中遇到非法字符时，t_error()用来处理这类错误。这种情况下，t.value包含了余下还未被处理的输入字串，在之前的例子中，错误处理方法是这样的： 1234# Error handling ruledef t_error(t): print "Illegal character '%s'" % t.value[0] t.lexer.skip(1) &ensp;&ensp;&ensp;这个例子中，我们只是简单的输出不合法的字符，并且通过调用t.lexer.skip(1)跳过一个字符。 构建使用lexer&ensp;&ensp;&ensp;函数lex.lex()使用Python的反射机制读取调用上下文中的正则表达式，来创建lexer。lexer一旦创建好，有两个方法可以用来控制lexer对象： lexer.input(data) 重置lexer和输入字串 lexer.token() 返回下一个LexToken类型的标记实例，如果进行到输入字串的尾部时将返回None &ensp;&ensp;&ensp;推荐直接在lex()函数返回的lexer对象上调用上述接口，尽管也可以向下面这样用模块级别的lex.input()和lex.token()： 1234567lex.lex()lex.input(sometext)while 1: tok = lex.token() if not tok: break print tok &ensp;&ensp;&ensp;在这个例子中，lex.input()和lex.token()是模块级别的方法，在lex模块中，input()和token()方法绑定到最新创建的lexer对象的对应方法上。最好不要这样用，因为这种接口可能不知道在什么时候就失效。 @TOKEN装饰&ensp;&ensp;&ensp;在一些应用中，你可能需要定义一系列辅助的记号来构建复杂的正则表达式，例如： 1234567digit = r'([0-9])'nondigit = r'([_A-Za-z])'identifier = r'(' + nondigit + r'(' + digit + r'|' + nondigit + r')*)'def t_ID(t): # want docstring to be identifier above. ????? ... &ensp;&ensp;&ensp;在这个例子中，我们希望ID的规则引用上面的已有的变量。然而，使用文档字符串无法做到，为了解决这个问题，你可以使用@TOKEN装饰器： 123456from ply.lex import TOKEN@TOKEN(identifier)def t_ID(t): ... &ensp;&ensp;&ensp;装饰器可以将identifier关联到t_ID()的文档字符串上以使lex.py正常工作，一种等价的做法是直接给文档字符串赋值： 1234def t_ID(t): ...t_ID.__doc__ = identifier 调试&ensp;&ensp;&ensp;如果想要调试，可以使lex()运行在调试模式： lexer = lex.lex(debug=1) &ensp;&ensp;&ensp;这将打出一些调试信息，包括添加的规则、最终的正则表达式和词法分析过程中得到的标记。 &ensp;&ensp;&ensp;除此之外，lex.py有一个简单的主函数，不但支持对命令行参数输入的字串进行扫描，还支持命令行参数指定的文件名： 12if __name__ == '__main__': lex.runmain() &ensp;&ensp;&ensp;想要了解高级调试的详情，请移步至最后的高级调试部分。 其他方式定义词法规则&ensp;&ensp;&ensp;上面的例子，词法分析器都是在单个的Python模块中指定的。如果你想将标记的规则放到不同的模块，使用module关键字参数。例如，你可能有一个专有的模块，包含了标记的规则： 1234567891011121314151617181920212223242526272829303132333435363738394041# module: tokrules.py# This module just contains the lexing rules# List of token names. This is always requiredtokens = ( 'NUMBER', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN',)# Regular expression rules for simple tokenst_PLUS = r'\+'t_MINUS = r'-'t_TIMES = r'\*'t_DIVIDE = r'/'t_LPAREN = r'\('t_RPAREN = r'\)'# A regular expression rule with some action codedef t_NUMBER(t): r'\d+' t.value = int(t.value) return t# Define a rule so we can track line numbersdef t_newline(t): r'\n+' t.lexer.lineno += len(t.value)# A string containing ignored characters (spaces and tabs)t_ignore = ' \t'# Error handling ruledef t_error(t): print "Illegal character '%s'" % t.value[0] t.lexer.skip(1) &ensp;&ensp;&ensp;现在，如果你想要从不同的模块中构建分析器，应该这样（在交互模式下）： 123456789101112&gt;&gt;&gt; import tokrules&gt;&gt;&gt; lexer = lex.lex(module=tokrules)&gt;&gt;&gt; lexer.input("3 + 4")&gt;&gt;&gt; lexer.token()LexToken(NUMBER,3,1,1,0)&gt;&gt;&gt; lexer.token()LexToken(PLUS,'+',1,2)&gt;&gt;&gt; lexer.token()LexToken(NUMBER,4,1,4)&gt;&gt;&gt; lexer.token()None &ensp;&ensp;&ensp;module选项也可以指定类型的实例，例如： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import ply.lex as lexclass MyLexer: # List of token names. This is always required tokens = ( 'NUMBER', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN', ) # Regular expression rules for simple tokens t_PLUS = r'\+' t_MINUS = r'-' t_TIMES = r'\*' t_DIVIDE = r'/' t_LPAREN = r'\(' t_RPAREN = r'\)' # A regular expression rule with some action code # Note addition of self parameter since we're in a class def t_NUMBER(self,t): r'\d+' t.value = int(t.value) return t # Define a rule so we can track line numbers def t_newline(self,t): r'\n+' t.lexer.lineno += len(t.value) # A string containing ignored characters (spaces and tabs) t_ignore = ' \t' # Error handling rule def t_error(self,t): print "Illegal character '%s'" % t.value[0] t.lexer.skip(1) # Build the lexer def build(self,**kwargs): self.lexer = lex.lex(module=self, **kwargs) # Test it output def test(self,data): self.lexer.input(data) while True: tok = lexer.token() if not tok: break print tok# Build the lexer and try it outm = MyLexer()m.build() # Build the lexerm.test("3 + 4") # Test it 额外状态维护&ensp;&ensp;&ensp;在你的词法分析器中，你可能想要维护一些状态。这可能包括模式设置，符号表和其他细节。例如，假设你想要跟踪NUMBER标记的出现个数。 &ensp;&ensp;&ensp;一种方法是维护一个全局变量： 12345678num_count = 0def t_NUMBER(t): r'\d+' global num_count num_count += 1 t.value = int(t.value) return t &ensp;&ensp;&ensp;除此之外，还可以还可以记录信息到lexer对象内部。可以通过当前标记的lexer属性访问： 12345678def t_NUMBER(t): r'\d+' t.lexer.num_count += 1 # Note use of lexer attribute t.value = int(t.value) return tlexer = lex.lex()lexer.num_count = 0 # Set the initial count &ensp;&ensp;&ensp;还可以自定义你的lexer类型： 12345678910111213class MyLexer: ... def t_NUMBER(self,t): r'\d+' self.num_count += 1 t.value = int(t.value) return t def build(self, **kwargs): self.lexer = lex.lex(object=self,**kwargs) def __init__(self): self.num_count = 0 参考链接 PLY (Python Lex-Yacc)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>lex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP工具基本使用:StanfordNLP和SnowNLP]]></title>
    <url>%2F2017%2F07%2F21%2F2017-07-21-nlptools%2F</url>
    <content type="text"><![CDATA[正文The blog show the simple use of the Nlp tools: SnowNlpenvpython install install：pip install snownlp use1234567891011121314151617181920212223242526content = '...'s = SnowNLP(content)print("snownlp split word")swords = s.wordsprint(swords)print("snownlp cixing")print(*s.tags)print("snownlp tf-idf")print(s.tf)print(s.idf)print("tokenization")print(s.sentences)print("sentiment")print(s.sentiments)print("keywords")print(s.keywords(3))print("summary")print(s.summary(3)) StanfordNLPenvjava install download the StanfordCoreNLP(the core api of the stanfordnlp).unzip it and import all the jars to build path. dowwload the StanfordNLP-chinese(the language jar needed for the project).import it to the build path. usecode: 12345678910111213141516171819202122232425 String text = "...";String props = "StanfordCoreNLP-chinese.properties";StanfordCoreNLP pipeline = new StanfordCoreNLP(props);Annotation annotation = new Annotation(text);pipeline.annotate(annotation);List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);for (CoreMap sentence : sentences)&#123; for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class))&#123; String word = token.get(CoreAnnotations.TextAnnotation.class); //分词 String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class); //分词位置 String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class); //实体 System.out.println(word + "\t" + pos + "\t" + ne); &#125; Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class); System.out.println("语法树："); System.out.println(tree.toString()); SemanticGraph dependencies = sentence.get(SemanticGraphCoreAnnotations.EnhancedDependenciesAnnotation.class); System.out.println("依存句法："); System.out.println(dependencies.toString()); &#125; warningyou need adjust the jvm memory before you run it. 参考链接 StanfordNLP]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用scrapy的写并行爬虫]]></title>
    <url>%2F2017%2F07%2F18%2F2017-07-18-scrapymid%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;前段时间我们写了爬取电视剧，今天我把它改变成了爬取游戏，然后boss又来需求了，让这两个并行爬取（因为后面会使用并行爬取加快爬取速度），然后就自己鼓捣了一下。 &ensp;&ensp;&ensp;先跑去官网看了一下。 &ensp;&ensp;&ensp;找到了如何并行运行爬虫： 123456789101112131415import scrapyfrom scrapy.crawler import CrawlerProcessclass MySpider1(scrapy.Spider): # Your first spider definition ...class MySpider2(scrapy.Spider): # Your second spider definition ...process = CrawlerProcess()process.crawl(MySpider1)process.crawl(MySpider2)process.start() # the script will block here until all crawling jobs are finished &ensp;&ensp;&ensp;这是一个简单的python脚本，使用python script_name即可运行它，然后自己写了个脚本运行，发现这种方法有个缺陷：不能经过pipeline处理。 &ensp;&ensp;&ensp;那怎么办？要想使用pipeline，需要使用scrapy的命令，如scrapy crawl spider_name，但是现有的scrapy命令不提供咱们要实现的功能怎么办？自定义呗^-^，于是一查，果然scrapy提供了自定义命令： 在与spiders同级的目录下创建目录，mkdir commands 在commands目录中添加__init__.py文件 在commands目录创建crawlall.py文件（文件名自拟，只要跟后面的配置相同即可),并写入下面代码 settings.py目录下创建setup.py，写入： 123456789from setuptools import setup, find_packagessetup(name='scrapy-mymodule', entry_points=&#123; 'scrapy.commands': [ 'crawlall=cnblogs.commands:crawlall', ], &#125;, ) &ensp;&ensp;&ensp;这个文件的含义是定义了一个crawlall命令，cnblogs.commands为命令文件目录，crawlall为命令名。 在settings.py中添加配置： COMMANDS_MODULE = &#39;project_name.commands&#39; 运行命令scrapy crawlall&ensp;&ensp;&ensp;写入setup.py中的代码 12345678910111213141516171819202122232425262728293031323334353637383940from scrapy.commands import ScrapyCommandfrom scrapy.crawler import CrawlerRunnerfrom scrapy.utils.conf import arglist_to_dictclass Command(ScrapyCommand): requires_project = True def syntax(self): return '[options]' def short_desc(self): return 'Runs all of the spiders' def add_options(self, parser): ScrapyCommand.add_options(self, parser) parser.add_option( "-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE", help="set spider argument (may be repeated)") parser.add_option("-o", "--output", metavar="FILE", help="dump scraped items \ into FILE (use - for stdout)") parser.add_option("-t", "--output-format", metavar="FORMAT", help="format to use for dumping items with -o") def process_options(self, args, opts): ScrapyCommand.process_options(self, args, opts) try: opts.spargs = arglist_to_dict(opts.spargs) except ValueError: raise UsageError( "Invalid -a value, use -a NAME=VALUE", print_help=False) def run(self, args, opts): spider_loader = self.crawler_process.spider_loader for spidername in args or spider_loader.list(): self.crawler_process.crawl(spidername, **opts.spargs) self.crawler_process.start() &ensp;&ensp;&ensp;观察上面的代码，大部分都看不懂，但是run函数内的东西应该能看懂，大题知道它是在拿到所有的爬虫并运行它，那么事情就简单了，我们对爬虫的调度工作就放在这里了： 123456def run(self, args, opts): runner = CrawlerRunner() runner.crawl(GamespiderSpider) runner.crawl(TengxunSpider) d = runner.join() d.addBoth(lambda _: reactor.stop()) &ensp;&ensp;&ensp;运行发现可以抓取但是还是无法使用pipeline，对比代码发现我们的代码再run,crawl函数中没有传递**opts.spargs，那么现在一切都明了了，前面的脚本无法使用pipeline是因为无法得到相关参数（args,opts),这些参数与scrapy相关，包含了框架的一些行为和信息。 123456def run(self, args, opts): runner = CrawlerRunner() runner.crawl(GamespiderSpider, **opts.spargs) runner.crawl(TengxunSpider, **opts.spargs) d = runner.join() d.addBoth(lambda _: reactor.stop()) &ensp;&ensp;&ensp;改为上面的代码可以完美完成任务。 参考链接]]></content>
      <categories>
        <category>crawl</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>crawl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy初涉]]></title>
    <url>%2F2017%2F07%2F17%2F2017-07-17-scrapystart%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;scrapy是一个开源的爬虫框架，由于实习需要，今天就看了一下，用这个框架的话很省很多时间啊，以前写爬虫时使用requests或者urllib，都是从无一点一点向上写，现在scrapy帮我们实现了一些底层的东西，比如获得一个页面。下面讲一下scrapy的基本用法。 &ensp;&ensp;&ensp;scrapy的基本结构： &ensp;&ensp;&ensp;包含的组件： 引擎(Scrapy Engine) 调度器(Scheduler) 下载器(Downloader) 蜘蛛(Spiders) 项目管道(Item Pipeline) 下载器中间件(Downloader Middlewares) 蜘蛛中间件(Spider Middlewares) 调度中间件(Scheduler Middlewares) &ensp;&ensp;&ensp;工作流程：–&gt;绿线是数据流向 从初始的URL开始，Scheduler会将其交给Downloader进行下载 下载之后会交给Spider进行分析 Spider分析出来的结果有两种 一种是需要进一步抓取的链接，如”下一页”的链接，它们会被传回Scheduler 另一种是需要保存的数据，它们被送到Item Pipeline里进行后期的处理（详细分析，过滤，存储等） 在数据流动的通道里还可以安装各种各样的中间件，进行必要的处理。 scrapy基本命令&ensp;&ensp;&ensp;创建scrapy项目： scrapy startproject first_crawl &ensp;&ensp;&ensp;通过tree命令查看项目结构: scrapy shellscrapy shell http://你要调试xpath的网址 爬取腾讯视频的所有电视剧&ensp;&ensp;&ensp;编写item，item的定义再items.py文件中。 1234class TVItem(scrapy.Item): title = scrapy.Field() iid = scrapy.Field() content = scrapy.Field() &ensp;&ensp;&ensp;编写Spiders，spider 可以使用scrapy genspider spider_name生成。 123456789101112131415161718192021222324252627282930313233class TengxunSpider(scrapy.Spider): name = 'tengxun' start_urls = ['http://v.qq.com/x/list/tv'] # start_urls = ['http://v.qq.com/x/list/tv?&amp;offset=4980'] def parse(self, response): for tv in response.xpath( '/html/body/div[3]/div/div/div[1]/div[2]/div/ul/li'): item = TVItem() item['title'] = tv.xpath('div[1]/strong/a/text()').extract_first() item['iid'] = tv.xpath('div[1]/strong/a/@href').extract_first() content = dict() content['score'] = tv.xpath( 'div[1]/div/em[1]/text()').extract_first( ) + tv.xpath('div[1]/div/em[2]/text()').extract_first() actors = [] for tvp in tv.xpath('div[2]/a/text()'): actors.append(tvp.extract()) content['actors'] = actors # item['content'] = json.dumps(content) item['content'] = content # print(item['name']) # print(item['score_l']) # print(item['score_s']) # print(item['actors']) yield item next_page = response.xpath( '/html/body/div[3]/div/div/div[2]/a[2]/@href').extract_first() if next_page != 'javascript:;': print(type(next_page)) url = urljoin(response.url, next_page) yield scrapy.Request(url, callback=self.parse) &ensp;&ensp;&ensp;编写pipeline.py文件。 123456789class TVPipeline(object): def process_item(self, item, spider): tmp = item['iid'][item['iid'].rfind('/') + 1:] tmp = tmp[:tmp.rfind('.')] item['iid'] = 'tv' + tmp # with open('tv.txt', 'a+') as f: # f.write(item['iid'] + ' ' + item['title'] + # ' ' + item['content'] + '\n') return item &ensp;&ensp;&ensp;这样就完成了一个爬取电视剧的爬虫。 参考链接]]></content>
      <categories>
        <category>crawl</category>
      </categories>
      <tags>
        <tag>crawl</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[m-Rnn]]></title>
    <url>%2F2017%2F07%2F04%2F2017-07-04-m-Rnn%2F</url>
    <content type="text"><![CDATA[论文地址 文章提出一种多模态图像文本匹配的方法，使用CNN-RNN架构，如下图。 其中前两层是词嵌入层，讲每一个词变为一个稠密向量。接下来是recurrent层，用来捕捉句子的语义和语法信息。这里注意训练好词向量之后不是将它们拼接起来而是输入到recurrent层，用来捕捉句子的语义和语法信息。 我们先将上一个状态r(t-1)映射到词向量空间内w(t),然后再加上w(t)。 下一层是多模态层，将文本向量，图像向量和recurrent层输出向量映射成为到一个统一空间。 再相加得到多模态的输出。 其中g函数使用如下： 公式原文 将值映射到一个相比双曲正切更加非线性的范围，而且收敛速度更快。 最后一层是softmax层，输出概率。 其中L代表句子长度，wi代表词，I代表图像。 N代表训练集单词数，Ns代表训练集中句子的个数。 最小化cost相当于最大化由图像产生句子的最大概率。 数据集 IAPR TC-12 20000张图片，每张图片评价1.7个描述，每张图片至少一个描述。 Flickr8K 8000张图片，每张图片5个描述，6000-train 1000-test 1000-vaildate Flickr30K 31783张图片，每张图片5个描述。 MS COCO 82783张图片，每张图片5个描述。 部分实验结果 总结m-rnn是一个先生成后检索模型，其应该是第一个使用 CNN + RNN 这种 encoder-decoder 模型来做图文相关任务. 使用 VggNet 来抽取图片特征 使用两个 Embedding 层来对每个单词进行稠密特征编码 多模态部分的输入有图片特征、单词编码和上下文信息 目标函数是给定图片 I，使得生成的图片描述尽可能像图片的真实描述 S 。 image的特征并没有输入到RNN]]></content>
  </entry>
  <entry>
    <title><![CDATA[图文匹配论文阅读]]></title>
    <url>%2F2017%2F07%2F01%2F2017-07-01-image-text-mapping%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;今天来一起阅读两篇图文匹配比较早起的论文：KCCA方法和Deep Fragment KCCA Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics (Extended Abstract) &ensp;&ensp;&ensp; 提出了一种新的数据集PASCAL VOC-2008和Flickr 8K。其中前者有1000张图片，后者有8092张图片。每张图片对应五个对其的描述。提出一种基于KCCA的图像文本匹配检索模型。提出看待图像描述问题的新角度：检索角度，Ranking-based。提出一种评价Ranking-based system的评价标准，用以前推荐系统用的Recall@k评价。 基于检索 Deep Fragment Embeddings for Bidirectional Image Sentence Mapping &ensp;&ensp;&ensp;文章提出了一种图像文本匹配的模型。不同于前面已有的直接将图像与文本映射到一个相同的空间中进行计算相似度。本文在更细的粒度上进行映射：即先将图像跟文本片段化，在将这些片段映射到一个共同的空间。在前面已有的ranking-objective基础上，本文设置两个ranking-objective，一个全局的一个局部片段的。&ensp;&ensp;&ensp;本文的主要视角是图像的结构是非常复杂的，包含很多相互作用的物体。于是我们就将图片和文本分割成小片段得到他们之间隐含的关系。这里我们有一个基本的假设：图像对应的文本是本图片中包含的物体关系是很大的。而不是抽象到更高的层次。 &ensp;&ensp;&ensp;我们使用物体探测得到的物体作为图像片段，使用句子依赖树中的关系作为文本的片段。 文本片段化。我们希望将文本中描述的物体及其他们之间的关系抽取出来。所以我们使用句子依赖树，本文抛弃句子依赖树中的树结构，而使用树中没一个关系作为片段。如下图，将每一个词用词向量表示，然后用下面的式子将没一个关系依赖映射到嵌入空间。 其中w1表示词的one-hot向量，Wew1为词的词向量。WR和bR为每个关系的权重矩阵，s为关系最终在嵌入空间内的表示，s的维度可以通过交叉验证调节。 图片片段化我们假设图片的描述都是基于图片中的物体和图片的整体上下文。那么很自然，图片的整体上下文和其中的物体就可以作为图片的fragements。通过RCNN网络得到物体，然后将每个物体和整张图片输入下面的式子映射到嵌入空间。 目标函数本文目标函数由两部分组成，一部分是全局目标函数，匹配的图像和文本的分数应该高于不匹配的，一部分是fragement层次上的目标函数，用于控制fragement之间的匹配。 其中theta是参数的表示， 片段对齐目标函数fragement alignment objective直觉上就是句子中的一个片段如blue ball，必然在图像中有一个fragement与其对应且分数很高，其它图像中的片段与blue ball的匹配分数很低。其实这个规则很容易被打破 句子的片段有可能不跟图片中跟任何片段相关。 句子的片段所对应的图像中的物体可能没有被RCNN检测出来。 其他图片中有这个片段但是与其对应的描述中忽略了对这个片段的描述。即使如此，这个规则在大多数情况下是能够很好使用的。下面就是fragement alignment objective:上面式子中的求和是对训练集中的所有片段进行球和。ViTSj代表两段vi和sj的匹配分数。yij,代表片段vi和sj是否出现在同一个图像文本对中，如果出现则为1,否则为-1。 而上面的目标函数假设fragement中的对应是稠密的，但是现实生活中往往不是这样的，比如下图中”boy playing”只对应一个图像fragement，于是我们使用多实例学习。 上图中代表负样例，绿色代表正样例，黄色代表在正样例包里的临时负样例。 MILMIL2 最终的目标函数：其中是文本段j对应的正样例bag,mv(i)和ms(j)代表段vi和sj对应的图片和文本在训练数据的位置，其实最后一行表明的是yij只能取值-1和1，然后不对应于同一个图像-文本对的任意vi和sj段的yij值是-1。第一个等式的意思是保证每个正样例bag中至少有一个正样例。 这个目标函数的意思就是为了防止漏掉一些没有检测到的文本fragement和正样例bag中的一些图像fragement的对应。让正样例bag中的fragement对应的yij不在是常量而是变量，然后取最小的目标函数至即可。 全局目标函数 Recall that the Global Ranking Objective ensures that the computed image-sentence similarities are consistent with the ground truth annotation. 其中gk代表第k张图像对应的fragement集合，gl则是第l个句子对应的fragement集合。用max截断是因为不应太小，再小也是负样本。 上面的全局目标函数使得匹配的图像文本Skk的分数比Skl和Slk的分数至少delta。 实验]]></content>
      <tags>
        <tag>papper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中的uniq命令]]></title>
    <url>%2F2017%2F06%2F30%2F2017-6-30-linuxuniq%2F</url>
    <content type="text"><![CDATA[uniq是lunix中一个很常用的命令，常用来做文本处理，对文本行进行去重过滤。 命令讲解找出或忽略重复的行。 命令格式:1uniq [OPTION]... [INPUT [OUTPUT]] 对输入的数据中相邻的行进行过滤输出到给定的输出项上。默认情况下是将相邻行中重复的项去掉，只留下重复项中的第一个。 这里要注意，uniq是对相邻行进行过滤，所以要使uniq达到预期结果，一般要先对uniq的输入进行排序。 常用选项： -c, --count 过滤掉重复行，并在输出的每一行前面加上此行出现的次数。 -d, --repeated 只显示在文件中重复的行。 -u, --unique 只显示文件中不重复的行。 -n 在进行每一行的比较时，前n个字段以及他们之间的空白都不参与比较。 +n 在进行每一行的比较时，前n个字符不参与比较。 -f n 与-n相同，这里n是字段数。 -s n 与+n相同，这里n是字符数。 实验结果123456789# 文件内容$ cat testa hehe ab hehe bc hehe cc hehe dc hehe df hehe ff hehe f 注意在文件中如果只有一个回车，那么此行会被认识空行也会被任务是一行，输出。 12345678# 过滤行并显示次数。$ uniq -c test1 a hehe a1 b hehe b1 c hehe c2 c hehe d2 f hehe f1 1234# 只显示重复的行。$ uniq -d testc hehe df hehe f 12345# 只显示没有重复的行。$ uniq -u testa hehe ab hehe bc hehe c 1234567# 忽略第一个字段。$ uniq -f 1 testa hehe ab hehe bc hehe cc hehe df hehe f 1234567# 忽略第一个字符，其实效果和上一个一样。$ uniq -s 1 testa hehe ab hehe bc hehe cc hehe df hehe f 参考内容 Linux man page]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本管理工具之git使用]]></title>
    <url>%2F2017%2F06%2F19%2F2017-06-19-git%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;git作为最流行的版本控制工具之一，其优点有很多，能够分布式的进行版本管理，而且每个用户端都是一个独立的仓库，可以进行提交。 版本控制&ensp;&ensp;&ensp;对于一些需要多个人协作完成的工程项目，这时每个人可能负责的功能不同，每个人都在自己的本地机器开发，然后需要将每个人的代码合并到一起组成完整的项目。 &ensp;&ensp;&ensp;这个时候问题就来了，每个人提交的代码有可能会产生冲突，比如A在一个文件处修改了一行，而B也在这里修改了这一行，但是修改的不同，那么在合并的时候就产生冲突了，为了避免、发现、解决冲突，版本管理的理念应运而生。 参考链接]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goole开源构建工具bazel的简单使用]]></title>
    <url>%2F2017%2F06%2F09%2F2017-06-09-bazelstart%2F</url>
    <content type="text"><![CDATA[正文bazel概述&ensp;&ensp;&ensp;bazel是一个构建代码工具，能对代码进行编译和测试，可以支持几乎任何语言。 &ensp;&ensp;&ensp;bazel通过一个名为BUILD的文件进行构建代码，BUILD文件中写有构建代码的规则，这些规则语法简单，很容易书写，我们可以通过书写这些规则来规定我们需要的代码构建方式，得到我们自己需要的结果。 &ensp;&ensp;&ensp;下面展示了c语言的部分规则。 1234567891011121314cc_library( name = "hello-time", srcs = ["hello-time.cc"], hdrs = ["hello-time.h"],)cc_binary( name = "hello-world", srcs = ["hello-world.cc"], deps = [ ":hello-time", "//lib:hello-greet", ],) &ensp;&ensp;&ensp;有的时候代码中是存在依赖关系的，观察上面的第二条构建规则，其中deps代表依赖，表示hello-world依赖于：hello-time和//lib:hello-greet。 bazel会根据BUILD文件生成一张依赖图，这张依赖图会一直保存在内存中，以让后面 bazel进行增量构建和并行计算使用。 下图是上面展示规则的依赖图 &ensp;&ensp;&ensp;这种依赖图，可以通过bazel的命令来生成它。 &ensp;&ensp;&ensp;基于规则的构建能够生成正确的、可重用的构建结果，缓存的存在使得bazel能够进行增量构建。 快速使用bazel创建工作区&ensp;&ensp;&ensp;bazel是通过工作区为单位来工作的，要进行构建的一个任务的源码必须放在一个工作区内，这样在构建完成后就会在工作区内生成构建结果(如,bazel-bin 和bazel-out)。那么究竟什么是工作区呢？很简单，就是一个包含有名为WORKSPACE文件的目录，目录的位置在哪不重要，重要的是一定要包含一个名为WORKSPACE的文件，这个WORKSPACE文件可以是空白的，但是必须存在，也可以在其中加入一些控制内容，这个不是本文的范畴，所以不做讨论。 所以创建工作区即为： 123mkdir testcd testtouch WORKSPACE 创建BUILD文件&ensp;&ensp;&ensp;bazel使用BUILD来指定代码构建的动作，包括怎么构建，构建的结果，构建的源文件等。 &ensp;&ensp;&ensp;BUILD内一般包含一系列的规则声明。这些规则声明指定输入和输出，还有从输入到输出的计算如何进行。 下面就是一个简单的规则声明： 12345genrule( name = "hello", outs = ["hello_world.txt"], cmd = "echo Hello World &gt; $@",) 然后就能通过下面的命令构建它： bazel build :hello 命令行出现如下信息： 1234INFO: Found 1 target...Target //:hello up-to-date: bazel-genfiles/hello_world.txtINFO: Elapsed time: 2.255s, Critical Path: 0.07s &ensp;&ensp;&ensp;输出的目标我们可以通过它的标签（name的值）来使用。bazel的构建结果放在另一个目录中，bazel-genfiles目录只是一个符号文件，它的真实位置在别处。这样做的目的是不污染源代码目录树。 &ensp;&ensp;&ensp;一个规则可以使用其他规则输出作为输入，通过它的标签指定即可。如下： 123456789101112genrule( name = "hello", outs = ["hello_world.txt"], cmd = "echo Hello World &gt; $@",)genrule( name = "double", srcs = [":hello"], outs = ["double_hello.txt"], cmd = "cat $&lt; $&lt; &gt; $@",) 构建c++代码&ensp;&ensp;&ensp;这里以c++为例展示一下用bazel构建程序的方法。 创建工作区&ensp;&ensp;&ensp;创作下面目录及文件形成工作区。 1234567891011└── my-project ├── lib │ ├── BUILD │ ├── hello-greet.cc │ └── hello-greet.h ├── main │ ├── BUILD │ ├── hello-time.cc │ ├── hello-time.h │ └── hello-world.cc └── WORKSPACE 写入源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364cd ~/gitroot/my-projectmkdir ./maincat &gt; main/hello-world.cc &lt;&lt;'EOF'#include "lib/hello-greet.h"#include "main/hello-time.h"#include &lt;iostream&gt;#include &lt;string&gt;int main(int argc, char** argv) &#123; std::string who = "world"; if (argc &gt; 1) &#123; who = argv[1]; &#125; std::cout &lt;&lt; get_greet(who) &lt;&lt;std::endl; print_localtime(); return 0;&#125;EOFcat &gt; main/hello-time.h &lt;&lt;'EOF'#ifndef MAIN_HELLO_TIME_H_#define MAIN_HELLO_TIME_H_void print_localtime();#endifEOFcat &gt; main/hello-time.cc &lt;&lt;'EOF'#include "main/hello-time.h"#include &lt;ctime&gt;#include &lt;iostream&gt;void print_localtime() &#123; std::time_t result = std::time(nullptr); std::cout &lt;&lt; std::asctime(std::localtime(&amp;result));&#125;EOFmkdir ./libcat &gt; lib/hello-greet.h &lt;&lt;'EOF'#ifndef LIB_HELLO_GREET_H_#define LIB_HELLO_GREET_H_#include &lt;string&gt;std::string get_greet(const std::string &amp;thing);#endifEOFcat &gt; lib/hello-greet.cc &lt;&lt;'EOF'#include "lib/hello-greet.h"#include &lt;string&gt;std::string get_greet(const std::string&amp; who) &#123; return "Hello " + who;&#125;EOF 编写BUILD文件参考链接 bazel google documentation]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>bazel</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法题系列--风口的猪-中国牛市]]></title>
    <url>%2F2017%2F06%2F08%2F2017-06-08-fengkou%2F</url>
    <content type="text"><![CDATA[题目描述风口之下，猪都能飞。当今中国股市牛市，真可谓“错过等七年”。 给你一个回顾历史的机会，已知一支股票连续n天的价格走势，以长度为n的整数数组表示，数组中第i个元素（prices[i]）代表该股票第i天的股价。 假设你一开始没有股票，但有至多两次买入1股而后卖出1股的机会，并且买入前一定要先保证手上没有股票。若两次交易机会都放弃，收益为0。 设计算法，计算你能获得的最大收益。 输入数值范围：2&lt;=n&lt;=100,0&lt;=prices[i]&lt;=100输入例子:3,8,5,1,7,8 输出例子:12 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657int calculateMax(vector&lt;int&gt; prices) &#123; int n = prices.size(); if(n &lt; 2) return 0; if(n == 2) return prices[1] - prices[0]; int *t = new int[prices.size() + 10]; //t[i] 代表到0-i为止进行一次买和一次卖能得到的最大收益 t[0] = 0; int mmin = prices[0]; int max_res = 0; for (int i = 1; i &lt; n; ++i) //求t[i] &#123; //printf("%d %d %d\n", i, mmin, prices[i]); int ans = prices[i] - mmin; if (ans &gt; max_res) max_res = ans; //printf("%d\n", max_res); t[i] = max_res; if(mmin &gt; prices[i]) mmin = prices[i]; &#125; int *s = new int[n + 10]; //s[i]代表从n-2到i(逆序)为止进行一次买和卖能得到的最大收益 s[n-1] = 0; int mmax = prices[n-1]; max_res = 0; for (int i = n - 2; i &gt;= 0; --i) &#123; int ans = mmax - prices[i]; if (ans &gt; max_res) max_res = ans; s[i] = max_res; if(mmax &lt; prices[i]) mmax = prices[i]; &#125; max_res = 0; for (int i = 0; i &lt; n - 1; ++i) &#123; int ans = t[i] + s[i+1]; if(max_res &lt; ans) max_res = ans; &#125; for (int i = 0; i &lt; n; ++i) &#123; if(max_res &lt; s[i]) max_res = s[i]; if(max_res &lt; t[i]) max_res = t[i]; &#125; //printf("---s"); //for (int i = 0; i &lt; n; ++i) //printf("%d %d\n", t[i], s[i]); return max_res; &#125;]]></content>
      <categories>
        <category>思维算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>模拟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LCA倍增算法]]></title>
    <url>%2F2017%2F06%2F03%2F2015-06-03-lca%2F</url>
    <content type="text"><![CDATA[正文lca问题&ensp;&ensp;&ensp;lca问题即最近公共祖先问题，ji 参考链接]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>lca</tag>
        <tag>倍增算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim宏录制]]></title>
    <url>%2F2017%2F06%2F03%2F2017-06-03-vimdefine%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;在vim编辑文件时，有时会遇到要执行同一个动作序列多次的情况，这时就会用到宏录制。 下面我们就从例子出发讲解宏录制。 比如文本中的内容是： 1234567this is a doghehehehewoainiyou is a dogfuck youho ho hoI love you 现在问题来了，我想在文本的每一行前面加入一个#号。怎么办？先考虑如果是一行的话怎么改？ 先把光标停在行的第一个字符，然后按i键进入插入模式，然后输入#，再按Esc键退出插入模式，最后按j键到达下一行行首。这样就改完了一行，如果改下一行可以在下一行行首按.键，但是就这样一行一行的改？那么如果有一百行咋办？那么这就需要宏录制了，宏录制的原理就是将一系列的操作记录下来，然后如果有要进行这系列操作多次的话，只需要播放这个记录多次即可。就类似复读机。 宏录制步骤 按下q间和一个字母键，如qa、qb、… 进行一系列操作 按下q键，这样前面进行的一系列操作就保存在名为前面按下的字母键的记录中 按下number@加前面按下的字母键即可让前面的一系列操作执行多次 实验案例还是以前面的案例为准 1234567this is a doghehehehewoainiyou is a dogfuck youho ho hoI love you 将光标移到第一行的地一个字符上 按下qa键 然后按i键进入插入模式，然后输入#，再按Esc键退出插入模式，最后按j键到达下一行行首 按下q键 按下6@a键（为什么是6?一共7行，已经修改了1行，还剩6行） 完成操作 实验结果：1234567#this is a dog#hehehehe#woaini#you is a dog#fuck you#ho ho ho#I love you 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>vim</tag>
        <tag>宏录制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyinotify模块--python监视文件系统的利器]]></title>
    <url>%2F2017%2F06%2F02%2F2015-06-02-pyinotify%2F</url>
    <content type="text"><![CDATA[正文&ensp;&ensp;&ensp;pyinotify模块用来对Linux中的文件系统进行监视，能够实时捕捉到文件系统的变化—增删改查等。pyinotify不仅能对文件监视还能对目录监视，而且当监视对象处于可移动介质时，那么umount此介质上的文件系统后，被监视目标对用的watch对象将被自动删除，并且会产生一个umount事件。 查看内核是否支持inotify机制 grep INOTIFY_USER /boot/config-$(uname -r) 输出：CONFIG_INOTIFY_USER=y 表示支持inotify机制 Inotify事件 IN_ACCESS，即文件被访问 IN_MODIFY，文件被 write IN_ATTRIB，文件属性被修改，如 chmod、chown、touch 等 IN_CLOSE_WRITE，可写文件被 close IN_CLOSE_NOWRITE，不可写文件被 close IN_OPEN，文件被 open IN_MOVED_FROM，文件被移走,如 mv IN_MOVED_TO，文件被移来，如 mv、cp IN_CREATE，创建新文件 IN_DELETE，文件被删除，如 rm IN_DELETE_SELF，自删除，即一个可执行文件在执行时删除自己 IN_MOVE_SELF，自移动，即一个可执行文件在执行时移动自己 IN_UNMOUNT，宿主文件系统被 umount IN_CLOSE，文件被关闭，等同于(IN_CLOSE_WRITE | IN_CLOSE_NOWRITE) IN_MOVE，文件被移动，等同于(IN_MOVED_FROM | IN_MOVED_TO) APIAPI WatchManager类&ensp;&ensp;&ensp;pyinotify中使用watch对象进行对文件系统的监视，watch对象中有对监视对象的操作。那么管理这些watch对象的对象就是WatchManager对象。它封装了对 watch的管理操作，我们只需要添加包含特定操作的watch对象到WatchManager即可，剩余的对watch的操作WatchManager会自行完成。 常用方法： add_watch(self, path, mask, proc_fun=None, rec=False, auto_add=False, do_glob=False, quiet=True, exclude_filter=None) path 要监视的文件系统的路径 mask 要监听的文件系统的事件 rec 如果为True, 则递归的将此watch添加到文件系统下的每个子目录上，默认为 False。 auto_add 如果为True，则在监视的文件系统中创建新文件或目录时，会自动将watch 添加到新文件或目录上。 return: dict of {str: int} 返回值是一个字典，字典的key代表路径，value代表路径对应的文件系统的watch对象的wd值（可以理解为用来标识watch的值）。 代码：12wm = WatchManager()wm.add_watch(path, mask, auto_add=True, rec=True) ProcessEvent类&ensp;&ensp;&ensp;ProcessEvent对象用来处理文件系统的事件。一般使用时会集成它，然后重写它内部的相应事件方法。 事件方法： 方法名用process_加上上面提到的事件即可。如process_IN_CLOSE、 process_IN_CREATE等。 代码：123456789class EventHandler(ProcessEvent): def process_IN_CREATE(self, event): print("Create file:%s" % os.path.join(event.path, event.name)) def process_IN_DELETE(self, event): print("Delete file:%s" % os.path.join(event.path, event.name)) def process_IN_MODIFY(self, event): print("Modify file:%s" % os.path.join(event.path, event.name)) Notifier类&ensp;&ensp;&ensp;Notifier类接受事件并处理它。它会维持一个事件队列，每次从事件队列中取出事件处理，新事件也加入队列。那么事件从哪来？看了上面可以知道，WatchManager用来管理所有的watch及watch观察到的事件。怎么处理事件？由上一节可以知道用户定义的ProcessEvent子类用来处理事件。所以先要为Notifier指定WatchManager和ProcessEvent对象。 notifier = Notifier(wm, EventHandler()) 接下来就需要处理事件了，这里使用的是同步方式（异步方式可以自行查阅 API)。 这里依赖两个函数： process_events(self) 函数从事件队列中取事件，然后交给相应的事件处理对象 (ProcessEvent类或其子类的对象)处理。 check_events(self, timeout=None) 检查文件系统是否产生新事件, 直到超时timeout毫秒就阻塞它。如果有，返回True。 read_events(self) 读取事件并为其创建相应数据结构，然后加入事件队列。 stop(self) 关闭notifier，销毁所有的watch对象，事件等。 12345678while(True): try: notifier.process_events() if(notifier.check_events()): notifier.read_events() except keyboardInterrupt: notifier.stop() break 完整代码：12345678910111213141516171819202122232425262728293031import osfrom pyinotify import WatchManager, Notifier, ProcessEvent, IN_CREATE, IN_DELETE, IN_MODIFYclass EventHandler(ProcessEvent): def process_IN_CREATE(self, event): print("Create file:%s" % os.path.join(event.path, event.name)) def process_IN_DELETE(self, event): print("Delete file:%s" % os.path.join(event.path, event.name)) def process_IN_MODIFY(self, event): print("Modify file:%s" % os.path.join(event.path, event.name))def FSMonitor(path="."): wm = WatchManager() mask = IN_DELETE | IN_CREATE | IN_MODIFY notifier = Notifier(wm, EventHandler()) wm.add_watch(path, mask, auto_add=True, rec=True) print("now starting monitor %s" % path) while(True): try: notifier.process_events() if(notifier.check_events()): notifier.read_events() except keyboardInterrupt: notifier.stop() breakif __name__ == "__main__": FSMonitor("/home/hadis/Documents/doc/doc/pro/python") 实验结果： 在目录下创建一个test文件夹后： &ensp;&ensp;&ensp;好了,pyinotify的简单应用就介绍到这里了，它的更深入的应用就靠各位根据API慢慢探索了。 参考链接 API python pyinotify模块详解 Linux文件实时同步–inotify + rsync + pyinotify 转载请注明:Artemis的博客–&gt; [点此看原文]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pyinotify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中命令和文件的查询]]></title>
    <url>%2F2017%2F05%2F23%2F2017-05-23-linuxfind%2F</url>
    <content type="text"><![CDATA[目录 whereis which find locate 正文&ensp;&ensp;&ensp;&ensp;Linux中有的时候需要检索文件和命令。这个时候Linux提供的强大查找命令就派上了用场。 whereis&ensp;&ensp;&ensp;搜索程序的位置。注意，它只能用于程序名 。 whereis -b (bin) 搜索二进制文件 whereis -m (man) 搜索man page说明文件 whereis -s (source) 搜索源文件 如果省略参数则，返回所有信心 which&ensp;&ensp;&ensp;作用于系统中的命令（包括用户安装的）。从PATH环境变量中指定的路径目录中查找目标命令，并返回第一个结果。所以它的执行结果就跟PATH中的路径顺序有关了，这样它就可以查看一个命令到底是执行的哪个位置的程序，比如用户装了两个 python程序，都是python2.7版本，但是安装的位置不一样（比如一个是系统自带，一个是 anaconda自带）那么怎么区分在终端敲python时执行的是哪一个呢？which就能完成区分的功能。 find&ensp;&ensp;&ensp;find是Linux提供的一个非常强大的一个文件查找命令。他能在给定的文件系统上查找任何你想要的文件。 命令格式： find [目录] [属性] [属性值] [指定动作] 具体含义： [目录] 要查找的目录，默认当前目录 [属性] 要查找的文件的条件属性，换句话说，就是要根据什么来查找。如：(name, size, user, uid, 权限等) [属性值] 如果前面指定了查找条件属性，那么就需要在这一给定属性值，告诉命令具体根据属性的什么属性值查找文件。 [指定动作] 在找到相应条件的文件后，对其指定的动作，如-ls表示找到文件后用ls 命令展示在终端 常用应用： find [dir] -name find [dir] -group find [dir] -uid find [dir] -type find [dir] -size {n(等于n) -n(小于n) +n(大于n)} find [dir] -size -4 -size +2 找大于2小于4的 find -ctime 1 找创建一天的 find -cmin 1 创建时间至少是一天的 find -cmin +1 创建时间至少大于一天的 find -cmin -10 find -newer file1 找比file新的文件 find /mnt -perm 222 找权限是222的文件 find -perm +222 只要有一位大于等于就匹配(或关系) find -perm -222 每一位都需要有w即可，(和关系) find -perm -2(o) find -perm -22(g-o) find -perm -222(u-g-o) locatelocate 从/var/lib/mlocate/mlocate.db中查找要查找的文件，这个数据库默认每天更新一次，可以手动用命令:upatedb命令更新数据库。它的功能和find -name类似，但是它的效率高，因为locate查找数据库，而find是检索磁盘文件系统。 locate -i(ignore case) 参考链接 Linux下的五个查找命令：grep、find、locate、whereis、 which]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image-Text matching papper]]></title>
    <url>%2F2017%2F05%2F22%2F2017-05-25-imagetextpapper%2F</url>
    <content type="text"><![CDATA[&ensp;&ensp;&ensp;Image-Text matching是与多个方向相关的一个任务，下面就展示一下我自己收集的一些论文。 正文2017 Image Retrieval Under Review CVPR&ensp;A类 2017 Asymmetrically Weighted CCA And Hierarchical Kernel Sentence Embedding For Image &amp; Text Retrieval引用量：1 in submission for TPAMI Learning Two-Branch Neural Networks for Image-Text Matching Tasks引用量：0 CVPR&ensp;A类 Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval引用量：0 VQA arXiv Hierarchical Recurrent Attention Network for Response Generation引用量：0 Image Caption &amp; Image Annonation arXiv Learning Semantics for Image Annotation引用量：0 RL Under Review CVPR&ensp;A类 Self-critical Sequence Training for Image Captioning引用量：3 Improved Image Captioning via Policy Gradient optimization of SPIDEr引用量：4 2016 基于检索 CVPR&ensp;A类 Learning Deep Structure-Preserving Image-Text Embeddings引用量：19 Aggregating Image and Text Quantized Correlated Components 引用量：1 DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations 引用量：10 ECCV&ensp;B类 CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples引用量：16 Kernel-Based Supervised Discrete Hashing for Image Retrieval引用量：1 Deep Image Retrieval: Learning global representations for image search引用量：12 arXiv Linking Image and Text with 2-Way Nets引用量：1 Multimodal CVPR&ensp;A类 MDL-CW: A Multimodal Deep Learning Framework with CrossWeights 引用量：0 图像生成 ICML&ensp;A类 Generative Adversarial Text to Image Synthesis引用量：46 Visual Grounding ECCV&ensp;B类 Modeling Context in Referring Expressions引用量：8 Modeling Context Between Objects for Referring Expression Understanding引用量：3 Structured Matching for Phrase Localization 引用量：2 CVPR&ensp;A类 Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes引用量：6 Image Caption &amp; Image Annonation ICLR Order-Embeddings of Images and Language引用量：31 在视觉-语义的层级上对其做偏序结构处理 Trainable performance upper bounds for image and video captioning引用量：2 Generating Images from Captions with Attention引用量：25 CVPR&ensp;A类 What value do explicit high level concepts have in vision to language problems?引用量：8 Image Captioning with Semantic Attention引用量：36 Learning Deep Representations of Fine-grained Visual Descriptions引用量：16 Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation引用量：4 ECCV&ensp;B类 SPICE: Semantic Propositional Image Caption Evaluation引用量：12 RNN Fisher Vectors for Action Recognition and Image Annotation引用量：5 Leveraging Visual Question Answering for Image-Caption Ranking引用量：4 BMVC&ensp;C类 Oracle performance for visual captioning引用量：2 Meeting of the Association for Computational Linguistics Generating Natural Questions About an Image引用量：1 arXiv Image Caption Generation with Text-Conditional Semantic Attention引用量：1 Boosting Image Captioning with Attributes引用量：4 Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning引用量：3 文本相似 HICSS Text-Based Document Similarity Matching Using Sdtext 引用量：0 Generate Model arXiv Learning to Generate with Memory引用量：5 Multi-task learning SigKdd&ensp;A类 Multi-Task Feature Interaction Learning引用量：0 VQA ECCV&ensp;B类 Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering引用量：39 Revisiting Visual Question Answering Baselines引用量：14 Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering Leveraging Visual Question Answering for Image-Caption Ranking引用量：4 CVPR&ensp;A类 Deep Supervised Hashing for Fast Image Retrieval 引用量：5 Visual7W: Grounded Question Answering in Images引用量：34 李飞飞老师的文章，这篇提出了一个新的数据集Visual7W Where To Look: Focus Regions for Visual Question Answering引用量：29 加入attention机制的一篇文章 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources引用量：12 沈春华老师的文章，这篇加入了外接知识库 Answer-Type Prediction for Visual Question Answering 引用量：7 Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction Stacked Attention Networks for Image QuestionAnswering 采用多次关注聚焦的方式来处理定位问题关注点 Neural Module Networks 根据问题不同动态组合网络 MovieQA: Understanding Stories in Movies through Question-Answering Yin and Yang: Balancing and Answering Binary Visual Questions arXiv Visual Question Answering: Datasets, Algorithms, and Future Challenges 综述性文章 Visual Question Answering: A Survey of Methods and Datasets 综述性文章 Image Captioning and Visual Question Answering Based on Attributes and External Knowledge 沈春华老师的文章，提取高层次语义概念的图像特征 AAAI&ensp;A类 Learning to Answer Questions From Image Using Convolutional Neural Network NIPS&ensp;A类 Hierarchical Question-Image Co-Attention for Visual Question Answering 采用图像attention问题，再用问题attention图像 PMLR Dynamic Memory Networks for Visual and Textual Question Answering 图像匹配 ECCV&ensp;B类 Matching Handwritten Document Images引用量：3 Learning Image Matching by Simply Watching Video引用量：3 Cross-Modal Retrieval SigKdd&ensp;A类 Deep Visual-Semantic Hashing for Cross-Modal Retrieval引用量：6 ICMR&ensp;B类 Correlation Autoencoder Hashing for Supervised Cross-Modal Search引用量：2 arXiv Correlation Hashing Network for Efficient Cross-Modal Retrieval引用量：3 RL ICLR Sequence Level Training with Recurrent Neural Networks引用量：56 2015 基于检索 ICCV&ensp;A类 Multimodal Convolutional Neural Networks for Matching Image and Sentence引用量：31 LEWIS: Latent Embeddings for Word Images and their Semantics引用量：1 CVPR&ensp;A类 Associating Neural Word Embeddings with Deep Image Representations using Fisher Vectors 引用量：21 Deep correlation for matching images and text 引用量：31 采用 canonical correlation analysis 作为目标函数，但这种目标函数不容易求导 arXiv Deep Learning Applied to Image and Text Matching引用量：0 类似综述类的一类文章，文章很长，但是在后半段作者阐述了自己的工作。总的来说，前面的综述部分还可以。 基于生成 ICLR Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)引用量：181 CVPR&ensp;A类 Show and Tell: A Neural Image Caption Generator作者：Bengio 引用量：583 Image Matching ICCV&ensp;A类 Multi-Image Matching via Fast Alternating Minimization引用量：7 Image Caption &amp; Image Annonation CVPR&ensp;A类 Deep Visual-Semantic Alignments for Generating Image Descriptions引用量：535 Learning a Recurrent Visual Representation for Image Caption Generation引用量：76 ICML&ensp;A类 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention引用量：537 Phrase-based Image Captioning引用量：26 ICCV&ensp;A类 Guiding Long-Short Term Memory for Image Caption Generation引用量：1 图像语义分割 CVPR&ensp;A类 Fully Convolutional Networks for Semantic Segmentation引用量：902 Multi-task learning CVPR&ensp;A类 Curriculum Learning of Multiple Tasks VQA ICCV&ensp;A类 Ask Your Neurons: A Neural-based Approach to Answering Questions about Images VQA: Visual Question Answering 提出了目前最大的数据集mscocoQA 网页：http://www.visualqa.org/ Visual Madlibs: Fill in the blank Image Generation and Question Answering NIPS&ensp;A类 Exploring Models and Data for Image Question Answering CVPR&ensp;A类 Don’t Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases 2014 基于检索 NIPS&ensp;A类 Deep Fragment Embeddings for Bidirectional Image Sentence Mapping引用量:161 Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models引用量：202 基于生成 CVPR&ensp;A类 From Captions to Visual Concepts and Back引用量:180 Visual Grounding Multimodal Representations TPAMI&ensp;A类 Spherical and Hyperbolic Embeddings of Data ICML Multimodal neural language models log-bilinear nerual language models arXiv Explain images with multi-modal recurrent neural networks RNN Image Caption &amp; Image Annonation Meeting of the Association for Computational Linguistics Comparing Automatic Evaluation Measures for Image Description引用量：42 论文比较了几种常见的图像描述问题的评价方法 Computer Science Explain Images with Multimodal Recurrent Neural Networks引用量：94 ICLR Deep Convolutional Ranking for Multilabel Image Annotation引用量：71 ECCV Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections normalized CCA TACL Grounded compositional semantics for findingand describing images with sentences dependency tree recursive networks ACL 2014 Workshop on Semantic Parsing A deeparchitecture for semantic parsing proposed a two-step embedding and generation procedure for semantic parsing. VQA NIPS&ensp;A类 A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input 图像语义分割 CVPR&ensp;A类 Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5)引用量：1969 2013 or before Image Retrieval TPAMI&ensp;A类 2007–Supervised Learning of Semantic Classes for Image Annotation and Retrieval 引用量：980 Multimodal Learning NIPS&ensp;A类 2012–Multimodal Learning with Deep Boltzmann Machines 引用量：362 ICML Mul-timodal deep learning autoencoders ICCV 2011–Learning cross-modality similarity formultinomial data topic models kernel CCA normalized CCA]]></content>
      <categories>
        <category>papper</category>
      </categories>
      <tags>
        <tag>Image-Text matching</tag>
        <tag>papper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 装饰器]]></title>
    <url>%2F2017%2F05%2F13%2F2017-5-14-pythondecorator%2F</url>
    <content type="text"><![CDATA[参考链接 Python Decorator Tutorial with Example GrahamDumpleton 如何理解Python装饰器？ Python装饰器学习（九步入门）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>decorator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建你的tensorflow模型(译)]]></title>
    <url>%2F2017%2F05%2F13%2F2017-05-13-tensorflowcodestructure%2F</url>
    <content type="text"><![CDATA[本文翻译自：Structuring Your TensorFlow Models 非本人原创。 目录 定义计算图 使用属性 延迟加载装饰器 使用域组织图 正文在用tensorflow构建神经网络程序时，随着网络结构的复杂，很容易会构建得到一份很庞大的代码。那么如何去管理这些代码，从而使代码有良好的可阅读性和可重用性？对于性子急的同学，可以戳下面的链接去直接查看代码：working example gist 定义计算图一般在构建tensorflow代码时，从定义一个类开始构建每个模型是一个明智的选择。那么，这个类的接口是什么？通常，你的模型是跟输入数据(input data)、目标占位符(target placeholders)和为训练、验证、预测模型提供的操作联系在一起的。 1234567891011121314151617181920212223242526class Model: def __init__(self, data, target): data_size = int(data.get_shape()[1]) target_size = int(target.get_shape()[1]) weight = tf.Variable(tf.truncated_normal([data_size, target_size])) bias = tf.Variable(tf.constant(0.1, shape=[target_size])) incoming = tf.matmul(data, weight) + bias self._prediction = tf.nn.softmax(incoming) cross_entropy = -tf.reduce_sum(target, tf.log(self._prediction)) self._optimize = tf.train.RMSPropOptimizer(0.03).minimize(cross_entropy) mistakes = tf.not_equal( tf.argmax(target, 1), tf.argmax(self._prediction, 1)) self._error = tf.reduce_mean(tf.cast(mistakes, tf.float32)) @property def prediction(self): return self._prediction @property def optimize(self): return self._optimize @property def error(self): return self._error 这是一段tensorflow的基本代码，但是这段代码存在很多问题，最显著问题是，该段代码将整个图定义在一个函数内（即初始化函数）。这样做的可阅读性和可重用性都不好。 使用属性仅仅将代码划分为每个函数并不能起作用。因为每次函数被调用，计算图就将会使用函数中的代码进行扩展。因此我们必须保证，只有在第一次调用函数时，函数中的操作才会被添加到计算图中。这就是延迟加载。 1234567891011121314151617181920212223242526272829303132333435class Model: def __init__(self, data, target): self.data = data self.target = target self._prediction = None self._optimize = None self._error = None @property def prediction(self): if not self._prediction: data_size = int(self.data.get_shape()[1]) target_size = int(self.target.get_shape()[1]) weight = tf.Variable(tf.truncated_normal([data_size, target_size])) bias = tf.Variable(tf.constant(0.1, shape=[target_size])) incoming = tf.matmul(self.data, weight) + bias self._prediction = tf.nn.softmax(incoming) return self._prediction @property def optimize(self): if not self._optimize: cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction)) optimizer = tf.train.RMSPropOptimizer(0.03) self._optimize = optimizer.minimize(cross_entropy) return self._optimize @property def error(self): if not self._error: mistakes = tf.not_equal( tf.argmax(self.target, 1), tf.argmax(self.prediction, 1)) self._error = tf.reduce_mean(tf.cast(mistakes, tf.float32)) return self._error 这段代码就比第一段代码好多了，各个功能被分配到了各个函数，这样方便你单独的考虑某个功能。然而，这段代码为了实现延迟加载的特性，有一些臃肿。下面让我们再优化一下它。 延迟加载装饰器Python是一门非常灵活的语言。所以让我展示一下如何从上面最后一段代码段中去除多余的代码。我们将使用装饰器，装饰器的行为就像@property一样，但是装饰器起作用使其函数只计算一次。它将计算结果存储于一个命名（预先准备一个前缀）在被装饰的函数后的成员变量中，然后在每次随后的调用中返回这个值。如果你不熟悉装饰器，你可能希望这个链接[] 12345678910111213import functoolsdef lazy_property(function): attribute = '_cache_' + function.__name__ @property @functools.wraps(function) def decorator(self): if not hasattr(self, attribute): setattr(self, attribute, function(self)) return getattr(self, attribute) return decorator 使用这个装饰器，我们的样例代码就像下面一样简单了： 1234567891011121314151617181920212223242526272829class Model: def __init__(self, data, target): self.data = data self.target = target self.prediction self.optimize self.error @lazy_property def prediction(self): data_size = int(self.data.get_shape()[1]) target_size = int(self.target.get_shape()[1]) weight = tf.Variable(tf.truncated_normal([data_size, target_size])) bias = tf.Variable(tf.constant(0.1, shape=[target_size])) incoming = tf.matmul(self.data, weight) + bias return tf.nn.softmax(incoming) @lazy_property def optimize(self): cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction)) optimizer = tf.train.RMSPropOptimizer(0.03) return optimizer.minimize(cross_entropy) @lazy_property def error(self): mistakes = tf.not_equal( tf.argmax(self.target, 1), tf.argmax(self.prediction, 1)) return tf.reduce_mean(tf.cast(mistakes, tf.float32)) 注意我们在初始化函数中提到的属性。在这种方式下，整张图能够确保在运行 tf.initialize_variables()函数时被定义。 使用域组织图我们现在有一种清楚的方式在代码中定义模型，但是产生的计算图仍然很拥挤。如果你可视化计算图，它将包含很多的内连小节点。解决方式是用tf.name_scope(‘name’)或者 tf.variable_scope(‘name’)将每个函数的内容修饰。图上的节点将被分组。我们只需将上面的装饰器稍加修改即可。 1234567891011121314import functoolsdef define_scope(function): attribute = '_cache_' + function.__name__ @property @functools.wraps(function) def decorator(self): if not hasattr(self, attribute): with tf.variable_scope(function.__name): setattr(self, attribute, function(self)) return getattr(self, attribute) return decorator 因为这个装饰器所拥有的对tensorflow的特殊功能和它的延迟缓存，我给它定义了一个新的名字(define_scope)。除此之外，这个装饰器看起来和上一个是一样的。 我们甚至能够使@define_scope有在tf.variable_scope之前运行的参数传递，例如定义一个属于这个scope默认的initializer。详情可以看最后的代码。 我们现在可以以结构化的、简洁的方式定义有组织的计算图了。这让我很受用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import functoolsimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef doublewrap(function): """ A decorator decorator, allowing to use the decorator to be used without parentheses if not arguments are provided. All arguments must be optional. """ @functools.wraps(function) def decorator(*args, **kwargs): if len(args) == 1 and len(kwargs) == 0 and callable(args[0]): return function(args[0]) else: return lambda wrapee: function(wrapee, *args, **kwargs) return decorator@doublewrapdef define_scope(function, scope=None, *args, **kwargs): """ A decorator for functions that define TensorFlow operations. The wrapped function will only be executed once. Subsequent calls to it will directly return the result so that operations are added to the graph only once. The operations added by the function live within a tf.variable_scope(). If this decorator is used with arguments, they will be forwarded to the variable scope. The scope name defaults to the name of the wrapped function. """ attribute = '_cache_' + function.__name__ name = scope or function.__name__ @property @functools.wraps(function) def decorator(self): if not hasattr(self, attribute): with tf.variable_scope(name, *args, **kwargs): setattr(self, attribute, function(self)) return getattr(self, attribute) return decoratorclass Model: def __init__(self, image, label): self.image = image self.label = label self.prediction self.optimize self.error @define_scope(initializer=tf.contrib.slim.xavier_initializer()) def prediction(self): x = self.image x = tf.contrib.slim.fully_connected(x, 200) x = tf.contrib.slim.fully_connected(x, 200) x = tf.contrib.slim.fully_connected(x, 10, tf.nn.softmax) return x @define_scope def optimize(self): logprob = tf.log(self.prediction + 1e-12) cross_entropy = -tf.reduce_sum(self.label * logprob) optimizer = tf.train.RMSPropOptimizer(0.03) return optimizer.minimize(cross_entropy) @define_scope def error(self): mistakes = tf.not_equal( tf.argmax(self.label, 1), tf.argmax(self.prediction, 1)) return tf.reduce_mean(tf.cast(mistakes, tf.float32))def main(): mnist = input_data.read_data_sets('./mnist/', one_hot=True) image = tf.placeholder(tf.float32, [None, 784]) label = tf.placeholder(tf.float32, [None, 10]) model = Model(image, label) sess = tf.Session() sess.run(tf.initialize_all_variables()) for _ in range(10): images, labels = mnist.test.images, mnist.test.labels error = sess.run(model.error, &#123;image: images, label: labels&#125;) print('Test error &#123;:6.2f&#125;%'.format(100 * error)) for _ in range(60): images, labels = mnist.train.next_batch(100) sess.run(model.optimize, &#123;image: images, label: labels&#125;)if __name__ == '__main__': main() 参考链接 Structuring Your TensorFlow Models 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>code structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-03-29-install]]></title>
    <url>%2F2017%2F03%2F29%2F2017-03-29-install%2F</url>
    <content type="text"><![CDATA[源码安转config –help makefile make make install 默认/usr/local/ –prefix 修改 wine tar.gz 一个在linux下使用windows软件的软件 rpm 包只保留一些最常用功能。 实现软件的安转、查找、更新、和卸载 rpm -ivh +软件包rpm -qa 查询显示所有包]]></content>
  </entry>
  <entry>
    <title><![CDATA[python数据分析pandas高级用法]]></title>
    <url>%2F2017%2F03%2F25%2F2017-03-25-pandas%2F</url>
    <content type="text"><![CDATA[目录 层级索引 数据的分组和聚合 正文层级索引MultiIndex对象pandas中用来管理层级索引的对象。 例子: 1234import pandas as pdimport numpy as npser_obj = pd.Series(np.random.randn(12),index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd'],[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]])print ser_obj 12345678910111213a 0 -0.674611 1 -0.942212 2 0.851033b 0 0.935383 1 0.576104 2 0.434570c 0 1.956513 1 -0.318670 2 0.792113d 0 -1.353721 1 -0.195022 2 1.085860dtype: float64 12print type(ser_obj.index)print ser_obj.index 123&lt;class 'pandas.indexes.multi.MultiIndex'&gt;MultiIndex(levels=[[u'a', u'b', u'c', u'd'], [0, 1, 2]], labels=[[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]]) 选取子集12# 外层选取print ser_obj['c'] 12340 1.9565131 -0.3186702 0.792113dtype: float64 12# 内层选取print ser_obj[:, 2] 12345a 0.851033b 0.434570c 0.792113d 1.085860dtype: float64 交换分层顺序1print ser_obj.swaplevel() 123456789101112130 a -0.6746111 a -0.9422122 a 0.8510330 b 0.9353831 b 0.5761042 b 0.4345700 c 1.9565131 c -0.3186702 c 0.7921130 d -1.3537211 d -0.1950222 d 1.085860dtype: float64 排序分层1print ser_obj.swaplevel().sortlevel() 123456789101112130 a -0.674611 b 0.935383 c 1.956513 d -1.3537211 a -0.942212 b 0.576104 c -0.318670 d -0.1950222 a 0.851033 b 0.434570 c 0.792113 d 1.085860dtype: float64 数据的分组和聚合分组 (groupby)• 对数据集进行分组,然后对每组进行统计分析• SQL能够对数据进行过滤,分组聚合• pandas能利用groupby进行更加复杂的分组运算 分组运算过程split-&gt;apply-&gt;combine图示: • GroupBy对象:DataFrameGroupBy,SeriesGroupBy• GroupBy对象没有进行实际运算,只是包含分组的中间数据• 对GroupBy对象进行分组运算/多重分组运算,如mean()• 非数值数据不进行分组运算• size() 返回每个分组的元素个数 12345678dict_obj = &#123;'key1' : ['a', 'b', 'a', 'b', 'a', 'b', 'a', 'a'], 'key2' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'data1': np.random.randn(8), 'data2': np.random.randn(8)&#125;df_obj = pd.DataFrame(dict_obj)print df_obj 123456789 data1 data2 key1 key20 -0.079203 -0.844599 a one1 0.379854 0.849821 b one2 -0.961888 -1.502866 a two3 -0.063710 -1.136585 b three4 0.537074 0.326732 a two5 0.854460 -0.474144 b two6 1.098841 -0.521574 a one7 0.364664 0.014630 a three 12# dataframe根据key1进行分组print type(df_obj.groupby('key1')) 1&lt;class 'pandas.core.groupby.DataFrameGroupBy'&gt; 12# data1列根据key1进行分组print type(df_obj['data1'].groupby(df_obj['key1'])) 1&lt;class 'pandas.core.groupby.SeriesGroupBy'&gt; 123456# 分组运算grouped1 = df_obj.groupby('key1')print grouped1.mean()grouped2 = df_obj['data1'].groupby(df_obj['key1'])print grouped2.mean() 12345678 data1 data2key1a 0.191898 -0.505536b 0.390201 -0.253636key1a 0.191898b 0.390201Name: data1, dtype: float64 123# sizeprint grouped1.size()print grouped2.size() 12345678key1a 5b 3dtype: int64key1a 5b 3dtype: int64 按列名分组• obj.groupby(‘label’)• 按列名多层分组• obj.groupby([‘label1’, ‘label2’])-&gt;多层dataframe• 按自定义的key分组• obj.groupby(self_def_key)• 自定义的key可为列表或多层列表• unstack可以将多层索引的结果转换成单层的dataframe 例子: 12# 按列名分组#df_obj.groupby('key1') 1231 52 3dtype: int64 123# 按自定义key分组，列表self_def_key = [1, 1, 2, 2, 2, 1, 1, 1]df_obj.groupby(self_def_key).size() 1231 52 3dtype: int64 12# 按自定义key分组，多层列表df_obj.groupby([df_obj['key1'], df_obj['key2']]).size() 12345678key1 key2a one 2 three 1 two 2b one 1 three 1 two 1dtype: int64 123# 按多个列多层分组grouped2 = df_obj.groupby(['key1', 'key2'])print grouped2.size() 12345678key1 key2a one 2 three 1 two 2b one 1 three 1 two 1dtype: int64 12345# 多层分组按key的顺序进行grouped3 = df_obj.groupby(['key2', 'key1'])print grouped3.mean()printprint grouped3.mean().unstack() ```python data1 data2key2 key1one a 0.509819 -0.683087 b 0.379854 0.849821three a 0.364664 0.014630 b -0.063710 -1.136585two a -0.212407 -0.588067 b 0.854460 -0.474144 data1 data2 key1 a b a bkey2one 0.509819 0.379854 -0.683087 0.849821three 0.364664 -0.063710 0.014630 -1.136585two -0.212407 0.854460 -0.588067 -0.474144 • 拆分:进行分组的根据• 应用:每个分组运行的计算规则• 合并:把每个分组的计算结果合并起来 参考链接]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-03-22-topicmodel]]></title>
    <url>%2F2017%2F03%2F22%2F2017-03-22-topicmodel%2F</url>
    <content type="text"><![CDATA[正文主题模型，作用就是从给定的一篇文章中，抽取总结出它的主题。主题模型有广泛的应用，在当遇到抽取主题类的问题时，可以尝试主题模型。 直观版首先思考这么一个问题，某牛叉企业要招聘一个工程师，HR收到了很多很多的简历，HR想直接通过简历来看出谁是大牛，谁是菜逼。 可以想到简历中包含了简历作者的个人特征，如热爱编程、拿过ACM冠军、百度实习经历等，由这些构成了每个人的简历，然后每个人的简历可以在HR眼中分成好的和坏的。 这位人力总监要做的事是: 拿出一份简历，记录每份简历包含的特征，然而他并不知道，这些特征代表什么。 于是他开始猜，看到简历A中说A是ACM冠军，那应该是个好程序员。但是他又看到A的学历只是小学毕业，心里多了几分质疑。然后他继续看B、C、… 当然，HR也只是在猜，没有任何证据可以证实。但是这位HR是久经职场的老司机，他通过经验统计来调整自己的猜想: 选一份张三的简历，选一个特征条形衬衫，为什么张三喜欢穿条纹衬衫？也许是因为穿条纹衬衫是优秀程序员的信仰。 也就是说，越多的优秀程序员穿条纹衬衫，越让HR猜想张三的其他个人人特征也符合优秀程序员的喜好,并且张三本人穿条纹衬衫 是一个优秀的程序员自自我修养的体现。 继续猜,继续拿张三和条纹衬衫两个元素。HR转念一想,也有可能爱穿条纹的都是彩笔。 于是他按照上面面的逻辑,再看看张三穿条纹衬衫是彩笔的可能性有多少，把所有的简历和所有的特征都做个组合,都来猜一下是彩笔还是大牛。 久经沙场之后,老司机HR掌握了如下信息: 对于是不是优秀程序员的分类,它通过人头统计大概有了数，这让他在以后看到新简历的时候,一眼就知道他是不是个优秀程序员，对于每个特征C，他也能说出大概百分之多少的人拥有特征C可以说明他们是优秀的程序员。 总结成公式为: 以上,就是我们用现实的例子模拟的LDA模型来区分简历好坏，在文本的主题分类中,我们的例子和实际之间的联系如下: 为了后续的讲解，这里先回顾一下贝叶斯模型: 贝叶斯模型条件概率条件概率就是一个事件在另一个事件发生条件下的概率。 全概率和贝叶斯有人的地方就有江湖，数学界也一样，在数学界一直有一场争论就是概率中的频率学派和贝叶斯学派的争论。 参考链接]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>主题模型</tag>
        <tag>LDA</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之权限管理]]></title>
    <url>%2F2017%2F03%2F16%2F2017-03-16-authority%2F</url>
    <content type="text"><![CDATA[正文ll命令ll命令是linux中列出文件（夹）详细信息的命令，相当于ls -l 如图所示，观察文件a前面的详细信息，可以分为几段： 第一段一共十位，第一位表示文件的类型，-代表普通文件，d代表目录文件，b代表块设备文件，c代表字符设备文件，s代表socket文件，p代表管道文件，l代表软连接文件。 其余的九位每三位分成一组，共分成三组，然后每组分别代表用户(user)，用户所属组 (group)，不属于用户组的其他用户(others)。然后每组中三位分别表示读权限，写权限和执行权限。 权限和命令间的关系： r 对于文件代表是否能用cat等命令进行查看。对于目录，代表能否用ls等命令查看目录内的内容。 w 对于文件代表能否改变文件内的内容。对于目录代表能否改变目录内的内容（添加、删除文件等）。 x 对于文件代表文件(是否能运行)，对于文件夹代表是否能cd登录进去。 第二段是指文件的个数，如果是文件那么此段值就是１，如果是文件夹则显示其中含有文件的个数。 第三段是指文件所属的所有者 第四段是指文件所有者所属的组 第五段文件的大小，默认以byte为单位。空目录一般显示1024byte。 第六段表示文件最后被修改的时间 第七段文件名 权限Linux的权限主要就是上面ll中列出的第一段的后九位，即用户，用户组，其他用户对文件的读，写和执行权限。下面就介绍Linux中对权限的操作命令。 umask命令接着前面说，上面说的９位表示权限的位，每一位的取值就两种可能，一种是具备这种权限，一种是不具备。这样的话如果用１表示具备，０表示不具备，那么这九位表示权限的位就能转化位九位数字来表示。如rwxr-x—就能用111101000来表示。再将这九位数字每三位分一组，每组数就可以看做一个二进制数。111101000-&gt;(111)(101)(000)。然后再将每组数转换位十进制(111)(101)(000)-&gt;750。这样权限就能用三位十进制数表示了，但是每一位的值不大于７。 接着说umask权限过滤符。在Linux中每次创建新文件时，都会用umask的值对文件的权限进行过滤，从而得到文件初始权限。比如umask的值是022，按照上面的讨论可以反向的将022 转换为９位的权限值022-&gt;000010010-&gt;—-w–w-，然后新文件时用—-w–w-对 rwxrwxrwx进行过滤(相同的过滤掉)后得到的权限就是文件的初始权限。这里得到的是 rwxr-xr-x，这就是umask的作用。在命令行之间敲umask可以查看umask的值，umask value命令可以设置umask值。 umask -S命令会显示带符号的权限信息，而不是数字。比如umask的值为022，那么 umask -S会显示u=rwx,g=rx,o=rx。 chmod命令chmod命令用于给文件赋权限。用法很简单，如果想给文件增加所有者的写权限，则用 chmod u+w filename,同样的chmod后面的权限操作符由三部分构成，第一部分是指出要修改权限的对象，如u、g、o，第二部分是指出要做的操作，增加权限用+，减少权限-，第三部分指出要增加或减少的权限rw或x。如要为文件a增加其他用户的写权限，命令为 chmod o+w a。chmod也可以直接向文件赋予上述的三位数字的权限，如要向文件赋予 rwxr-xr-x权限，则命令为chmod 755 filename。 如： chmod 4644 = chmod u+s chmod 2644 = chmod g+s chmod 1644 = chmod o+t chattr命令改变文件的属性。可以用lsattr命令查看文件的属性。与chmod这个命令相比，chmod只是改变文件的读写、执行权限，更底层的属性控制是由chattr来改变的。 + ：在原有参数设定基础上，追加参数。 - ：在原有参数设定基础上，移除参数。 = ：更新为指定参数设定。 A：文件或目录的 atime (access time)不可被修改(modified), 可以有效预防例如手提电脑磁盘I/O错误的发生。 S：硬盘I/O同步选项，功能类似sync。 a：即append，设定该参数后，只能向文件中添加数据，而不能删除，多用于服务器日志文件安全，只有root才能设定这个属性。 c：即compresse，设定文件是否经压缩后再存储。读取时需要经过自动解压操作。 d：即no dump，设定文件不能成为dump程序的备份目标。 i：设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容。i参数对于文件 系统的安全设置有很大帮助。 j：即journal，设定此参数使得当通过mount参数：data=ordered 或者 data=writeback 挂 载的文件系统，文件在写入时会先被记录(在journal中)。如果filesystem被设定参数为 data=journal，则该参数自动失效。 s：保密性地删除文件或目录，即硬盘空间被全部收回。 u：与s相反，当设定为u时，数据内容其实还存在磁盘中，可以用于undeletion。 各参数选项中常用到的是a和i。a选项强制只可添加不可删除，多用于日志系统的安全设定。而i是更为严格的安全设定，只有superuser (root) 或具有CAP_LINUX_IMMUTABLE处理能力（标识）的进程能够施加该选项。 SUIDset user ID chmod u+s 如果权限中带着S位那么所有用户都能像当前这个用户一样使用这个被赋予权限的命令或文件。 使用SUID时要注意几点： SUID只对二进制文件有效 调用者对该文件有执行权 在执行过程中，调用者会暂时获得该文件的所有者权限 该权限只在程序执行的过程中有效 SUID的执行过程(图片转自下面参考链接的博客): SGIDSet GID 当SGID修饰文件时，它的作用和SUID相似，用户将得到该文件所属组的权限。当SGID修饰目录时，而且此时用户对目录有写权限，那么用户在目录中创建的任何东西都继承到目录所属的组。 chmod g+s [name] SBITSticky Bit 只能用于文件夹，在others权限的执行位上加上一个t位。当某一个目录拥有SBIT权限时，则任何一个能够在这个目录下建立文件的用户，该用户在这个目录下所建立的文件，只有该用户自己和root可以删除，其他用户均不可以。 chmod o+t [name] 也可以使用数字为文件系统增加SUID、SGIT、SBIT权限。 4表示SUID 2表示SGID 1表示SBIT 比如chmod 4777 [name]就给文件增加了SUID，如果想同时为文件增加多种权限则将相应值相加即可，比如chmod 6777 [name]就为文件同时增加了SUID和SGID。 参考链接:linux下ll命令列出的文件类型 （总结）Linux的chattr与lsattr命令详解linux中SUID，SGID和SBIT的奇妙用途 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间序列分析之隐马尔科夫模型]]></title>
    <url>%2F2017%2F03%2F14%2F2017-03-14-hmm%2F</url>
    <content type="text"><![CDATA[正文HMM曾经统治过语音识别领域，虽然它现在已经在一定程度上被RNN取代，但是其思想还是值得理解的。而且其在很多领域还有广泛的应用。 预习阅读图模型 条件概率的贝叶斯模型 引子首先从一个例子开始。俗话说，人心不可测，而妹子的心更不可测。现在想这样一个模型。如下图，有两个状态，一个是妹子的心情X，和妹子的心情的外在表现(如摔东西，哭等)Y，那么如何通过Y推测X呢？ 由贝叶斯公司可以得到： 其中P(Y|X)，P(X)，P(Y)可以通过统计得到，那么这就可以得到一个能够通过妹子的外在表现推测妹子心情的模型。 HMM将上面的模型，扩展一下，想一想，妹子的心情是多种多样的，而且可以相互变换，于是得到下面的模型。 首先，Y1，Y2，Y3，这些外在特征是可以通过观测数据得到的。 首先想一想，怎么得到 现在再来考虑 由图模型假设没有边相连的变量是相互独立的。 再将带入可得到一个递归方程，根据这个方程进行动态规划就能求得 Time Machine看下面一个模型： 模型中事件序列x1,x2,…,xt存在相关性，如图中x3是x1和x2的线性组合，其实就是加权和。这种模型称为auto regressive model。模型中的参数是可学的，也可以称可fit的。这是最简单的时间序列模型。 RNN初涉RNN是一种神经网络结构，它在一些方面能够替代HMM，且效果更好。LSTM是一种特殊的 RNN,把sigmod函数变成了另一系列的东西。RNN能并行运算，所以它能处理大的数据，RNN存在梯度消失问题，LSTM就是为了解决这个问题，这里只是粗略提及一点，后面会详细介绍。 总结HMM是一个思想，没有统一的形式，在不同的问题中不一样，要看实际问题仔细分析，然后借鉴其思想解决问题。 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>时间序列</tag>
        <tag>hmm</tag>
        <tag>概率图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从贝叶斯到图模型]]></title>
    <url>%2F2017%2F03%2F12%2F2017-03-13-bayes%2F</url>
    <content type="text"><![CDATA[正文贝叶斯公式我们将基本事件空间B分割成n个互斥的的事件空间Bi，那么事件A的概率可以表示为: 考虑乘法公式: 可得: 这就是全概率公式。 再由条件概率定义: 这就是著名的贝叶斯定理。 经典问题生病问题加入一种病的患病率是万分之一，医院对这种病的检测准确率是99%，那么你去检测，医生告诉你检测呈阳性，那么你由多大几率真正患病？ 设D=1代表患病，D=0代表不患病，C=0代表检测呈隐性，C=1代表检测呈阳性。 那么由题意可知: P(D=1)=0.0001 P(C=1|D=1)=0.99 P(C=0|D=0)=0.99 则: P(C=0|D=1)=1-P(C=1|D=1)=0.01 P(C=1|D=0)=1-P(C=0|D=0)=0.01 所以由贝叶斯公式: 奖品问题有三扇门，只有一扇门后面有大奖，参与者可以选择一扇门，然后主持人从余下的两扇门中选择一扇没有奖品的打开，然后再问参与者是否换一扇门。请从数学的角度说明换好还是不换好。 设H=i(i=1,2,3)代表奖品在第i门后，D=i(i=1,2,3)代表主持人打开第i扇门。 不失一般性，不妨设用户打开第一扇门，则： P(D=2|H=1)=P(D=3|H=1)=0.5 P(D=2|H=2)=0 P(D=3|H=2)=1 P(D=2|H=3)=1 P(D=3|H=3)=0 由全概率公式： 不妨设主持人打开第三扇门，则由贝叶斯公式： 所以换得大奖的几率大。 贝叶斯决策经过贝叶斯计算出的概率称为后验概率，怎样根据后验概率来决策呢？ 风险事件在状态wi可能有很多种动作，那么每一种动作所造成的代价称为风险。用风险函数代表状态wi做出动作aj所造成的代价，即风险。 动作有很多种，比如在分类问题中，动作可能代表分到某一类去，而代价就是损失函数。 经验损失最小化前面计算得到的后验概率P(wi|A)代表在一些条件下处于状态wi的概率，那么用它乘以损失函数: 这就是平均风险，也称为经验风险，找到使它最小化的动作的过程称为经验风险最小化。然后就可以进行决策。即使经验风险达到最小化的动作就是决策结果。 实际中，贝叶斯较难进行经验损失最小化，不过贝叶斯决策理论倒是为我们提供了一个与其他分类器做对比的评价依据，也就是说贝叶斯决策很多情况下是作为对比对象而存在的。 图模型之有向图背景一般来说在用概率模型时，如果拥有了联合概率密度就相当于拥有了上帝视角，可以得到任何想要的东西，但是需要的耗费是指数级的，比如一共有d个变量，每个变量有0和1两个取值，那么就得维护一个2^d的概率表，以备查阅。这样一来耗费太大。基于这个问题的解决，人们提出忽略变量之间的依赖关系，只关心变量之间的核心依赖关系，即定义一个有向无环图，只需记录这些核心关系即可，这样就得到了图模型的有向图方法。 有向无环图有向无环图，Directed probabilistic graphical models(DAG)，是指图的结点间是有向边，而且整个图没有环，如下图： 图中没有线相连的结点是被假设没有关系的。结点跟它的祖先也是相互独立的，换句话说，结点只跟与它相连的直系父节点有关系。这个假设是根据实际场景的，每个人对它的定义和理解也是不同的。 那么: 这就告诉了我们，如果我们有n个变量xi那么这些变量的联合分布律是: 怎么计算？假设A有两个取值0和1，那么先求A的分布律： 如何计算上面的式子？最简单的方法是四重for循环，但是这样的话时间复杂度太大。那么怎么办呢？想想算法中的动态规划，这里叫做消元。变换求和次序得: 其中最后两步的变换是由于概率的归一性： 这里动态规划的思想是通过移动求和次序，消去变量，减少了重复计算。由四重循环变为了两重循环，大大减少了计算量。 Naïve Bayes最后再讲一下有向概率图的一个特殊情况Naïve Bayes Naïve Bayes是指下面这种有向概率图: 上图显示一个分类过程，其实naivebayes第二层结点就是一些分类，从第一层进行的动作就是将数据分到某一个类中，根据上面的理论可以很容易写出上图的联合概率密度，然后再计算第二层given第一层结点的条件概率密度，选择最大的即可进行分类。 参考链接:贝叶斯决策论 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>bayes</tag>
        <tag>概率图模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中的文件和文件系统的压缩与打包]]></title>
    <url>%2F2017%2F03%2F11%2F2017-03-11-Linuxcompress%2F</url>
    <content type="text"><![CDATA[正文&nbsp;&nbsp; &nbsp;Linux提供了一些文件的压缩和打包命令，现在就让我们一起来学习他们。其中每个命令有很多参数，这里只是说明一些常用的，其它一些可以通过man page来查看。 Linux常用的压缩命令compress 命令&nbsp;&nbsp; &nbsp;compress命令使用“Lempress-Ziv”编码压缩数据文件。compress是个历史悠久的压缩程序，文件经它压缩后，其名称后面会多出”.Z”的扩展名。当要解压缩时，可执行uncompress指令。事实上uncompress是指向compress的符号连接，因此不论是压缩或解压缩，都可通过compress指令单独完成。这个命令已经过时了，已经不常用，这里为了完整性，也解释一下。 -f：不提示用户，强制覆盖掉目标文件； -c：将结果送到标准输出，无文件被改变； -r：递归的操作方式； -b\&lt;压缩效率>：压缩效率是一个介于9~16的数值，预设值为”16”，指定愈大的数值，压缩效率就愈高； -d：对文件进行解压缩而非压缩； -v：显示指令执行过程； -V：显示指令版本及程序预设值。 uncompress命令&nbsp;&nbsp; &nbsp;uncompress命令用来解压缩由compress命令压缩后产生的“.Z”压缩包。 -f：不提示用户，强制覆盖掉目标文件； -c：将结果送到标准输出，无文件被改变； -r：递归的操作方式。 一般用法:compress -v filenamecompress -c -v filename &gt; targetfilename gzip命令&nbsp;&nbsp; &nbsp;gzip, zcat是Linux提供的一对操作文件压缩的命令。gzip对文件进行压缩解压，zcat查看gzip压缩的文件。 gzip filename 压缩 zcat 查看gzip压缩完的内容 gzip -d filename 解压 gzip -c filename &gt; filename 保留源文件 bzip2, bzcat命令&nbsp;&nbsp; &nbsp;bzip2和gzip一样的用法。 zip用法 zip aaa.zip aaa 压缩aaa文件 unzip aaa.zip 解压 rar命令&nbsp;&nbsp; &nbsp;另外有的时候需要rar，unrar命令，如果系统不提供，可以自行下载。 unrar命令 unrar e Extract files without archived paths unrar x Extract files with full path Linux的归档命令 tar&nbsp;&nbsp; &nbsp;很多时候，比如你要传输文件时，有许多文件需要传输，由于文件数很多，所以依次传输的话太繁琐，这个时候如果能够把这些文件组合成一个整体一次传输，传输完成后再分解成多个文件就方便了。幸运的是，Linux提供了这个命令，即tar命令。 选项 -f 指定要归档文件的名字 -c 对文件归档 -v 显示归档过程 -x 对文件解档 -t 查看归档文件内容 常用命令 tar -cvf aaa.tar aaa 将文件夹aaa下的文件归档，归档后名字为aaa.tar tar -cvf aaa.tar aaa –remove-files 归档文件并删除源文件 tar -tvf hosts.tar 在不解档的情况下查看文件内容 tar -xvf aaa.tar 解档 tar -xvf aaa.tar -C aa/ 解档到aa目录下 tar -xvf aaa.tar a1 只解档aaa.tar中的a1文件 tar可以跟gzip或bzip2组合使用 tar -zcvf xxx.tar.gz xxx 归档并压缩 tar -zxvf xxx.tar.gz 解压并解档 tar -jcvf xx.tar.bz2 xx 归档并压缩 tar -jxvf xx.tar.bz2b 解压并解当 Linux的备份命令备份的种类 完全备份&nbsp;&nbsp; &nbsp;每次都将原文件系统备份，耗费空间。 差异备份&nbsp;&nbsp; &nbsp;每次在原文件系统的基础上做新数据的备份，容易还原。 增量备份&nbsp;&nbsp; &nbsp;每次在前一次备份的基础上做新数据的备份，还原麻烦，但是节省空间。 dump 命令&nbsp;&nbsp; &nbsp;dump是linux提供的备份工具程序，可将目录或整个文件系统备份至指定的设备，或备份成一个大文件。这里重点注意，dump针对的文件系统而不是单个文件。 语法 dump [-cnu][-0123456789][-b &lt;区块大小&gt;][-B &lt;区块数目&gt;][-d &lt;密度&gt;][-f &lt;设备名称&gt;][-h &lt;层级&gt;][-s &lt;磁带长度&gt;][-T &lt;日期&gt;][目录或文件系统] dump [-wW] 选项 -0123456789 备份的层级。 -b&lt;区块大小&gt; 指定区块的大小，单位为KB。 -B&lt;区块数目&gt; 指定备份卷册的区块数目。 -c 修改备份磁带预设的密度与容量。 -d&lt;密度&gt; 设置磁带的密度。单位为BPI。 -f&lt;设备名称&gt; 指定备份设备。 -h&lt;层级&gt; 当备份层级等于或大雨指定的层级时，将不备份用户标示为”nodump”的文件。 -n 当备份工作需要管理员介入时，向所有”operator”群组中的使用者发出通知。 -s&lt;磁带长度&gt; 备份磁带的长度，单位为英尺。 -T&lt;日期&gt; 指定开始备份的时间与日期。 -u 备份完毕后，在/etc/dumpdates中记录备份的文件系统，层级，日期与时间等。 -w 与-W类似，但仅显示需要备份的文件。 -W 显示需要备份的文件及其最后一次备份的层级，时间与日期。 常用命令 dump -0u -f filesystem dumpname restore -t xxx.dump #cat the content of the dump file restore -r(restore) -f 恢复文件 restore -i -f aaa.dump 进入交互模式 dump的重要特性&nbsp;&nbsp; &nbsp;dump -0 会完成完整的系统备份，以后的备份，dump会做对前面的备份等级中小于当前等级的最大的备份的增量备份。也就是说，比如，前面已经做了0,2,3,4,6级备份，当前要做5级备份，由于0,2,3,4,6中小于5级的最大等级是4级，所以当前五级备份做的是对前面四级备份的增量备份。 &nbsp;&nbsp; &nbsp;通过dump这样的特性，可以完成对文件系统的增量备份（按0,1,2…按递增等级顺序备份),也可以完成差异备份(按n,n-1,n-2,…的递减等级顺序备份)。 跳过文件&nbsp;&nbsp; &nbsp;如果想要让文件系统中的某个文件不备份，可以使用chattr命令。 chattr +d &lt;filename&gt;向文件添加一个属性，使其在dump备份时跳过此文件。 光盘写入工具 mkisofsmkisofs -o xx.iso file1 file2 ...cp /mnt/cdrom xx.iso # copy the CDrom to isomount -o loop dd命令&nbsp;&nbsp; &nbsp;dd 是 Linux/UNIX 下的一个非常有用的命令，作用是用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换。 选项 if =输入文件（或设备名称）。 of =输出文件（或设备名称）。 ibs = bytes 一次读取bytes字节，即读入缓冲区的字节数。 skip = blocks 跳过读入缓冲区开头的ibs*blocks块。 obs = bytes 一次写入bytes字节，即写入缓冲区的字节数。 bs = bytes 同时设置读/写缓冲区的字节数（等于设置ibs和obs）。 cbs = byte 一次转换bytes字节。 count=blocks 只拷贝输入的blocks块。 conv = ASCII 把EBCDIC码转换为ASCIl码。 conv = ebcdic 把ASCIl码转换为EBCDIC码。 conv = ibm 把ASCIl码转换为alternate EBCDIC码。 conv = block 把变动位转换成固定字符。 conv = ublock 把固定位转换成变动位。 conv = ucase 把字母由小写转换为大写。 conv = lcase 把字母由大写转换为小写。 conv = notrunc 不截短输出文件。 conv = swab 交换每一对输入字节。 conv = noerror 出错时不停止处理。 conv = sync 把每个输入记录的大小都调到ibs的大小（用NUL填充）。 dd if=[STDIN] of=[STDOUT] 输入和输出 seek=BLOCKS 跳过一段以后才输出 拷贝光盘&nbsp;&nbsp; &nbsp;注意：光碟是标准的 iso9660格式才行 dd if=/dev/cdrom of=cdrom.isocdrecord -v cdrom.iso 特殊块 dd if=/dev/zero of=hello.txt bs=20M count=1 创建一个20M的空文件 /dev/null，外号叫无底洞，你可以向它输出任何数据，它不会达到饱和。 /dev/zero,是一个输入设备，你可你用它来初始化文件。 /dev/null——它是空设备，也称为位桶（bit bucket）。任何写入它的输出都会被抛弃。如果不想让消息以标准输出显示或写入文件，那么可以将消息重定向到位桶。 /dev/zero——该设备无穷尽地提供0，可以使用任何你需要的数目——设备提供的要多的多。他可以用于向设备或文件写入字符串0。 cpio + find 备份 find -name file | cpio -o &gt; xx.cpio cpio -i \&lt; xx.cpio 参考链接:Linux-dd命令详解 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF算法]]></title>
    <url>%2F2017%2F03%2F11%2F2017-03-11-tfidf%2F</url>
    <content type="text"><![CDATA[正文定义term frequency-inverse document frequency 是一种用于抽取一个语料库中一个文件的关键词，从而得到一个词对于语料库中的一个文件的重要程度。 原理TF首先思考，对于一篇文章而言，什么词能成为关键词呢？可以很快考虑到，词频，即出现次数多的词，有很大概率是关键词。 A=某个词在文档中的次数 B=文章的总次数 C=文档中出现最多的词的次数 或 IDF有了词频之后，可以发现，在其中，“的”，“我”, “你”等词出现的次数最多。然而显然这些词并不是关键词，那么怎么办？ 可以发现，只是词频最大的单词并不一定是关键词，有些词出现次数少一些，但是它比较重要，所以是关键词，所以应该找一个标准来定义词的重要度。 可以认同的是，如果一个词在别的文章中出现次数较少，而在本文中多次出现，那么这个词重要性就比较高。这就是逆文档概率(IDF)。 D=语料库的总文档数E=包含该词的文档数+1 结论有了TF和IDF,那么只要把他们相乘就能得到关键词的重要程度了 TF-IDF = TF x IDF 总结TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。 参考链接:TF-IDF与余弦相似性的应用（一）：自动提取关键词 转载请注明:Artemis的博客–&gt; 点此看原文]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>提取关键词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式浅析]]></title>
    <url>%2F2017%2F03%2F09%2F2017-03-09-regexp%2F</url>
    <content type="text"><![CDATA[很早之前就听说过正则表达式，在学习vim和bash的时候也用过一些简单正则表达式，但是一直没有系统的学习一下。最近在学习nlp的相关知识，于是就系统的学习了一下。 语言和类基本每种编程语言都会支持正则表达式，这里为了演示方便使用java,java中下面三个类会支持正则表达式: java.lang.String java.util.regex.Pattern java.util.regex.Matcher 123456789"cao88786656ll".replaceAll("\\d", "-")"cao88786656ll" .mathes("\\d")"cao88786656ll" .replaceAll("\\d", "-")"cao88786656ll" .replaceFirst(regex, replacement )Pattern 和 Matcher的用法Pattern p = Pattern.compile("[a-z]&#123;2&#125;" );Matcher m = p.matcher( "fgh");System. out.println(m.matches()); MetaCharacters正则表达式包含一些基本的符号，下面： . 任意一种字符 * 0个或多个 + 一个或多个 ? 0个或一个 {n，m} 出现次数大于n小于m \d [0-9] \D [^\d] \s 空白字符 \S [^\s] \w [a-zA-Z_0-9] \W [^\w] 这里要注意0宽度(zero length)匹配(&quot;&quot;).mathes(&quot;a?&quot;)匹配一个\(&quot;\\&quot;).mathes(&quot;\\\\&quot;) 范围 [ ]一个中括号只匹配一个字符 ^ 除了… | 或 &amp;&amp; 且 - 范围什么到什么 边界匹配 ^在[ ]外面表示开头 $表示结束 字符串替换12345String s = "java Java JAVA JAvA I love jAVa you hate JAVA" ;Pattern p = Pattern. compile("JAVA", Pattern. CASE_INSENSITIVE);Matcher m = p.matcher(s);s = m.replaceAll("java");System. out.println(s); Pattern. CASE_INSENSITIVE 不区分大小写 奇数变成大写，偶数变成小写123456789101112131415161718String s = "java Java JAVA JAvA I love jAVa you hate JAVA" ;Pattern p = Pattern. compile("JAVA", Pattern. CASE_INSENSITIVE);StringBuffer buf = new StringBuffer();Matcher m = p.matcher(s);int i = 0;while(m.find())&#123; i++; if(i % 2 == 0) &#123; m.appendReplacement( buf, "java"); &#125; else m.appendReplacement( buf, "JAVA");&#125;m.appendTail( buf);System. out.println(buf); 分组把整个正则表达式匹配成功的那个子串是一个组而且是第0组,然后在这个第一组之内又可以分组然后从组号1开始,用小括号分组 ( ) 从外向内数。1234567Pattern p = Pattern. compile("(\\d&#123;3,5&#125;)([a-z]&#123;2&#125;)");String s = "123aa-124234bb-235cc-00";Matcher m = p.matcher(s);while(m.find())&#123; System. out.println(m.group(1));&#125; matches find lookingat都是Matcher类的函数matched 匹配:find找一个匹配的子串 find一次之后会把找到的子串去掉下一次找的话从剩余的里面找lookingat表示每次都是从头上找一个匹配的子序列 start end上一次匹配的起始位置和结束位置,但是上一次必须得匹配不匹配的话会把报错123456789101112131415161718Pattern p = Pattern.compile("\\d&#123;3,5&#125;" );Matcher m = p.matcher( "123-234-5645-00");System.out.println(m.matches());//m.reset();符/*System.out.println(m.find());System.out.println(m.start() + "-" + m.end());System.out.println(m.find());System.out.println(m.start() + "-" + m.end());System.out.println(m.find());System.out.println(m.start() + "-" + m.end());System.out.println(m.find());*/System.out.println(m.start() + "-" + m.end());System.out.println(m.lookingAt());System.out.println(m.start() + "-" + m.end());System.out.println(m.lookingAt());System.out.println(m.start() + "-" + m.end());System.out.println(m.lookingAt());System.out.println(m.lookingAt()); 匹配空白行的regular expression^[\\s&amp;&amp;[^\\n]]*\\n$ flags的简写Pattern.CASE_INSENSITIVE 可以简写为(?i) back refenrences\\aa代表以后和第a组匹配的东西一样才能使整个串匹配123456789101112131415161718192021222324Pattern p = Pattern. compile("(\\d\\d)\\1");String s = "1212";Matcher m = p.matcher(s);System. out.println(m.matches());truePattern p = Pattern. compile("(\\d\\d)\\1");String s = "1213";Matcher m = p.matcher(s);falsePattern p = Pattern. compile("(\\d(\\d))\\2");String s = "121";Matcher m = p.matcher(s);System. out.println(m.matches());falsePattern p = Pattern. compile("(\\d(\\d))\\2");String s = "122";Matcher m = p.matcher(s);System. out.println(m.matches());true POSIX character classes\p{Lower}\p{Upper}具体可以查看文档. quantifiers修订词限定词 Greedy 普通写法 Reluctant Greedy 加上一个? 勉强的 Possessive Greedy 加上一个+1234567891011121314151617181920212223242526272829303132333435 @Test public void test()&#123; Pattern p = Pattern. compile("(.&#123;3,10&#125;)[0-9]"); String s = "aaaa5bbbb6"; Matcher m = p.matcher(s); if(m.find()) System. out.println(m.start() + "-" + m.end()); else System. out.println("not match" ); &#125; 答案 0-10 @Test public void test()&#123; Pattern p = Pattern. compile("(.&#123;3,10&#125;+)[0-9]"); String s = "aaaa5bbbb6"; Matcher m = p.matcher(s); if(m.find()) System. out.println(m.start() + "-" + m.end()); else System. out.println("not match" ); &#125; 答案 not matchPossessive吞完之后不吐用的较少 @Test public void test()&#123; Pattern p = Pattern. compile("(.&#123;3,10&#125;?)[0-9]"); String s = "aaaa5bbbb6"; Matcher m = p.matcher(s); if(m.find()) System. out.println(m.start() + "-" + m.end()); else System. out.println("not match" ); &#125; 0-5 non-capturing groups(？=a)组内以问号开头那么就不捕获这个组123456789101112Pattern p = Pattern. compile(".&#123;3&#125;(?=a)");String s = "444a66b";Matcher m = p.matcher(s);while(m.find()) System. out.println(m.group());444Pattern p = Pattern.compile("(?=a).&#123;3&#125;" );String s = "444a66b";Matcher m = p.matcher(s);while(m.find()) System. out.println(m.group());a66 匹配email的regexp[\\w[._]]+@[\\w[._]]+\\.[\\w[._]]+ 样例程序统计代码空行数找网页中的邮箱 正则表达式]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>java</tag>
      </tags>
  </entry>
</search>
